{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.func import grad, vmap, hessian, jacfwd, jacrev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(4, 3)\n",
    "        self.hidden_fc = nn.Linear(3, 3)\n",
    "        self.out_fc = nn.Linear(3, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.input_fc(x))\n",
    "        x = F.tanh(self.hidden_fc(x))\n",
    "        x = self.out_fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(weights, x):\n",
    "    return torch.tanh(x.dot(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 2D and 1D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m4\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m examples \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m7\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m output \u001b[39m=\u001b[39m model(weights, examples)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "Cell \u001b[0;32mIn[86], line 2\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(weights, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodel\u001b[39m(weights, x):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtanh(x\u001b[39m.\u001b[39;49mdot(weights))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D tensors expected, but got 2D and 1D tensors"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "weights = torch.randn(4, requires_grad=True)\n",
    "examples = torch.randn(7, 4)\n",
    "output = model(weights, examples)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2620, -0.2274, -0.2316, -0.0540,  0.2235, -0.5061,  0.1701],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = vmap(model, in_dims=(None, 0))(weights, examples)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5955, -0.8633,  0.8782, -0.2237, -0.2024, -0.0707,  0.7701],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "weights = torch.randn((7, 4), requires_grad=True)\n",
    "examples = torch.randn(4)\n",
    "output = vmap(model, in_dims=(0, None))(weights, examples)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9974, -0.5747,  0.6330, -0.1288,  0.9984, -0.8981,  0.9824],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "weights = torch.randn((7, 4), requires_grad=True)\n",
    "examples = torch.randn(7, 4)\n",
    "output = vmap(model, in_dims=(0, 0))(weights, examples)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 2D and 1D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[39m=\u001b[39m model(weights, torch\u001b[39m.\u001b[39;49mrandn([\u001b[39m7\u001b[39;49m, \u001b[39m4\u001b[39;49m]))\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "Cell \u001b[0;32mIn[73], line 2\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(weights, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodel\u001b[39m(weights, x):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mdot(weights)\u001b[39m.\u001b[39mrelu()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D tensors expected, but got 2D and 1D tensors"
     ]
    }
   ],
   "source": [
    "output = model(weights, torch.randn([7, 4]))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = Policy()\n",
    "out = policy(torch.randn([7, 4]))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradient(net):\n",
    "    for p in net.parameters():\n",
    "        print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Out\n",
      "tensor([-0.5106,  0.0694], grad_fn=<AddBackward0>)\n",
      "tensor([-0.6150,  0.4325], grad_fn=<AddBackward0>)\n",
      "--------------------------------\n",
      "Loss\n",
      "tensor(0.6033, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6239, grad_fn=<MseLossBackward0>)\n",
      "--------------------------------\n",
      "Gradients\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "--------------------------------\n",
      "Gradients backward 1\n",
      "tensor([[-0.0099, -0.0038, -0.0069, -0.0068],\n",
      "        [ 0.0075,  0.0029,  0.0052,  0.0052],\n",
      "        [ 0.0159,  0.0061,  0.0111,  0.0109]])\n",
      "tensor([-0.0294,  0.0224,  0.0473])\n",
      "tensor([[ 0.1060, -0.1415, -0.0192],\n",
      "        [-0.0661,  0.0882,  0.0120],\n",
      "        [ 0.0369, -0.0492, -0.0067]])\n",
      "tensor([-0.2717,  0.1694, -0.0945])\n",
      "tensor([[0.3334, 0.0222, 0.5116],\n",
      "        [0.1421, 0.0094, 0.2180]])\n",
      "tensor([-1.0106, -0.4306])\n",
      "--------------------------------\n",
      "Gradients backward 2\n",
      "tensor([[ 0.0380,  0.1084, -0.0780,  0.0187],\n",
      "        [ 0.0387,  0.0759, -0.0410,  0.0217],\n",
      "        [-0.0029, -0.0381,  0.0391,  0.0009]])\n",
      "tensor([-0.0790, -0.0099,  0.0669])\n",
      "tensor([[ 0.1506, -0.0726, -0.0433],\n",
      "        [-0.1851, -0.0959,  0.0762],\n",
      "        [ 0.0736,  0.0076, -0.0265]])\n",
      "tensor([-0.3821,  0.4643, -0.1855])\n",
      "tensor([[ 0.5201, -0.4664,  0.3810],\n",
      "        [ 0.1534, -0.0201,  0.2101]])\n",
      "tensor([-2.1256, -0.4981])\n"
     ]
    }
   ],
   "source": [
    "policy = Policy()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "state1 = torch.randn(4)\n",
    "out1 = policy(state1)\n",
    "print(\"-\"*32)\n",
    "print(\"Out\")\n",
    "print(out1)\n",
    "\n",
    "torch.manual_seed(55)\n",
    "state2 = torch.randn(4)\n",
    "out2 = policy(state2)\n",
    "print(out2)\n",
    "\n",
    "true = torch.tensor([0.5, 0.5])\n",
    "loss1 = F.mse_loss(true, out1)\n",
    "loss2 = F.mse_loss(true, out2)\n",
    "print(\"-\"*32)\n",
    "print(\"Loss\")\n",
    "print(loss1)\n",
    "print(loss2)\n",
    "print(\"-\"*32)\n",
    "print(\"Gradients\")\n",
    "print_gradient(policy)\n",
    "print(\"-\"*32)\n",
    "print(\"Gradients backward 1\")\n",
    "loss1.backward()\n",
    "print_gradient(policy)\n",
    "print(\"-\"*32)\n",
    "print(\"Gradients backward 2\")\n",
    "loss2.backward()\n",
    "print_gradient(policy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Out\n",
      "tensor([-0.5106,  0.0694], grad_fn=<AddBackward0>)\n",
      "tensor([-0.6150,  0.4325], grad_fn=<AddBackward0>)\n",
      "--------------------------------\n",
      "Loss\n",
      "tensor(0.6033, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6239, grad_fn=<MseLossBackward0>)\n",
      "--------------------------------\n",
      "Gradients\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "--------------------------------\n",
      "Gradients backward 2\n",
      "tensor([[ 0.0479,  0.1122, -0.0712,  0.0255],\n",
      "        [ 0.0312,  0.0730, -0.0463,  0.0166],\n",
      "        [-0.0189, -0.0442,  0.0280, -0.0100]])\n",
      "tensor([-0.0496, -0.0323,  0.0195])\n",
      "tensor([[ 0.0445,  0.0689, -0.0240],\n",
      "        [-0.1190, -0.1841,  0.0642],\n",
      "        [ 0.0368,  0.0568, -0.0198]])\n",
      "tensor([-0.1103,  0.2949, -0.0910])\n",
      "tensor([[ 0.1867, -0.4885, -0.1306],\n",
      "        [ 0.0113, -0.0296, -0.0079]])\n",
      "tensor([-1.1150, -0.0675])\n",
      "--------------------------------\n",
      "Gradients backward 1\n",
      "tensor([[ 0.0380,  0.1084, -0.0780,  0.0187],\n",
      "        [ 0.0387,  0.0759, -0.0410,  0.0217],\n",
      "        [-0.0029, -0.0381,  0.0391,  0.0009]])\n",
      "tensor([-0.0790, -0.0099,  0.0669])\n",
      "tensor([[ 0.1506, -0.0726, -0.0433],\n",
      "        [-0.1851, -0.0959,  0.0762],\n",
      "        [ 0.0736,  0.0076, -0.0265]])\n",
      "tensor([-0.3821,  0.4643, -0.1855])\n",
      "tensor([[ 0.5201, -0.4664,  0.3810],\n",
      "        [ 0.1534, -0.0201,  0.2101]])\n",
      "tensor([-2.1256, -0.4981])\n"
     ]
    }
   ],
   "source": [
    "policy = Policy()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "state1 = torch.randn(4)\n",
    "out1 = policy(state1)\n",
    "print(\"-\"*32)\n",
    "print(\"Out\")\n",
    "print(out1)\n",
    "\n",
    "torch.manual_seed(55)\n",
    "state2 = torch.randn(4)\n",
    "out2 = policy(state2)\n",
    "print(out2)\n",
    "\n",
    "true = torch.tensor([0.5, 0.5])\n",
    "loss1 = F.mse_loss(true, out1)\n",
    "loss2 = F.mse_loss(true, out2)\n",
    "print(\"-\"*32)\n",
    "print(\"Loss\")\n",
    "print(loss1)\n",
    "print(loss2)\n",
    "print(\"-\"*32)\n",
    "print(\"Gradients\")\n",
    "print_gradient(policy)\n",
    "print(\"-\"*32)\n",
    "print(\"Gradients backward 2\")\n",
    "loss2.backward()\n",
    "print_gradient(policy)\n",
    "print(\"-\"*32)\n",
    "print(\"Gradients backward 1\")\n",
    "loss1.backward()\n",
    "print_gradient(policy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.randn([])\n",
    "func = torch.sin\n",
    "cos = grad(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ = cos(torch.tensor(0.))\n",
    "output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ = torch.cos(torch.tensor(0.))\n",
    "output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_sin = grad(cos)\n",
    "output_ = neg_sin(torch.tensor(torch.pi / 2))\n",
    "output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ = torch.sin(torch.tensor(torch.pi / 2))\n",
    "output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x1, x2 = x\n",
    "    return x1 ** 3 + 2 *x1 * x2 + x2 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_f = hessian(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_= torch.tensor([1.0, 1.0])\n",
    "hess = hessian_f(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 2.],\n",
       "        [2., 6.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    x1, x2 = x\n",
    "    return x1 ** 3, x2 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[6., 0.],\n",
      "        [0., 0.]]), tensor([[0., 0.],\n",
      "        [0., 6.]]))\n"
     ]
    }
   ],
   "source": [
    "hessian_f2 = hessian(f2)\n",
    "input_= torch.tensor([1.0, 1.0])\n",
    "hess = hessian_f2(input_)\n",
    "print(hess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0673, 0.0000]), tensor([0.0000, 0.1309]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2)\n",
    "jacobian = jacfwd(f2)(x)\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "def loss_fn(predictions, targets):\n",
    "    return F.nll_loss(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "num_models = 10\n",
    "batch_size = 64\n",
    "data = torch.randn(batch_size, 1, 28, 28, device=device)\n",
    "\n",
    "targets = torch.randint(10, (64,), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device=device)\n",
    "predictions = model(data) # move the entire mini-batch through the model\n",
    "\n",
    "loss = loss_fn(predictions, targets)\n",
    "# loss.backward() # back propogate the 'average' gradient of this mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = torch.autograd.grad(loss, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-2.2089e-03, -6.2267e-04, -4.9707e-04],\n",
       "           [-3.0661e-03,  5.1212e-04, -1.4034e-03],\n",
       "           [-1.1445e-03, -1.1397e-03,  3.2198e-04]]],\n",
       " \n",
       " \n",
       "         [[[-4.0881e-03, -2.4897e-04, -2.7835e-03],\n",
       "           [ 2.8883e-03,  1.5631e-03, -1.0181e-03],\n",
       "           [-5.9385e-03, -3.3814e-03,  2.2684e-04]]],\n",
       " \n",
       " \n",
       "         [[[ 1.8838e-03, -9.2863e-04,  8.8398e-04],\n",
       "           [ 2.8276e-03, -3.4123e-03, -2.9668e-03],\n",
       "           [ 1.7818e-03, -1.2885e-03, -2.6468e-03]]],\n",
       " \n",
       " \n",
       "         [[[-1.8404e-03, -6.5563e-04, -9.2311e-04],\n",
       "           [ 4.2281e-03, -3.2949e-04,  1.2964e-04],\n",
       "           [ 9.3796e-05, -1.4823e-03, -7.7952e-04]]],\n",
       " \n",
       " \n",
       "         [[[ 2.2720e-03,  1.6101e-03,  1.2282e-03],\n",
       "           [ 4.0830e-03, -2.5433e-03, -3.4472e-03],\n",
       "           [ 2.3777e-03, -5.6451e-04, -2.7303e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 9.0371e-04, -6.3975e-04,  2.1883e-03],\n",
       "           [-1.8931e-03, -1.5285e-03, -2.8925e-04],\n",
       "           [-2.2178e-03,  2.2870e-04, -3.6063e-03]]],\n",
       " \n",
       " \n",
       "         [[[-4.6432e-04, -1.9080e-03,  3.3471e-03],\n",
       "           [ 2.7337e-03, -3.3325e-03, -6.1380e-04],\n",
       "           [-6.7477e-04,  6.2705e-04, -1.7971e-03]]],\n",
       " \n",
       " \n",
       "         [[[-2.1594e-03,  1.4414e-03,  2.2019e-03],\n",
       "           [-2.4159e-03,  1.1501e-03,  8.6257e-04],\n",
       "           [ 7.8549e-04,  1.5494e-03, -1.3551e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5575e-03,  1.1462e-03, -2.5912e-03],\n",
       "           [-3.7264e-03,  4.1203e-03,  1.5813e-03],\n",
       "           [ 1.5668e-03, -5.5303e-04,  5.9099e-04]]],\n",
       " \n",
       " \n",
       "         [[[-1.6820e-03, -8.7951e-04, -5.9436e-04],\n",
       "           [-1.2321e-03, -1.1061e-03,  7.4760e-04],\n",
       "           [-2.9755e-03,  5.3216e-04,  1.6180e-03]]],\n",
       " \n",
       " \n",
       "         [[[-1.9979e-04,  1.7024e-03,  1.2165e-03],\n",
       "           [ 1.3677e-03, -6.5024e-04,  1.8753e-03],\n",
       "           [ 1.9928e-03,  1.1623e-03, -3.1905e-03]]],\n",
       " \n",
       " \n",
       "         [[[-3.1348e-04,  1.1047e-03, -9.8577e-04],\n",
       "           [-7.7099e-04, -3.8643e-04, -8.3353e-04],\n",
       "           [-1.8705e-03,  3.8450e-04, -1.7002e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 7.4057e-04, -1.6990e-03,  7.3532e-04],\n",
       "           [ 2.7885e-04, -4.1984e-03,  1.2699e-03],\n",
       "           [ 3.5001e-03,  2.2941e-03,  4.5318e-04]]],\n",
       " \n",
       " \n",
       "         [[[-3.9521e-04, -1.3498e-03, -1.8519e-03],\n",
       "           [ 9.6624e-04,  5.8874e-04,  1.3766e-03],\n",
       "           [-8.6559e-04,  1.4279e-03, -3.3899e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 2.9728e-03, -4.4253e-03, -1.8502e-03],\n",
       "           [ 2.8449e-04, -1.7876e-03, -4.9313e-03],\n",
       "           [-4.5515e-03,  3.6276e-03,  9.8553e-04]]],\n",
       " \n",
       " \n",
       "         [[[-3.6175e-03,  3.8740e-05,  2.3446e-05],\n",
       "           [-8.3643e-03,  1.5428e-03, -1.1956e-03],\n",
       "           [ 1.1968e-03, -7.1219e-04, -2.6548e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.2231e-03,  1.1731e-03, -2.0478e-03],\n",
       "           [ 3.9898e-03,  4.5379e-04, -2.3833e-03],\n",
       "           [-7.5087e-03, -5.5684e-04,  5.7831e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 2.7705e-03,  8.9062e-04, -2.3565e-03],\n",
       "           [-4.6042e-04,  6.7709e-04, -1.7760e-03],\n",
       "           [ 2.9662e-03,  1.7346e-03,  1.3173e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 2.9273e-04,  4.7226e-03,  1.6669e-03],\n",
       "           [ 9.7960e-04,  4.3442e-03,  3.8955e-04],\n",
       "           [-5.8725e-04, -5.2591e-03,  2.2376e-03]]],\n",
       " \n",
       " \n",
       "         [[[-6.0536e-05,  2.6559e-03, -4.7235e-03],\n",
       "           [-2.2534e-04,  3.1289e-04,  8.3712e-04],\n",
       "           [ 1.5268e-04, -2.2862e-04, -1.2347e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5241e-04,  2.5328e-03,  1.1811e-03],\n",
       "           [-1.2992e-03,  3.1183e-04, -1.0751e-03],\n",
       "           [ 3.4480e-03,  1.0670e-03,  1.9574e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.3609e-03, -1.3891e-03,  1.6094e-04],\n",
       "           [-3.3848e-05, -1.4655e-03,  5.9449e-04],\n",
       "           [-9.1521e-05, -3.4171e-04,  5.8122e-03]]],\n",
       " \n",
       " \n",
       "         [[[-1.6143e-03,  1.2579e-03, -3.6714e-03],\n",
       "           [-1.5341e-03,  2.0435e-03,  1.4113e-03],\n",
       "           [ 1.7216e-03, -7.3939e-04, -2.1203e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 3.2746e-04,  3.4543e-03,  8.0712e-04],\n",
       "           [ 1.3481e-04, -1.1307e-04,  2.1485e-03],\n",
       "           [ 4.0449e-04, -6.0137e-04,  4.3249e-04]]],\n",
       " \n",
       " \n",
       "         [[[-1.4749e-03, -4.3328e-03, -2.0427e-03],\n",
       "           [-6.1414e-03, -6.2182e-03,  1.5068e-03],\n",
       "           [ 1.3026e-03, -4.7759e-04,  1.7081e-03]]],\n",
       " \n",
       " \n",
       "         [[[-6.1233e-04,  6.5060e-04,  6.4209e-04],\n",
       "           [-2.8682e-03,  1.3944e-04, -1.9077e-03],\n",
       "           [-1.0307e-03,  2.1986e-04,  7.9928e-04]]],\n",
       " \n",
       " \n",
       "         [[[ 1.2406e-03,  2.9543e-04, -3.9666e-03],\n",
       "           [ 1.0377e-03,  3.3795e-03,  4.6301e-03],\n",
       "           [ 1.0842e-03, -2.0975e-03,  1.0562e-03]]],\n",
       " \n",
       " \n",
       "         [[[-2.3474e-03,  1.0770e-04,  4.0362e-03],\n",
       "           [-4.2300e-03, -1.1047e-03,  3.7941e-04],\n",
       "           [ 3.0530e-03,  1.8077e-03,  1.3237e-03]]],\n",
       " \n",
       " \n",
       "         [[[-2.1345e-03,  2.1685e-03, -2.2946e-03],\n",
       "           [ 2.7249e-03, -2.0812e-03,  4.7114e-04],\n",
       "           [-4.2272e-03,  1.5912e-03,  3.5189e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1273e-03, -8.8784e-04, -2.6315e-03],\n",
       "           [ 2.0540e-03,  3.5547e-03,  7.9941e-04],\n",
       "           [-6.3364e-04, -2.7952e-03, -1.9252e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 2.4795e-03,  8.8158e-03, -2.0592e-03],\n",
       "           [ 2.4665e-03, -3.2195e-04,  4.4915e-04],\n",
       "           [-1.6849e-03,  4.4531e-03,  2.7777e-03]]],\n",
       " \n",
       " \n",
       "         [[[-1.8918e-03, -1.5427e-03,  6.4902e-05],\n",
       "           [-1.3329e-04, -1.8462e-04, -4.5953e-04],\n",
       "           [-2.1966e-03,  8.9648e-04, -3.6386e-04]]]], device='cuda:0'),\n",
       " tensor([ 4.3921e-03,  2.3316e-04, -9.0390e-04,  1.9125e-03,  2.9614e-03,\n",
       "         -2.4089e-03, -3.5566e-05, -3.3751e-03,  4.7513e-03,  2.8065e-04,\n",
       "          2.2315e-03,  2.3907e-03,  4.4165e-03, -2.8236e-03,  2.2342e-04,\n",
       "          1.6040e-03,  9.3437e-03,  2.1984e-03,  3.0119e-03, -4.6059e-03,\n",
       "          1.7785e-03, -1.5882e-03, -1.4594e-03, -9.2032e-04,  9.5125e-03,\n",
       "         -3.3901e-03, -6.7619e-04,  5.2187e-03,  2.3185e-03, -1.0573e-03,\n",
       "          6.3441e-03, -1.2353e-03], device='cuda:0'),\n",
       " tensor([[[[-4.2772e-03, -4.4683e-03, -3.0236e-03],\n",
       "           [ 2.6647e-04, -5.4864e-04, -5.0023e-03],\n",
       "           [-6.7047e-03, -1.5505e-03, -1.0514e-03]],\n",
       " \n",
       "          [[-5.2348e-03, -1.3476e-03, -9.3628e-04],\n",
       "           [-3.7061e-03, -3.2091e-03, -4.4513e-04],\n",
       "           [-7.1358e-04, -1.3298e-04, -4.2942e-03]],\n",
       " \n",
       "          [[ 5.2101e-04, -1.9164e-03, -8.3995e-04],\n",
       "           [-3.0007e-04, -2.6050e-04, -4.4660e-04],\n",
       "           [ 4.1956e-04, -1.5845e-03, -2.3888e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.4037e-03, -5.8876e-03, -4.6065e-05],\n",
       "           [ 3.3179e-04, -5.1900e-04, -2.5562e-03],\n",
       "           [-2.8834e-03, -1.7897e-03, -8.0513e-04]],\n",
       " \n",
       "          [[-2.4555e-03, -6.8021e-03, -1.8694e-03],\n",
       "           [-3.3387e-03, -3.4453e-03, -5.0851e-03],\n",
       "           [-2.0755e-03, -5.4510e-03, -2.1300e-03]],\n",
       " \n",
       "          [[-2.3852e-03,  1.1439e-04, -1.9381e-03],\n",
       "           [-1.2024e-03,  5.3486e-04,  5.0823e-04],\n",
       "           [-2.9904e-03, -3.0277e-03, -7.4395e-05]]],\n",
       " \n",
       " \n",
       "         [[[-2.5115e-03, -1.4191e-03, -2.2329e-03],\n",
       "           [-4.6179e-03, -3.8126e-03, -1.9469e-03],\n",
       "           [-2.3929e-03, -3.2780e-03, -4.0456e-03]],\n",
       " \n",
       "          [[ 1.3111e-04, -5.3818e-03, -6.4187e-03],\n",
       "           [-2.2630e-03, -3.5277e-03, -9.4368e-04],\n",
       "           [-3.1794e-03, -3.9139e-03, -4.1815e-03]],\n",
       " \n",
       "          [[-7.8789e-04, -6.7789e-04, -1.2080e-03],\n",
       "           [-2.1431e-04,  8.6241e-04, -1.5017e-03],\n",
       "           [-4.2081e-04, -4.5015e-04, -1.8483e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.7127e-04,  2.1457e-04, -5.2934e-04],\n",
       "           [-2.2307e-03, -2.0484e-03, -1.5206e-03],\n",
       "           [-2.3499e-03, -1.0948e-03, -2.5879e-03]],\n",
       " \n",
       "          [[-4.4076e-03, -2.9034e-03, -5.2104e-03],\n",
       "           [-3.8278e-03, -5.0901e-03, -5.0601e-03],\n",
       "           [-4.3194e-03, -3.9640e-03, -3.3279e-03]],\n",
       " \n",
       "          [[-2.3727e-03, -1.3530e-03,  1.1844e-04],\n",
       "           [-2.1783e-03, -8.4527e-04, -6.1957e-04],\n",
       "           [-3.1939e-04, -1.9137e-03, -1.7628e-03]]],\n",
       " \n",
       " \n",
       "         [[[-2.0026e-03,  1.2531e-04,  8.8655e-04],\n",
       "           [ 3.1317e-03,  1.9460e-04,  1.4784e-03],\n",
       "           [-1.1501e-03,  1.6676e-03,  2.1520e-03]],\n",
       " \n",
       "          [[-8.9749e-04, -8.2467e-05,  9.1302e-04],\n",
       "           [-1.7913e-03,  4.2967e-05, -6.7433e-04],\n",
       "           [ 2.7624e-03,  4.9956e-04,  2.1572e-03]],\n",
       " \n",
       "          [[ 1.6528e-03, -3.1545e-04,  8.6090e-04],\n",
       "           [ 2.7177e-04, -5.8446e-04,  1.5083e-03],\n",
       "           [ 1.6918e-03, -7.7679e-04,  1.5076e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.0833e-04, -8.8100e-04, -9.9484e-04],\n",
       "           [ 8.2077e-04,  1.7749e-03,  3.3982e-04],\n",
       "           [ 7.6247e-04,  1.5546e-04,  7.4415e-04]],\n",
       " \n",
       "          [[ 3.3634e-03,  3.0793e-05,  7.8971e-04],\n",
       "           [ 2.3634e-03,  1.6024e-03,  5.2507e-04],\n",
       "           [ 6.4477e-04,  9.1169e-04,  2.0216e-03]],\n",
       " \n",
       "          [[-8.9146e-04, -7.7150e-05,  5.2139e-04],\n",
       "           [ 2.7702e-04,  1.7235e-03, -1.0679e-03],\n",
       "           [-3.1549e-04,  3.9574e-04,  1.8579e-03]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-2.3265e-03, -2.9502e-03, -1.2088e-03],\n",
       "           [-1.4346e-03,  5.9769e-04, -3.5264e-03],\n",
       "           [-1.6478e-03, -4.5003e-04, -1.9234e-03]],\n",
       " \n",
       "          [[-3.8388e-03,  8.9984e-04, -3.8016e-04],\n",
       "           [-1.9960e-03,  1.0054e-03, -1.6562e-03],\n",
       "           [-1.3836e-03, -1.4417e-03, -1.3116e-03]],\n",
       " \n",
       "          [[ 2.1809e-03, -1.5920e-03, -1.0915e-03],\n",
       "           [ 1.4501e-03, -3.4835e-03,  6.0913e-04],\n",
       "           [ 1.8512e-03, -1.3778e-03, -6.5771e-04]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.8509e-04, -1.6137e-03,  7.3933e-04],\n",
       "           [-1.6366e-04, -7.8303e-04,  4.1398e-04],\n",
       "           [ 3.9146e-04, -5.1340e-04,  6.8051e-04]],\n",
       " \n",
       "          [[ 9.5832e-04, -1.1462e-04,  3.1552e-05],\n",
       "           [-9.7533e-04, -2.0353e-03,  3.7130e-04],\n",
       "           [-1.4622e-03, -1.8386e-03,  4.5917e-04]],\n",
       " \n",
       "          [[-1.0321e-03,  5.0238e-04, -3.9862e-03],\n",
       "           [-3.4582e-04,  1.2031e-03, -6.9953e-04],\n",
       "           [-2.2868e-03,  3.9800e-04,  3.3648e-04]]],\n",
       " \n",
       " \n",
       "         [[[ 3.4804e-04,  5.5330e-04, -7.4674e-04],\n",
       "           [ 1.5614e-04,  4.3498e-04,  1.3450e-03],\n",
       "           [-4.6784e-04,  1.0895e-03,  4.5120e-04]],\n",
       " \n",
       "          [[ 8.5842e-04, -2.8452e-04, -1.3187e-03],\n",
       "           [ 1.8963e-03, -5.1441e-04, -1.3126e-03],\n",
       "           [ 7.9913e-04,  3.1608e-04,  1.3227e-03]],\n",
       " \n",
       "          [[-1.0207e-04, -1.3140e-04,  7.2097e-04],\n",
       "           [-4.4597e-04,  5.4085e-05,  5.7741e-04],\n",
       "           [ 3.9808e-05,  5.6928e-04, -1.5508e-05]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.5097e-04, -6.5396e-04,  1.5004e-03],\n",
       "           [ 8.8299e-04, -5.6473e-04,  1.0307e-03],\n",
       "           [ 5.2626e-04, -7.3175e-05,  5.2588e-04]],\n",
       " \n",
       "          [[ 1.0117e-03,  1.0150e-03,  2.2732e-04],\n",
       "           [ 1.9488e-03,  9.1043e-04,  7.7784e-04],\n",
       "           [ 7.5545e-04,  1.5251e-03,  2.3630e-03]],\n",
       " \n",
       "          [[-1.1505e-04, -5.2901e-04, -2.3224e-04],\n",
       "           [-3.1130e-04, -1.8126e-05,  9.8214e-04],\n",
       "           [ 3.6204e-04,  6.0113e-04,  9.9346e-04]]],\n",
       " \n",
       " \n",
       "         [[[ 1.8170e-03,  7.0818e-03,  4.4253e-03],\n",
       "           [ 3.9024e-03,  2.3306e-03,  2.5540e-03],\n",
       "           [ 1.9777e-03,  3.7571e-03,  2.1066e-03]],\n",
       " \n",
       "          [[ 9.4856e-03,  6.4338e-03,  1.6441e-03],\n",
       "           [ 4.2490e-03,  4.4189e-03,  3.0589e-03],\n",
       "           [ 3.4302e-03,  2.5620e-03,  4.4874e-03]],\n",
       " \n",
       "          [[ 2.5816e-03, -2.9433e-04,  3.7685e-05],\n",
       "           [ 7.6315e-04,  1.1976e-03,  5.0015e-04],\n",
       "           [ 4.8981e-04,  1.1549e-03,  5.5459e-04]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.7179e-03,  1.8036e-03,  5.4550e-03],\n",
       "           [ 9.2468e-04,  3.8524e-04,  1.2832e-03],\n",
       "           [ 2.5325e-03,  1.0077e-03,  2.4314e-03]],\n",
       " \n",
       "          [[ 4.6488e-03,  2.0292e-03,  8.6762e-03],\n",
       "           [ 3.5629e-03,  7.2033e-03,  7.6083e-03],\n",
       "           [ 5.7518e-03,  4.0276e-03,  6.0179e-03]],\n",
       " \n",
       "          [[ 1.1107e-03,  9.7947e-04,  2.3533e-03],\n",
       "           [ 1.1145e-03,  6.1301e-04,  4.7960e-06],\n",
       "           [ 1.2846e-03,  6.6014e-04,  2.1637e-03]]]], device='cuda:0'),\n",
       " tensor([-0.0071, -0.0094,  0.0031,  0.0024,  0.0070, -0.0105,  0.0066,  0.0009,\n",
       "          0.0031,  0.0040, -0.0038, -0.0054,  0.0148,  0.0020, -0.0030, -0.0042,\n",
       "         -0.0056, -0.0020, -0.0026, -0.0073,  0.0080,  0.0120, -0.0136, -0.0082,\n",
       "          0.0186, -0.0006,  0.0129,  0.0026,  0.0076, -0.0142, -0.0185,  0.0133,\n",
       "          0.0223,  0.0066,  0.0061,  0.0019,  0.0171,  0.0027, -0.0098, -0.0054,\n",
       "          0.0051, -0.0039, -0.0042,  0.0011,  0.0034,  0.0020,  0.0050, -0.0091,\n",
       "          0.0093, -0.0039,  0.0009,  0.0082,  0.0040,  0.0105,  0.0080, -0.0016,\n",
       "          0.0004, -0.0029, -0.0075,  0.0045, -0.0026, -0.0022,  0.0017,  0.0109],\n",
       "        device='cuda:0'),\n",
       " tensor([[-2.9263e-03, -3.3769e-03, -3.5843e-03,  ..., -4.0415e-03,\n",
       "          -4.7342e-03, -4.3378e-03],\n",
       "         [-4.0523e-04,  4.5838e-04,  3.4505e-04,  ..., -1.1324e-04,\n",
       "           1.8987e-04, -7.7671e-05],\n",
       "         [-1.4769e-04, -5.7841e-04, -1.1417e-03,  ..., -2.8687e-04,\n",
       "          -1.1696e-03, -1.4877e-03],\n",
       "         ...,\n",
       "         [-1.5697e-03, -1.2152e-03,  2.2635e-03,  ..., -1.1451e-03,\n",
       "          -9.9490e-04, -1.2241e-03],\n",
       "         [ 3.1261e-04,  2.1110e-04,  6.3129e-04,  ..., -6.4517e-05,\n",
       "           7.4236e-04,  7.0760e-04],\n",
       "         [ 1.2928e-03,  3.7515e-04,  1.7549e-03,  ...,  2.9179e-04,\n",
       "           1.4375e-03,  1.0356e-03]], device='cuda:0'),\n",
       " tensor([-2.0538e-02,  7.7595e-04, -4.7198e-03, -5.3494e-03,  0.0000e+00,\n",
       "          0.0000e+00,  2.3243e-03,  2.4482e-04, -8.0212e-03, -8.5084e-03,\n",
       "          8.3435e-03,  0.0000e+00, -1.4762e-03,  2.3532e-02, -8.7503e-04,\n",
       "         -1.4160e-02, -1.7575e-03, -8.1326e-04,  1.2953e-02,  0.0000e+00,\n",
       "         -2.2748e-02, -2.5199e-02,  0.0000e+00,  1.5225e-03,  0.0000e+00,\n",
       "          1.9635e-03, -1.1138e-03,  0.0000e+00,  1.3561e-02, -4.7144e-03,\n",
       "         -7.5658e-03,  4.7417e-03,  7.1849e-03,  1.3974e-02, -7.1010e-04,\n",
       "          6.2399e-03,  2.3383e-02,  6.4923e-03,  8.6108e-03,  5.9423e-04,\n",
       "         -1.5719e-03,  0.0000e+00, -1.3994e-02,  7.9175e-03,  1.5434e-02,\n",
       "         -1.4982e-03,  1.5717e-03, -6.4681e-04, -8.7951e-03,  0.0000e+00,\n",
       "          2.4174e-02, -1.2120e-03,  0.0000e+00, -8.8046e-03,  6.3069e-04,\n",
       "         -4.2539e-03, -1.5991e-03, -2.5709e-02,  2.3886e-03,  3.8608e-02,\n",
       "         -3.0663e-05,  3.4279e-04,  2.6502e-02, -2.0306e-02,  8.4429e-03,\n",
       "         -1.1653e-02, -1.8829e-02,  2.8602e-03,  2.9504e-03,  3.1728e-02,\n",
       "          3.1719e-02, -8.6738e-04, -2.7754e-03,  6.9375e-04, -3.0103e-02,\n",
       "          0.0000e+00, -4.1406e-03,  6.6307e-03,  7.8725e-03,  8.2272e-04,\n",
       "         -1.8608e-02, -9.9906e-04,  4.6924e-03,  2.6819e-02,  2.0913e-03,\n",
       "         -2.6825e-03,  9.4809e-03, -1.2281e-03,  1.5471e-02, -8.6684e-04,\n",
       "          7.5421e-04,  1.0189e-02, -1.6009e-02,  2.1307e-03,  0.0000e+00,\n",
       "         -1.0373e-02, -1.0378e-02,  7.1396e-04,  0.0000e+00,  7.3489e-03,\n",
       "         -1.8415e-02, -2.0757e-02,  2.7002e-03, -1.1517e-02, -2.8742e-03,\n",
       "          0.0000e+00,  0.0000e+00,  5.7527e-03, -4.8795e-06,  1.3886e-02,\n",
       "         -4.8682e-04,  0.0000e+00, -1.4687e-02, -4.1344e-03,  1.6087e-03,\n",
       "          3.1585e-02,  1.4864e-03,  1.1440e-03,  2.0859e-03,  1.0365e-02,\n",
       "         -4.0856e-03,  8.2895e-04,  1.5452e-02,  0.0000e+00, -2.9476e-04,\n",
       "         -3.9324e-03,  1.8532e-03,  5.1535e-03], device='cuda:0'),\n",
       " tensor([[-3.7575e-03, -1.1140e-03, -1.3810e-03,  ..., -1.1223e-02,\n",
       "          -2.5037e-03, -5.6950e-04],\n",
       "         [-2.2630e-03, -7.0669e-04,  0.0000e+00,  ..., -1.6585e-02,\n",
       "          -2.2230e-03,  0.0000e+00],\n",
       "         [-5.5139e-03, -1.1913e-03, -4.9237e-05,  ..., -3.0052e-02,\n",
       "          -6.3163e-04, -1.4899e-03],\n",
       "         ...,\n",
       "         [-7.6834e-03,  0.0000e+00, -9.6802e-04,  ..., -2.3256e-02,\n",
       "          -1.9414e-03,  0.0000e+00],\n",
       "         [-7.3171e-04, -3.9883e-03, -2.6906e-03,  ..., -9.8024e-03,\n",
       "          -4.7094e-03, -2.1993e-03],\n",
       "         [-4.2471e-03, -6.0932e-05, -2.7669e-04,  ..., -8.0268e-03,\n",
       "          -5.8123e-04,  0.0000e+00]], device='cuda:0'),\n",
       " tensor([-0.0625, -0.0781, -0.1562, -0.1406, -0.0938, -0.0312, -0.1562, -0.1250,\n",
       "         -0.0938, -0.0625], device='cuda:0'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(2.0)\n",
    "b = torch.tensor(1.0)\n",
    "c = [a, b]\n",
    "d = torch.mean(torch.tensor(c))\n",
    "print(d)\n",
    "e = torch.sum(torch.tensor(c))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0, 2] = 0.3\n",
    "a[1, 3] = -0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "mask = np.tril(np.ones_like(a, dtype=bool))\n",
    "g = sns.heatmap(a, mask=mask, vmax=1.0, center=0, square=True,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "# wandb.log({\"plot\":fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True,  True,  True],\n",
       "       [False, False,  True,  True],\n",
       "       [False, False, False,  True],\n",
       "       [False, False, False, False]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.triu(np.ones_like(np.ones([4, 4]), dtype=bool))\n",
    "np.fill_diagonal(mask, False)\n",
    "mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag = np.diagonal(np.ones([4, 4]))\n",
    "diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (0) must match the size of tensor b (10) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m b \u001b[39m=\u001b[39m a \u001b[39m+\u001b[39;49m torch\u001b[39m.\u001b[39;49mones([\u001b[39m10\u001b[39;49m,])\n\u001b[1;32m      2\u001b[0m b\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (0) must match the size of tensor b (10) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "b = a + torch.ones([10,])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = loss.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.4254e-03,  1.8971e-03,  1.3855e-03],\n",
       "          [-2.3087e-03, -2.4522e-03, -2.7523e-03],\n",
       "          [ 1.2211e-03,  4.4381e-03, -3.8151e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.5369e-03,  1.3583e-03,  1.7925e-03],\n",
       "          [ 2.7614e-03,  2.3689e-03, -1.9222e-03],\n",
       "          [ 1.7296e-04,  2.8222e-06,  2.4934e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 4.5194e-03, -7.5458e-05,  1.1343e-03],\n",
       "          [ 3.2135e-03, -1.0193e-03,  2.6348e-03],\n",
       "          [ 1.4243e-04,  7.1246e-03,  1.0681e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.1538e-03, -2.4196e-03,  1.0600e-03],\n",
       "          [-2.1031e-03,  8.5481e-04,  2.8559e-04],\n",
       "          [-1.2824e-03, -1.1185e-03, -2.2762e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.1875e-03, -3.4017e-03,  2.1573e-03],\n",
       "          [-7.5445e-04, -2.2553e-03, -3.1270e-03],\n",
       "          [ 6.1012e-03,  9.6021e-04,  2.3630e-03]]],\n",
       "\n",
       "\n",
       "        [[[-3.0420e-04, -4.0214e-03,  1.0763e-03],\n",
       "          [ 1.0182e-03,  8.3798e-04,  6.2783e-04],\n",
       "          [-3.7354e-03, -5.2280e-04,  1.4669e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 3.7013e-03,  5.8305e-04, -3.3135e-03],\n",
       "          [ 3.5729e-03,  5.4313e-03, -8.3092e-04],\n",
       "          [-5.3461e-04,  3.6713e-03, -1.7497e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 9.9875e-04,  3.0006e-03, -8.7809e-04],\n",
       "          [ 1.4754e-03, -1.6565e-04, -1.1885e-03],\n",
       "          [ 2.9475e-03,  1.0710e-03, -1.0318e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7838e-03, -1.3396e-03, -3.4381e-03],\n",
       "          [ 2.6710e-05, -1.9268e-03,  4.7224e-03],\n",
       "          [ 1.0024e-03, -1.1530e-03, -2.2503e-03]]],\n",
       "\n",
       "\n",
       "        [[[-9.3571e-04, -1.2861e-03,  1.2705e-03],\n",
       "          [ 2.8926e-03,  8.8565e-04,  1.2904e-03],\n",
       "          [ 3.7430e-04,  3.0885e-04,  1.4370e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.7383e-03, -1.9058e-03,  3.0952e-03],\n",
       "          [-2.7668e-03, -2.4201e-03, -1.8336e-03],\n",
       "          [ 1.0914e-03,  3.3861e-03,  3.8735e-04]]],\n",
       "\n",
       "\n",
       "        [[[ 2.1097e-03,  6.3207e-04,  9.6330e-04],\n",
       "          [-6.4123e-04, -1.9921e-03,  2.6041e-03],\n",
       "          [ 5.4819e-04,  6.4985e-04, -8.3603e-04]]],\n",
       "\n",
       "\n",
       "        [[[-9.3259e-04, -3.1001e-04, -1.4190e-03],\n",
       "          [ 1.3208e-03, -8.0891e-04, -1.1835e-03],\n",
       "          [ 3.9103e-04,  2.4067e-03,  8.9305e-04]]],\n",
       "\n",
       "\n",
       "        [[[ 6.6845e-05,  1.8187e-03,  1.1663e-03],\n",
       "          [ 3.7083e-03, -3.1111e-03,  4.8680e-04],\n",
       "          [ 2.4154e-03, -2.7493e-03, -9.7890e-04]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5847e-03, -1.7849e-03, -2.4357e-03],\n",
       "          [ 1.3905e-03,  4.6839e-03, -5.4518e-03],\n",
       "          [ 2.6963e-03, -1.2880e-03,  9.7886e-04]]],\n",
       "\n",
       "\n",
       "        [[[-1.4126e-03,  6.7357e-04, -3.7218e-03],\n",
       "          [-1.5404e-03,  9.1202e-04, -6.9651e-04],\n",
       "          [-3.5501e-03,  2.1252e-03, -4.1750e-04]]],\n",
       "\n",
       "\n",
       "        [[[-7.8209e-06,  1.8262e-03,  4.1256e-03],\n",
       "          [ 2.1918e-03,  9.1649e-04,  7.5865e-04],\n",
       "          [-6.8742e-04, -8.6721e-04, -1.2241e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.7695e-03,  3.7759e-03, -8.4563e-04],\n",
       "          [-8.1899e-04, -7.5480e-05,  4.4398e-03],\n",
       "          [-3.8177e-04,  4.1296e-03, -3.8296e-03]]],\n",
       "\n",
       "\n",
       "        [[[-5.8660e-05,  5.7225e-04, -5.9396e-03],\n",
       "          [ 4.1504e-03,  3.9224e-03,  2.4750e-04],\n",
       "          [-2.3538e-03,  2.7602e-03,  2.4188e-03]]],\n",
       "\n",
       "\n",
       "        [[[-3.5428e-04, -1.5578e-03,  7.6002e-04],\n",
       "          [-5.1776e-04, -2.2329e-03,  9.2627e-04],\n",
       "          [ 2.1578e-03,  1.9902e-04, -4.6117e-04]]],\n",
       "\n",
       "\n",
       "        [[[-1.2800e-03, -3.5029e-03,  5.2545e-03],\n",
       "          [-2.7034e-03, -5.2085e-04,  8.1240e-04],\n",
       "          [-2.3886e-04,  1.5349e-05,  3.1178e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2290e-03,  2.3298e-03, -1.2749e-04],\n",
       "          [ 2.7226e-03, -2.3205e-03,  3.2032e-03],\n",
       "          [-1.8142e-03,  3.3340e-03, -4.1932e-03]]],\n",
       "\n",
       "\n",
       "        [[[-3.0016e-03, -1.3739e-03, -5.3193e-03],\n",
       "          [-3.1623e-04, -1.3648e-03,  2.4803e-03],\n",
       "          [ 2.5712e-03,  2.7665e-03,  3.4023e-03]]],\n",
       "\n",
       "\n",
       "        [[[-3.6680e-03,  2.8623e-03, -1.1173e-03],\n",
       "          [ 2.0672e-03, -1.8883e-04, -5.9108e-04],\n",
       "          [-3.3240e-03,  2.2215e-03,  2.3172e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9959e-03,  2.8422e-03,  2.0025e-03],\n",
       "          [-1.9778e-03, -3.0313e-03,  8.8866e-04],\n",
       "          [-1.6350e-04, -2.6631e-03, -3.3135e-03]]],\n",
       "\n",
       "\n",
       "        [[[-3.7800e-04,  1.3231e-03, -2.7029e-03],\n",
       "          [ 7.7870e-05,  1.9849e-03,  2.8713e-03],\n",
       "          [ 3.6836e-04, -2.6586e-03,  2.1560e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.7751e-03, -2.9477e-03, -2.6273e-03],\n",
       "          [ 1.8401e-03,  1.0972e-03, -5.9564e-04],\n",
       "          [-2.4978e-04,  1.1093e-04, -1.3825e-05]]],\n",
       "\n",
       "\n",
       "        [[[ 2.8738e-03, -1.2771e-03, -2.9750e-03],\n",
       "          [ 2.1527e-03,  2.8220e-04, -1.7677e-03],\n",
       "          [-3.6732e-03,  6.6345e-04, -2.9842e-05]]],\n",
       "\n",
       "\n",
       "        [[[-1.6640e-03,  8.6749e-04,  3.2101e-03],\n",
       "          [-3.5003e-03, -1.5411e-03,  1.0563e-03],\n",
       "          [-1.9613e-03,  1.1315e-03,  3.7274e-03]]],\n",
       "\n",
       "\n",
       "        [[[-8.1472e-04,  1.9873e-03,  1.5511e-03],\n",
       "          [ 2.1833e-03, -3.8745e-04,  3.3934e-03],\n",
       "          [ 1.9844e-04,  4.0280e-05, -3.0391e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3167e-03,  7.6720e-04,  2.8371e-04],\n",
       "          [-1.4870e-03,  2.7850e-03, -2.4117e-03],\n",
       "          [ 1.3418e-03, -1.9646e-03, -2.0868e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0597e-03,  9.2972e-04, -1.5203e-03],\n",
       "          [ 1.5653e-03, -2.4897e-03, -2.4078e-03],\n",
       "          [-5.4760e-03, -3.2711e-03, -2.2082e-03]]]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "conv1.bias\n",
      "conv2.weight\n",
      "conv2.bias\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 3.4254e-03,  1.8971e-03,  1.3855e-03],\n",
      "          [-2.3087e-03, -2.4522e-03, -2.7523e-03],\n",
      "          [ 1.2211e-03,  4.4381e-03, -3.8151e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5369e-03,  1.3583e-03,  1.7925e-03],\n",
      "          [ 2.7614e-03,  2.3689e-03, -1.9222e-03],\n",
      "          [ 1.7296e-04,  2.8222e-06,  2.4934e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5194e-03, -7.5458e-05,  1.1343e-03],\n",
      "          [ 3.2135e-03, -1.0193e-03,  2.6348e-03],\n",
      "          [ 1.4243e-04,  7.1246e-03,  1.0681e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1538e-03, -2.4196e-03,  1.0600e-03],\n",
      "          [-2.1031e-03,  8.5481e-04,  2.8559e-04],\n",
      "          [-1.2824e-03, -1.1185e-03, -2.2762e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.1875e-03, -3.4017e-03,  2.1573e-03],\n",
      "          [-7.5445e-04, -2.2553e-03, -3.1270e-03],\n",
      "          [ 6.1012e-03,  9.6021e-04,  2.3630e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.0420e-04, -4.0214e-03,  1.0763e-03],\n",
      "          [ 1.0182e-03,  8.3798e-04,  6.2783e-04],\n",
      "          [-3.7354e-03, -5.2280e-04,  1.4669e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.7013e-03,  5.8305e-04, -3.3135e-03],\n",
      "          [ 3.5729e-03,  5.4313e-03, -8.3092e-04],\n",
      "          [-5.3461e-04,  3.6713e-03, -1.7497e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 9.9875e-04,  3.0006e-03, -8.7809e-04],\n",
      "          [ 1.4754e-03, -1.6565e-04, -1.1885e-03],\n",
      "          [ 2.9475e-03,  1.0710e-03, -1.0318e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7838e-03, -1.3396e-03, -3.4381e-03],\n",
      "          [ 2.6710e-05, -1.9268e-03,  4.7224e-03],\n",
      "          [ 1.0024e-03, -1.1530e-03, -2.2503e-03]]],\n",
      "\n",
      "\n",
      "        [[[-9.3571e-04, -1.2861e-03,  1.2705e-03],\n",
      "          [ 2.8926e-03,  8.8565e-04,  1.2904e-03],\n",
      "          [ 3.7430e-04,  3.0885e-04,  1.4370e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.7383e-03, -1.9058e-03,  3.0952e-03],\n",
      "          [-2.7668e-03, -2.4201e-03, -1.8336e-03],\n",
      "          [ 1.0914e-03,  3.3861e-03,  3.8735e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1097e-03,  6.3207e-04,  9.6330e-04],\n",
      "          [-6.4123e-04, -1.9921e-03,  2.6041e-03],\n",
      "          [ 5.4819e-04,  6.4985e-04, -8.3603e-04]]],\n",
      "\n",
      "\n",
      "        [[[-9.3259e-04, -3.1001e-04, -1.4190e-03],\n",
      "          [ 1.3208e-03, -8.0891e-04, -1.1835e-03],\n",
      "          [ 3.9103e-04,  2.4067e-03,  8.9305e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 6.6845e-05,  1.8187e-03,  1.1663e-03],\n",
      "          [ 3.7083e-03, -3.1111e-03,  4.8680e-04],\n",
      "          [ 2.4154e-03, -2.7493e-03, -9.7890e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5847e-03, -1.7849e-03, -2.4357e-03],\n",
      "          [ 1.3905e-03,  4.6839e-03, -5.4518e-03],\n",
      "          [ 2.6963e-03, -1.2880e-03,  9.7886e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.4126e-03,  6.7357e-04, -3.7218e-03],\n",
      "          [-1.5404e-03,  9.1202e-04, -6.9651e-04],\n",
      "          [-3.5501e-03,  2.1252e-03, -4.1750e-04]]],\n",
      "\n",
      "\n",
      "        [[[-7.8209e-06,  1.8262e-03,  4.1256e-03],\n",
      "          [ 2.1918e-03,  9.1649e-04,  7.5865e-04],\n",
      "          [-6.8742e-04, -8.6721e-04, -1.2241e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7695e-03,  3.7759e-03, -8.4563e-04],\n",
      "          [-8.1899e-04, -7.5480e-05,  4.4398e-03],\n",
      "          [-3.8177e-04,  4.1296e-03, -3.8296e-03]]],\n",
      "\n",
      "\n",
      "        [[[-5.8660e-05,  5.7225e-04, -5.9396e-03],\n",
      "          [ 4.1504e-03,  3.9224e-03,  2.4750e-04],\n",
      "          [-2.3538e-03,  2.7602e-03,  2.4188e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.5428e-04, -1.5578e-03,  7.6002e-04],\n",
      "          [-5.1776e-04, -2.2329e-03,  9.2627e-04],\n",
      "          [ 2.1578e-03,  1.9902e-04, -4.6117e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.2800e-03, -3.5029e-03,  5.2545e-03],\n",
      "          [-2.7034e-03, -5.2085e-04,  8.1240e-04],\n",
      "          [-2.3886e-04,  1.5349e-05,  3.1178e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2290e-03,  2.3298e-03, -1.2749e-04],\n",
      "          [ 2.7226e-03, -2.3205e-03,  3.2032e-03],\n",
      "          [-1.8142e-03,  3.3340e-03, -4.1932e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.0016e-03, -1.3739e-03, -5.3193e-03],\n",
      "          [-3.1623e-04, -1.3648e-03,  2.4803e-03],\n",
      "          [ 2.5712e-03,  2.7665e-03,  3.4023e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.6680e-03,  2.8623e-03, -1.1173e-03],\n",
      "          [ 2.0672e-03, -1.8883e-04, -5.9108e-04],\n",
      "          [-3.3240e-03,  2.2215e-03,  2.3172e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9959e-03,  2.8422e-03,  2.0025e-03],\n",
      "          [-1.9778e-03, -3.0313e-03,  8.8866e-04],\n",
      "          [-1.6350e-04, -2.6631e-03, -3.3135e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.7800e-04,  1.3231e-03, -2.7029e-03],\n",
      "          [ 7.7870e-05,  1.9849e-03,  2.8713e-03],\n",
      "          [ 3.6836e-04, -2.6586e-03,  2.1560e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.7751e-03, -2.9477e-03, -2.6273e-03],\n",
      "          [ 1.8401e-03,  1.0972e-03, -5.9564e-04],\n",
      "          [-2.4978e-04,  1.1093e-04, -1.3825e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8738e-03, -1.2771e-03, -2.9750e-03],\n",
      "          [ 2.1527e-03,  2.8220e-04, -1.7677e-03],\n",
      "          [-3.6732e-03,  6.6345e-04, -2.9842e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.6640e-03,  8.6749e-04,  3.2101e-03],\n",
      "          [-3.5003e-03, -1.5411e-03,  1.0563e-03],\n",
      "          [-1.9613e-03,  1.1315e-03,  3.7274e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.1472e-04,  1.9873e-03,  1.5511e-03],\n",
      "          [ 2.1833e-03, -3.8745e-04,  3.3934e-03],\n",
      "          [ 1.9844e-04,  4.0280e-05, -3.0391e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3167e-03,  7.6720e-04,  2.8371e-04],\n",
      "          [-1.4870e-03,  2.7850e-03, -2.4117e-03],\n",
      "          [ 1.3418e-03, -1.9646e-03, -2.0868e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0597e-03,  9.2972e-04, -1.5203e-03],\n",
      "          [ 1.5653e-03, -2.4897e-03, -2.4078e-03],\n",
      "          [-5.4760e-03, -3.2711e-03, -2.2082e-03]]]], device='cuda:0')\n",
      "tensor([-8.0636e-03,  1.2920e-03,  9.4266e-03,  9.2271e-04,  1.1306e-02,\n",
      "        -2.5017e-04,  3.1817e-03,  3.9253e-03, -7.0727e-04, -1.6771e-03,\n",
      "        -2.9805e-03, -3.4018e-03,  1.0284e-03, -1.4926e-03,  6.0809e-03,\n",
      "         1.4765e-03,  3.8759e-03, -8.2180e-03,  6.3842e-03,  2.5061e-03,\n",
      "        -4.0859e-03,  1.6625e-03, -3.3166e-03, -4.8960e-03, -7.9004e-03,\n",
      "        -3.8487e-03,  1.8413e-03,  4.4974e-05,  1.0258e-03, -9.0065e-04,\n",
      "        -1.0188e-03,  3.6469e-03], device='cuda:0')\n",
      "tensor([[[[ 4.8719e-03,  4.1673e-03,  3.7004e-03],\n",
      "          [ 4.5982e-03,  4.2739e-03,  1.9904e-03],\n",
      "          [ 5.0598e-03,  1.0483e-03,  3.3145e-03]],\n",
      "\n",
      "         [[ 2.2449e-03,  1.5527e-03,  1.2474e-03],\n",
      "          [ 2.1585e-03,  2.3251e-03,  2.7447e-03],\n",
      "          [ 8.6950e-04,  1.5798e-03,  9.8402e-04]],\n",
      "\n",
      "         [[ 4.5175e-03,  5.3877e-03,  3.3281e-03],\n",
      "          [ 3.5465e-03,  4.2873e-03,  5.1848e-03],\n",
      "          [ 3.9837e-03,  3.5305e-03,  1.9045e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0580e-03,  3.3846e-03,  4.0264e-03],\n",
      "          [ 1.8739e-03,  1.6959e-03,  5.3781e-03],\n",
      "          [ 3.2293e-03,  4.6432e-03,  3.8620e-03]],\n",
      "\n",
      "         [[ 8.4263e-03,  9.5345e-03,  6.4064e-03],\n",
      "          [ 5.9366e-03,  6.4198e-03,  6.5517e-03],\n",
      "          [ 7.4459e-03,  6.4972e-03,  4.3474e-03]],\n",
      "\n",
      "         [[ 3.6841e-03,  3.5910e-03,  4.5965e-03],\n",
      "          [ 3.9758e-03,  4.7583e-03,  3.8928e-03],\n",
      "          [ 2.5595e-04,  4.6921e-03,  4.2788e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.7441e-03,  2.8234e-03,  4.4617e-03],\n",
      "          [ 4.1048e-03,  5.1945e-03,  3.7307e-03],\n",
      "          [ 2.9064e-03,  4.3770e-03,  1.1938e-03]],\n",
      "\n",
      "         [[ 7.8154e-04,  2.8878e-03,  1.1113e-03],\n",
      "          [ 2.4746e-03,  1.8704e-03,  1.1673e-03],\n",
      "          [ 1.1741e-03,  2.2149e-03,  2.4833e-03]],\n",
      "\n",
      "         [[ 2.5041e-03,  2.3482e-03,  4.7724e-03],\n",
      "          [ 4.0108e-03,  2.1918e-03,  4.3155e-03],\n",
      "          [ 4.0106e-03,  4.6382e-03,  5.0155e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4956e-03,  2.9146e-03,  2.3015e-03],\n",
      "          [ 1.1525e-03,  1.1466e-03,  2.7034e-03],\n",
      "          [ 2.6135e-03,  3.7976e-03,  5.7248e-03]],\n",
      "\n",
      "         [[ 6.5973e-03,  4.6698e-03,  8.5353e-03],\n",
      "          [ 7.4439e-03,  4.8893e-03,  4.6757e-03],\n",
      "          [ 5.3571e-03,  7.4744e-03,  7.7890e-03]],\n",
      "\n",
      "         [[ 2.0491e-03,  5.0120e-03,  4.4150e-03],\n",
      "          [ 6.4636e-03,  4.0470e-03,  4.3682e-03],\n",
      "          [ 6.6628e-03,  2.6583e-03,  3.4620e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.1885e-03, -1.7715e-03,  1.9231e-03],\n",
      "          [-1.5883e-03, -6.9952e-04, -4.6969e-04],\n",
      "          [ 5.8044e-04,  1.6310e-04,  3.6540e-04]],\n",
      "\n",
      "         [[-1.7232e-04, -6.6312e-04, -3.8633e-04],\n",
      "          [-2.4566e-04, -8.2367e-04, -8.6377e-04],\n",
      "          [-3.6651e-05, -5.2752e-05,  1.3025e-03]],\n",
      "\n",
      "         [[-7.6044e-04, -1.0376e-03, -1.7399e-03],\n",
      "          [-5.8598e-06,  7.3284e-05, -9.3676e-04],\n",
      "          [-2.2283e-03, -2.3369e-03, -1.0161e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.0922e-04, -3.9183e-04, -1.5314e-03],\n",
      "          [ 1.3480e-03,  6.9567e-04,  8.1991e-04],\n",
      "          [-1.2867e-03, -5.0052e-04, -1.0239e-03]],\n",
      "\n",
      "         [[-1.8165e-03, -2.4299e-03, -2.2106e-03],\n",
      "          [-1.0542e-03, -1.7843e-03, -5.8219e-04],\n",
      "          [ 4.6198e-05, -3.2888e-03, -3.9559e-04]],\n",
      "\n",
      "         [[-1.0850e-03,  8.9655e-04,  1.7881e-03],\n",
      "          [-3.2934e-03,  5.4554e-04, -2.4527e-03],\n",
      "          [-7.0522e-05,  1.7748e-03, -1.1819e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.7110e-03, -1.1824e-03, -5.8012e-03],\n",
      "          [-2.6555e-03, -6.0264e-03, -1.4676e-03],\n",
      "          [-2.5179e-03, -5.6553e-04, -1.1183e-03]],\n",
      "\n",
      "         [[-1.4378e-04, -6.1716e-04, -9.2430e-04],\n",
      "          [-1.5247e-03, -7.6999e-04, -7.3432e-04],\n",
      "          [-1.5569e-03, -2.1000e-03, -3.4184e-04]],\n",
      "\n",
      "         [[-2.0890e-03, -2.0872e-03, -8.0509e-04],\n",
      "          [-3.2383e-04,  5.6704e-04, -4.2656e-03],\n",
      "          [-2.6340e-03, -2.4555e-03, -4.6311e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.5953e-03, -2.6040e-03, -2.4792e-04],\n",
      "          [-7.2853e-04, -7.4625e-04, -2.8651e-03],\n",
      "          [-2.9706e-03, -4.2759e-03, -3.8200e-03]],\n",
      "\n",
      "         [[-3.5384e-03, -2.1943e-03, -4.8107e-03],\n",
      "          [-2.7692e-03, -1.1902e-03, -3.4152e-03],\n",
      "          [-5.4232e-03, -4.6599e-03, -1.0746e-03]],\n",
      "\n",
      "         [[-7.9545e-04, -4.8910e-03, -2.2942e-03],\n",
      "          [-6.0821e-03, -2.6933e-03, -3.9999e-03],\n",
      "          [-2.3149e-03, -1.6208e-03, -5.8479e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5045e-04, -3.2991e-03, -1.4024e-03],\n",
      "          [-9.2416e-04, -6.2721e-05,  3.2983e-03],\n",
      "          [ 1.2494e-03,  1.6584e-03, -2.7335e-03]],\n",
      "\n",
      "         [[ 1.0900e-03, -4.0927e-05, -1.9556e-03],\n",
      "          [ 5.2304e-04,  4.1209e-04,  3.6600e-04],\n",
      "          [ 3.3614e-04,  6.6407e-04,  1.2267e-03]],\n",
      "\n",
      "         [[ 9.6245e-04,  2.1856e-03, -2.1867e-03],\n",
      "          [ 7.0208e-04, -1.5129e-03,  2.2125e-04],\n",
      "          [ 6.2629e-05, -6.4353e-05,  3.0631e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.8108e-04,  8.2932e-04,  1.9763e-03],\n",
      "          [ 1.4295e-03, -9.3845e-05, -4.6516e-04],\n",
      "          [ 3.9882e-04,  5.7094e-04,  1.4145e-03]],\n",
      "\n",
      "         [[ 6.4434e-04,  3.3603e-03, -1.5784e-03],\n",
      "          [ 9.7572e-04, -1.0293e-03,  2.4359e-05],\n",
      "          [-4.1860e-04, -2.8981e-05,  3.2315e-03]],\n",
      "\n",
      "         [[-4.0753e-05,  1.2616e-04,  7.3332e-05],\n",
      "          [ 8.4184e-04,  4.1470e-03,  7.4012e-04],\n",
      "          [ 2.0057e-03, -7.5914e-04,  6.0907e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6179e-03,  1.5691e-03,  2.3380e-03],\n",
      "          [ 4.3350e-03,  4.9360e-04,  1.0783e-03],\n",
      "          [ 4.1216e-03,  9.8356e-04,  5.1685e-04]],\n",
      "\n",
      "         [[ 1.5872e-03,  6.3771e-04,  1.2973e-03],\n",
      "          [ 6.7649e-04,  2.2815e-03,  8.7148e-04],\n",
      "          [ 7.1448e-04,  1.5041e-03,  1.1716e-03]],\n",
      "\n",
      "         [[ 3.6105e-03,  8.6202e-04,  1.9424e-03],\n",
      "          [ 1.7016e-03,  2.2873e-03,  2.3431e-03],\n",
      "          [ 3.1565e-03,  3.2395e-03,  4.2052e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5077e-03,  1.9422e-03,  1.4310e-03],\n",
      "          [ 1.4017e-03,  9.8683e-04,  1.8967e-03],\n",
      "          [ 6.6469e-04,  2.2208e-03,  2.3542e-03]],\n",
      "\n",
      "         [[ 6.3191e-03,  2.8834e-03,  4.5840e-03],\n",
      "          [ 3.9975e-03,  5.8655e-03,  5.2395e-03],\n",
      "          [ 5.4721e-03,  5.7961e-03,  4.7973e-03]],\n",
      "\n",
      "         [[ 2.1067e-03,  2.1213e-03,  1.8321e-03],\n",
      "          [ 1.2203e-03,  1.9890e-03,  3.0142e-03],\n",
      "          [ 4.9992e-04, -1.0284e-04,  4.8123e-03]]]], device='cuda:0')\n",
      "tensor([ 0.0135,  0.0131, -0.0007, -0.0030, -0.0079, -0.0069, -0.0071,  0.0084,\n",
      "         0.0130,  0.0070, -0.0046,  0.0142,  0.0010,  0.0017, -0.0043, -0.0042,\n",
      "        -0.0037, -0.0067, -0.0136, -0.0099,  0.0083, -0.0028,  0.0094,  0.0162,\n",
      "        -0.0032, -0.0254, -0.0078,  0.0098,  0.0197, -0.0027, -0.0038, -0.0078,\n",
      "        -0.0183,  0.0054,  0.0022, -0.0132, -0.0063,  0.0143,  0.0032, -0.0125,\n",
      "         0.0050,  0.0058, -0.0015, -0.0060,  0.0143,  0.0010, -0.0010, -0.0003,\n",
      "        -0.0103, -0.0035,  0.0099,  0.0093,  0.0068,  0.0126,  0.0049, -0.0140,\n",
      "         0.0037, -0.0069, -0.0094, -0.0034,  0.0053, -0.0104,  0.0020,  0.0074],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.1268e-03,  1.3898e-03,  1.2715e-03,  ...,  2.6626e-04,\n",
      "          2.3495e-03,  9.5877e-04],\n",
      "        [ 1.7088e-03,  1.2782e-03,  1.8330e-03,  ...,  1.7754e-03,\n",
      "         -2.8549e-04,  9.4301e-04],\n",
      "        ...,\n",
      "        [-1.3485e-03, -3.7240e-04, -7.5504e-04,  ..., -1.4085e-04,\n",
      "         -4.3893e-05, -5.8260e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "tensor([ 0.0000e+00,  1.2742e-02,  1.6517e-02,  9.2853e-04, -1.5672e-03,\n",
      "        -6.3674e-03, -1.0432e-03,  2.4866e-03, -1.3670e-02,  1.4254e-02,\n",
      "        -1.5367e-02, -5.8988e-03,  1.4555e-02,  7.2240e-03,  1.8948e-03,\n",
      "        -1.1496e-03,  1.5467e-02,  2.9244e-02, -2.0499e-03,  1.1381e-03,\n",
      "         3.5762e-03,  1.1298e-02,  1.1884e-03, -9.9099e-03, -4.7673e-03,\n",
      "         0.0000e+00,  6.2370e-03, -2.1015e-03, -3.0465e-03,  1.5009e-02,\n",
      "        -2.1423e-05, -4.1708e-02, -2.2761e-04, -5.9366e-03, -6.4660e-03,\n",
      "         2.6509e-04,  0.0000e+00,  2.6803e-02, -2.5925e-03, -1.1229e-03,\n",
      "         9.0947e-04, -1.5665e-02, -7.5924e-03, -8.7720e-03,  1.3652e-02,\n",
      "         5.3936e-04,  4.2806e-03, -3.4430e-03, -5.8747e-04, -3.1472e-03,\n",
      "        -1.7610e-03,  2.0048e-02,  1.7823e-02, -2.7882e-03,  4.2222e-03,\n",
      "         0.0000e+00, -4.1511e-03,  0.0000e+00, -1.9087e-02, -6.4209e-04,\n",
      "        -2.7417e-02,  1.3733e-03,  4.3908e-02, -7.7258e-04, -1.7497e-03,\n",
      "         0.0000e+00,  1.6781e-04,  1.4739e-02,  0.0000e+00, -1.3240e-02,\n",
      "         3.6900e-03, -1.8102e-03, -2.4328e-02, -8.5850e-03,  3.9401e-03,\n",
      "        -2.5717e-02,  1.9306e-04,  1.0871e-02, -3.9156e-03,  5.5567e-04,\n",
      "         0.0000e+00,  4.6177e-03, -1.0769e-04,  1.9815e-02, -1.1610e-02,\n",
      "         1.9327e-04, -2.3968e-02, -1.4337e-03,  8.1909e-03, -1.0594e-02,\n",
      "         1.3821e-02,  1.0172e-02, -2.5902e-04,  1.2397e-03, -2.1180e-02,\n",
      "        -3.1452e-03,  6.3682e-03, -1.5627e-02,  1.3529e-02, -1.2278e-03,\n",
      "        -7.8823e-05, -1.7240e-02, -1.4964e-02, -1.6020e-03,  2.0281e-03,\n",
      "         5.1270e-04, -2.9551e-02,  1.1884e-02,  2.8744e-03,  1.9261e-02,\n",
      "        -2.6313e-03, -2.3440e-04,  6.6630e-03,  1.3710e-04,  6.1649e-04,\n",
      "         5.6450e-04, -4.9635e-03,  1.0168e-03,  0.0000e+00,  4.7691e-02,\n",
      "         8.0625e-04,  1.5349e-02,  4.4896e-03,  0.0000e+00, -2.3991e-02,\n",
      "        -7.0248e-03,  0.0000e+00,  0.0000e+00], device='cuda:0')\n",
      "tensor([[ 0.0000, -0.0102, -0.0118,  ..., -0.0062,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0130, -0.0160,  ..., -0.0063,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0229, -0.0243,  ..., -0.0108,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000, -0.0148, -0.0186,  ..., -0.0127,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0074, -0.0114,  ..., -0.0083,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0094, -0.0070,  ..., -0.0056,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "tensor([-0.0625, -0.0781, -0.1562, -0.1406, -0.0938, -0.0312, -0.1562, -0.1250,\n",
      "        -0.0938, -0.0625], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    print(params.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "8\n",
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 9216])\n",
      "torch.Size([128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "def compute_grad(sample, target):\n",
    "    \n",
    "    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n",
    "    target = target.unsqueeze(0)\n",
    "    prediction = model(sample)\n",
    "    loss = loss_fn(prediction, target)\n",
    "\n",
    "    return torch.autograd.grad(loss, list(model.parameters()))\n",
    "\n",
    "\n",
    "def compute_sample_grads(data, targets):\n",
    "    \"\"\" manually process each sample with per sample gradient \"\"\"\n",
    "    sample_grads = [compute_grad(data[i], targets[i]) for i in range(batch_size)]\n",
    "    print(len(sample_grads))\n",
    "    print(len(sample_grads[0]))\n",
    "    # print(sample_grads[0])\n",
    "    print(sample_grads[0][0].shape)\n",
    "    print(sample_grads[0][1].shape)\n",
    "    print(sample_grads[0][2].shape)\n",
    "    print(sample_grads[0][3].shape)\n",
    "    print(sample_grads[0][4].shape)\n",
    "    print(sample_grads[0][5].shape)\n",
    "    print(sample_grads[0][6].shape)\n",
    "    print(sample_grads[0][7].shape)\n",
    "    sample_grads = zip(*sample_grads)\n",
    "    sample_grads = [torch.stack(shards) for shards in sample_grads]\n",
    "    return sample_grads\n",
    "\n",
    "per_sample_grads = compute_sample_grads(data, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(per_sample_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 1, 3, 3])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 64, 32, 3, 3])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 128, 9216])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 10, 128])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(per_sample_grads[0].shape) # conv\n",
    "print(per_sample_grads[1].shape) # conv bias\n",
    "print(per_sample_grads[2].shape) # conv2\n",
    "print(per_sample_grads[3].shape) # conv2 bias\n",
    "print(per_sample_grads[4].shape) # fc\n",
    "print(per_sample_grads[5].shape) # fc bias\n",
    "print(per_sample_grads[6].shape) # fc2\n",
    "print(per_sample_grads[7].shape) # fc2 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/deprecated.py:101: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional_with_buffers is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('make_functional_with_buffers', 'torch.func.functional_call')\n"
     ]
    }
   ],
   "source": [
    "from functorch import make_functional, make_functional_with_buffers\n",
    "fmodel, params, buffers = make_functional_with_buffers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionalModuleWithBuffers(\n",
       "  (stateless_model): SimpleCNN(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[[-0.3067,  0.1911, -0.0259],\n",
       "           [ 0.0543, -0.0036,  0.1500],\n",
       "           [-0.0356,  0.1882, -0.1082]]],\n",
       " \n",
       " \n",
       "         [[[-0.0662,  0.3118,  0.3192],\n",
       "           [ 0.0460, -0.2665,  0.0266],\n",
       "           [ 0.3210,  0.0676,  0.1080]]],\n",
       " \n",
       " \n",
       "         [[[-0.1433,  0.2671, -0.0225],\n",
       "           [-0.2632, -0.0259, -0.0070],\n",
       "           [ 0.2196, -0.0851,  0.1175]]],\n",
       " \n",
       " \n",
       "         [[[ 0.3034,  0.2779,  0.1222],\n",
       "           [ 0.0174, -0.1194,  0.1674],\n",
       "           [ 0.2485, -0.1795, -0.0161]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1628, -0.2604, -0.0544],\n",
       "           [-0.1605, -0.0911,  0.1703],\n",
       "           [-0.1051,  0.0225, -0.0319]]],\n",
       " \n",
       " \n",
       "         [[[-0.0515,  0.3208,  0.0474],\n",
       "           [ 0.2314, -0.2939, -0.0184],\n",
       "           [-0.1988, -0.3276,  0.1966]]],\n",
       " \n",
       " \n",
       "         [[[-0.0996,  0.1013,  0.2949],\n",
       "           [ 0.2433,  0.0225,  0.0268],\n",
       "           [-0.0105, -0.2354, -0.0066]]],\n",
       " \n",
       " \n",
       "         [[[ 0.3243,  0.2703,  0.1242],\n",
       "           [-0.3195,  0.1039,  0.1193],\n",
       "           [ 0.1685, -0.2389, -0.1963]]],\n",
       " \n",
       " \n",
       "         [[[-0.3064, -0.2636, -0.1499],\n",
       "           [-0.0620, -0.0555, -0.1744],\n",
       "           [ 0.2333, -0.0789, -0.0430]]],\n",
       " \n",
       " \n",
       "         [[[ 0.3191,  0.3015,  0.2886],\n",
       "           [ 0.2070,  0.0398, -0.1213],\n",
       "           [ 0.1789, -0.1876, -0.2480]]],\n",
       " \n",
       " \n",
       "         [[[-0.0140,  0.1558, -0.1258],\n",
       "           [ 0.0469,  0.0496, -0.2393],\n",
       "           [-0.3230, -0.1863,  0.0543]]],\n",
       " \n",
       " \n",
       "         [[[-0.0252, -0.1449, -0.2972],\n",
       "           [ 0.0791,  0.2392,  0.3084],\n",
       "           [ 0.0114,  0.2392,  0.0652]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0945, -0.0557, -0.1238],\n",
       "           [-0.2277,  0.0735,  0.0530],\n",
       "           [-0.0873,  0.2156, -0.0798]]],\n",
       " \n",
       " \n",
       "         [[[-0.2306, -0.2841,  0.0719],\n",
       "           [-0.2858,  0.0198, -0.2005],\n",
       "           [ 0.0053, -0.1327, -0.0130]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1699, -0.2909,  0.3154],\n",
       "           [-0.2233, -0.1158, -0.1378],\n",
       "           [ 0.0104,  0.1830, -0.3311]]],\n",
       " \n",
       " \n",
       "         [[[-0.2546, -0.1337, -0.3162],\n",
       "           [ 0.2553,  0.0551,  0.1999],\n",
       "           [ 0.2650, -0.0787,  0.0322]]],\n",
       " \n",
       " \n",
       "         [[[-0.0997, -0.2311,  0.0221],\n",
       "           [ 0.1637,  0.2694,  0.0852],\n",
       "           [-0.0583, -0.2929, -0.3152]]],\n",
       " \n",
       " \n",
       "         [[[-0.1562,  0.0135, -0.0700],\n",
       "           [-0.0396, -0.0487, -0.0828],\n",
       "           [ 0.0163, -0.1825, -0.1465]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1257, -0.1331,  0.2397],\n",
       "           [ 0.2066, -0.0078,  0.0821],\n",
       "           [-0.1171, -0.0475, -0.2149]]],\n",
       " \n",
       " \n",
       "         [[[-0.1762, -0.2474,  0.2191],\n",
       "           [-0.2213,  0.0599, -0.1458],\n",
       "           [ 0.0328,  0.2378, -0.1347]]],\n",
       " \n",
       " \n",
       "         [[[-0.2379,  0.2443,  0.1214],\n",
       "           [-0.2506,  0.1797,  0.2155],\n",
       "           [-0.0148,  0.0323, -0.1577]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1002,  0.0886, -0.1339],\n",
       "           [ 0.0667, -0.3018, -0.0394],\n",
       "           [ 0.2526,  0.1007,  0.1285]]],\n",
       " \n",
       " \n",
       "         [[[-0.3148, -0.2355, -0.2821],\n",
       "           [-0.0784,  0.1835, -0.0549],\n",
       "           [-0.1579,  0.1264,  0.2703]]],\n",
       " \n",
       " \n",
       "         [[[ 0.3166, -0.1087,  0.2802],\n",
       "           [ 0.1545, -0.0638, -0.0593],\n",
       "           [ 0.0609,  0.2666,  0.0786]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2640, -0.1434,  0.0807],\n",
       "           [ 0.1352,  0.1933,  0.2896],\n",
       "           [-0.2789, -0.2700,  0.2803]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2702,  0.1460,  0.3262],\n",
       "           [ 0.1433,  0.1865,  0.3301],\n",
       "           [ 0.2136, -0.2389, -0.2198]]],\n",
       " \n",
       " \n",
       "         [[[-0.0114,  0.1393, -0.0692],\n",
       "           [-0.2395, -0.0492,  0.2287],\n",
       "           [ 0.3113,  0.2308,  0.2706]]],\n",
       " \n",
       " \n",
       "         [[[-0.0360, -0.1214, -0.1410],\n",
       "           [ 0.0632, -0.3130, -0.2356],\n",
       "           [ 0.3166, -0.1215, -0.1189]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1889, -0.2246,  0.0941],\n",
       "           [ 0.1401, -0.3255, -0.3210],\n",
       "           [ 0.1555,  0.1121, -0.2377]]],\n",
       " \n",
       " \n",
       "         [[[-0.2361, -0.0099, -0.3003],\n",
       "           [ 0.3269, -0.3040,  0.1240],\n",
       "           [ 0.2191,  0.2963, -0.2276]]],\n",
       " \n",
       " \n",
       "         [[[-0.1406,  0.1044,  0.1275],\n",
       "           [ 0.2836, -0.1701, -0.1410],\n",
       "           [-0.2327, -0.0961,  0.2549]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1081, -0.0089,  0.0887],\n",
       "           [-0.0347,  0.1877,  0.2622],\n",
       "           [ 0.3087,  0.2984, -0.1643]]]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2397,  0.0661,  0.0583, -0.0226,  0.2847,  0.1007,  0.1470,  0.1122,\n",
       "          0.2067,  0.0639, -0.3283, -0.1998,  0.0094,  0.1446, -0.1080,  0.2189,\n",
       "         -0.2105,  0.0219,  0.0180,  0.0200,  0.1825, -0.0881,  0.3071,  0.1006,\n",
       "         -0.0070, -0.1968,  0.2914,  0.1099,  0.2768, -0.2374,  0.0263,  0.2226],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-0.0388,  0.0515,  0.0499],\n",
       "           [ 0.0035,  0.0156, -0.0075],\n",
       "           [ 0.0351, -0.0440, -0.0237]],\n",
       " \n",
       "          [[-0.0003, -0.0415, -0.0552],\n",
       "           [ 0.0587,  0.0279, -0.0167],\n",
       "           [ 0.0560,  0.0217, -0.0536]],\n",
       " \n",
       "          [[-0.0109,  0.0531, -0.0587],\n",
       "           [ 0.0507,  0.0544, -0.0421],\n",
       "           [ 0.0442, -0.0249, -0.0350]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0077,  0.0275,  0.0530],\n",
       "           [-0.0170, -0.0271,  0.0420],\n",
       "           [-0.0112, -0.0054,  0.0188]],\n",
       " \n",
       "          [[-0.0544,  0.0297,  0.0045],\n",
       "           [ 0.0097,  0.0024, -0.0414],\n",
       "           [ 0.0184,  0.0583,  0.0535]],\n",
       " \n",
       "          [[-0.0178,  0.0483,  0.0467],\n",
       "           [-0.0248,  0.0227, -0.0229],\n",
       "           [ 0.0033,  0.0420,  0.0106]]],\n",
       " \n",
       " \n",
       "         [[[-0.0089,  0.0159, -0.0373],\n",
       "           [ 0.0012,  0.0579,  0.0084],\n",
       "           [-0.0350,  0.0421, -0.0089]],\n",
       " \n",
       "          [[-0.0276, -0.0259, -0.0141],\n",
       "           [-0.0237, -0.0438,  0.0524],\n",
       "           [-0.0335,  0.0213,  0.0444]],\n",
       " \n",
       "          [[-0.0332,  0.0136,  0.0304],\n",
       "           [-0.0101,  0.0189,  0.0095],\n",
       "           [-0.0497, -0.0269,  0.0535]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0479, -0.0399, -0.0431],\n",
       "           [ 0.0341, -0.0095, -0.0332],\n",
       "           [-0.0010,  0.0015,  0.0213]],\n",
       " \n",
       "          [[-0.0105, -0.0214, -0.0092],\n",
       "           [-0.0056,  0.0339, -0.0194],\n",
       "           [-0.0467,  0.0405,  0.0161]],\n",
       " \n",
       "          [[ 0.0279,  0.0212,  0.0249],\n",
       "           [-0.0224,  0.0263,  0.0252],\n",
       "           [-0.0538, -0.0416,  0.0355]]],\n",
       " \n",
       " \n",
       "         [[[-0.0466,  0.0432,  0.0577],\n",
       "           [ 0.0413, -0.0552, -0.0199],\n",
       "           [-0.0425,  0.0423, -0.0007]],\n",
       " \n",
       "          [[-0.0473,  0.0500,  0.0105],\n",
       "           [ 0.0359, -0.0420, -0.0586],\n",
       "           [-0.0417, -0.0544,  0.0415]],\n",
       " \n",
       "          [[-0.0183,  0.0359,  0.0583],\n",
       "           [-0.0108, -0.0108, -0.0339],\n",
       "           [-0.0363, -0.0418,  0.0034]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0255,  0.0209, -0.0266],\n",
       "           [-0.0382,  0.0123,  0.0324],\n",
       "           [-0.0127,  0.0152,  0.0079]],\n",
       " \n",
       "          [[ 0.0321,  0.0467,  0.0477],\n",
       "           [ 0.0178,  0.0063,  0.0184],\n",
       "           [-0.0349,  0.0098, -0.0395]],\n",
       " \n",
       "          [[ 0.0564,  0.0201,  0.0059],\n",
       "           [-0.0147, -0.0094, -0.0128],\n",
       "           [-0.0077, -0.0156,  0.0157]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.0033,  0.0319, -0.0142],\n",
       "           [-0.0243, -0.0277, -0.0125],\n",
       "           [-0.0521,  0.0025,  0.0264]],\n",
       " \n",
       "          [[-0.0482, -0.0219,  0.0356],\n",
       "           [-0.0047,  0.0412,  0.0086],\n",
       "           [-0.0545, -0.0582,  0.0508]],\n",
       " \n",
       "          [[ 0.0149, -0.0209, -0.0473],\n",
       "           [ 0.0144, -0.0210,  0.0551],\n",
       "           [ 0.0006,  0.0239,  0.0529]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0068, -0.0041,  0.0446],\n",
       "           [ 0.0275,  0.0056, -0.0572],\n",
       "           [-0.0542, -0.0094,  0.0297]],\n",
       " \n",
       "          [[ 0.0183, -0.0349, -0.0031],\n",
       "           [ 0.0496,  0.0560,  0.0226],\n",
       "           [ 0.0081, -0.0383,  0.0158]],\n",
       " \n",
       "          [[-0.0198,  0.0531,  0.0270],\n",
       "           [ 0.0093,  0.0255, -0.0457],\n",
       "           [-0.0182,  0.0275,  0.0327]]],\n",
       " \n",
       " \n",
       "         [[[-0.0169, -0.0517,  0.0251],\n",
       "           [ 0.0024, -0.0388, -0.0445],\n",
       "           [ 0.0096, -0.0323, -0.0118]],\n",
       " \n",
       "          [[-0.0511,  0.0111, -0.0375],\n",
       "           [ 0.0409,  0.0174, -0.0301],\n",
       "           [ 0.0035,  0.0061,  0.0194]],\n",
       " \n",
       "          [[ 0.0025,  0.0531,  0.0506],\n",
       "           [-0.0087, -0.0285, -0.0147],\n",
       "           [ 0.0372,  0.0563,  0.0075]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0117,  0.0174, -0.0402],\n",
       "           [-0.0467,  0.0558,  0.0368],\n",
       "           [ 0.0384,  0.0023, -0.0192]],\n",
       " \n",
       "          [[ 0.0270, -0.0007, -0.0453],\n",
       "           [ 0.0158,  0.0032,  0.0401],\n",
       "           [-0.0023, -0.0341,  0.0476]],\n",
       " \n",
       "          [[ 0.0381, -0.0205,  0.0326],\n",
       "           [-0.0436,  0.0355, -0.0317],\n",
       "           [ 0.0122,  0.0134,  0.0108]]],\n",
       " \n",
       " \n",
       "         [[[-0.0326, -0.0251,  0.0099],\n",
       "           [-0.0297,  0.0464,  0.0150],\n",
       "           [-0.0073,  0.0097, -0.0504]],\n",
       " \n",
       "          [[ 0.0243,  0.0102, -0.0075],\n",
       "           [-0.0527, -0.0520,  0.0499],\n",
       "           [ 0.0249, -0.0339,  0.0046]],\n",
       " \n",
       "          [[-0.0102,  0.0424,  0.0204],\n",
       "           [-0.0246, -0.0194,  0.0332],\n",
       "           [-0.0323,  0.0059,  0.0532]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0243, -0.0521, -0.0200],\n",
       "           [ 0.0185,  0.0102, -0.0158],\n",
       "           [ 0.0112,  0.0133,  0.0283]],\n",
       " \n",
       "          [[-0.0240, -0.0405,  0.0053],\n",
       "           [-0.0285, -0.0495,  0.0511],\n",
       "           [ 0.0130, -0.0317,  0.0383]],\n",
       " \n",
       "          [[ 0.0359, -0.0395,  0.0092],\n",
       "           [-0.0566, -0.0311, -0.0441],\n",
       "           [ 0.0281, -0.0047, -0.0359]]]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0490, -0.0586,  0.0464,  0.0321,  0.0564,  0.0448,  0.0545,  0.0034,\n",
       "          0.0461,  0.0154,  0.0170, -0.0055, -0.0046,  0.0142, -0.0123,  0.0367,\n",
       "          0.0111,  0.0485, -0.0113, -0.0396, -0.0504,  0.0094, -0.0503,  0.0258,\n",
       "         -0.0213,  0.0556, -0.0514, -0.0459,  0.0416,  0.0196, -0.0448, -0.0344,\n",
       "          0.0121,  0.0353,  0.0371,  0.0206,  0.0323,  0.0192, -0.0130,  0.0342,\n",
       "          0.0372, -0.0213,  0.0500,  0.0275,  0.0506, -0.0154,  0.0031, -0.0582,\n",
       "         -0.0353, -0.0180,  0.0151, -0.0281, -0.0528, -0.0187, -0.0324, -0.0072,\n",
       "          0.0581, -0.0433, -0.0009, -0.0482,  0.0484, -0.0314, -0.0343,  0.0336],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0074, -0.0032, -0.0017,  ..., -0.0026,  0.0083,  0.0069],\n",
       "         [-0.0034,  0.0044, -0.0093,  ...,  0.0101,  0.0057, -0.0014],\n",
       "         [-0.0023,  0.0017, -0.0004,  ..., -0.0077,  0.0038, -0.0017],\n",
       "         ...,\n",
       "         [-0.0037,  0.0076, -0.0015,  ..., -0.0021,  0.0014, -0.0030],\n",
       "         [ 0.0063, -0.0063,  0.0002,  ...,  0.0076, -0.0048, -0.0034],\n",
       "         [ 0.0025, -0.0056,  0.0028,  ..., -0.0087,  0.0082, -0.0091]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 3.2899e-03, -4.6095e-03,  6.1196e-03,  9.4985e-03, -1.8686e-03,\n",
       "          1.0290e-02,  1.0268e-02, -5.7700e-04,  6.4273e-03, -1.0370e-03,\n",
       "          7.7245e-03,  6.0259e-03,  9.3420e-03,  5.9257e-03,  8.4295e-03,\n",
       "          4.8200e-03, -1.5416e-03, -7.8033e-03, -1.5367e-04,  1.2757e-03,\n",
       "          5.8586e-04,  6.9228e-03, -1.6603e-03,  9.7651e-03, -8.6188e-03,\n",
       "         -4.5253e-03, -6.6604e-03, -4.5265e-03,  4.4565e-03,  1.8061e-03,\n",
       "          3.5398e-03, -5.2568e-03,  7.9680e-04,  7.5018e-04, -7.0554e-03,\n",
       "         -6.0512e-03, -2.8278e-03, -4.1192e-03, -5.5550e-03, -8.2866e-03,\n",
       "         -9.1691e-03, -8.9059e-03,  9.6988e-03,  3.2682e-03,  1.9589e-03,\n",
       "         -7.2004e-03, -5.8561e-03,  9.9833e-03,  8.9467e-03,  3.6724e-03,\n",
       "          6.8610e-03, -1.2421e-03,  2.6185e-03, -7.0904e-03, -6.4510e-03,\n",
       "         -4.1539e-03, -6.8975e-03,  7.4263e-03, -9.5405e-03,  3.5269e-03,\n",
       "          3.5004e-03, -9.1079e-03,  8.8401e-03,  1.0808e-05, -4.2065e-03,\n",
       "          2.7552e-03,  4.3145e-03,  1.0074e-02,  1.0318e-02,  9.4947e-03,\n",
       "          7.8811e-03, -2.3278e-03,  2.2268e-03,  6.8709e-03, -1.8409e-03,\n",
       "          1.2181e-03,  7.9651e-03, -5.2065e-03,  1.7998e-03,  3.7607e-03,\n",
       "          4.9195e-03, -6.0968e-03,  1.4357e-04, -4.4673e-03,  3.7234e-03,\n",
       "          6.8211e-03,  5.9215e-03, -7.1869e-03,  6.8421e-03, -5.2198e-03,\n",
       "         -8.0117e-03, -5.1719e-04, -7.4131e-03,  5.0222e-04, -2.6883e-03,\n",
       "         -7.8543e-03,  9.8656e-03, -1.6412e-03,  8.6024e-03, -7.0697e-03,\n",
       "         -4.4313e-03, -4.0410e-03, -7.4519e-03,  2.2935e-03,  6.4943e-03,\n",
       "         -6.0532e-03,  2.7989e-03,  4.9223e-03,  3.6201e-03,  6.2107e-03,\n",
       "         -5.6586e-03,  8.1490e-03,  2.8531e-03,  2.6947e-03,  9.6349e-03,\n",
       "          4.6493e-03, -2.7878e-03,  6.7230e-03,  7.0261e-03,  7.4750e-03,\n",
       "          3.0804e-04, -2.0073e-03,  6.0465e-03,  5.4595e-04,  8.1143e-03,\n",
       "         -4.3068e-03,  8.6956e-03, -2.6593e-03], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0216,  0.0624,  0.0342,  ..., -0.0713,  0.0843, -0.0313],\n",
       "         [-0.0535,  0.0265,  0.0132,  ..., -0.0211,  0.0103,  0.0830],\n",
       "         [-0.0265, -0.0376, -0.0028,  ..., -0.0145, -0.0244,  0.0744],\n",
       "         ...,\n",
       "         [ 0.0303,  0.0253, -0.0501,  ...,  0.0687, -0.0735, -0.0369],\n",
       "         [-0.0800, -0.0718,  0.0701,  ..., -0.0163,  0.0351, -0.0530],\n",
       "         [-0.0483, -0.0432, -0.0829,  ..., -0.0382,  0.0443, -0.0109]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0804, -0.0466,  0.0237,  0.0133, -0.0852,  0.0810, -0.0691, -0.0077,\n",
       "         -0.0071,  0.0629], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_stateless_model (params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = fmodel(params, buffers, batch) \n",
    "    loss = loss_fn(predictions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ft_compute_grad \u001b[39m=\u001b[39m grad(compute_loss_stateless_model)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "ft_compute_grad = grad(compute_loss_stateless_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    }
   ],
   "source": [
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, data, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([64, 32, 1, 3, 3])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 64, 32, 3, 3])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 128, 9216])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 10, 128])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(len(ft_per_sample_grads))\n",
    "print(ft_per_sample_grads[0].shape)\n",
    "print(ft_per_sample_grads[1].shape)\n",
    "print(ft_per_sample_grads[2].shape)\n",
    "print(ft_per_sample_grads[3].shape)\n",
    "print(ft_per_sample_grads[4].shape)\n",
    "print(ft_per_sample_grads[5].shape)\n",
    "print(ft_per_sample_grads[6].shape)\n",
    "print(ft_per_sample_grads[7].shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable function object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunc\u001b[39;00m \u001b[39mimport\u001b[39;00m functionalize\n\u001b[0;32m----> 2\u001b[0m func_model, params \u001b[39m=\u001b[39m functionalize(model)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable function object"
     ]
    }
   ],
   "source": [
    "from torch.func import functionalize\n",
    "func_model, params = functionalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "functional_call() missing 2 required positional arguments: 'parameter_and_buffer_dicts' and 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunc\u001b[39;00m \u001b[39mimport\u001b[39;00m functional_call\n\u001b[0;32m----> 2\u001b[0m fmodel, params, buffers \u001b[39m=\u001b[39m functional_call(model)\n",
      "\u001b[0;31mTypeError\u001b[0m: functional_call() missing 2 required positional arguments: 'parameter_and_buffer_dicts' and 'args'"
     ]
    }
   ],
   "source": [
    "from torch.func import functional_call\n",
    "output = functional_call(model.named_parameters(), model.named_buffers(),\n",
    "                         sample, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/.conda/envs/test_env/lib/python3.10/site-packages/glfw/__init__.py:916: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing import Any, Dict, Optional, Tuple, OrderedDict\n",
    "import random\n",
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "from hydra.utils import get_original_cwd, to_absolute_path\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import ray\n",
    "\n",
    "import envpool\n",
    "# import gymnasium as gym\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "from gym.spaces.dict import Dict as GymDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.func\n",
    "from src.functional_agent import FuncPPOAgentSep\n",
    "from src.utils import set_seed, make_batched_env\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"/home/kukjin/kukjin/Projects/MultiEnvRL/DARL_refactored/configs/ppo_trainer.yaml\"\n",
    "nn_cfg_path = \"/home/kukjin/kukjin/Projects/MultiEnvRL/DARL_refactored/configs/nn/nn.yaml\"\n",
    "ppo_cfg_path = \"/home/kukjin/kukjin/Projects/MultiEnvRL/DARL_refactored/configs/ppo/ppo.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defaults': [{'ppo': 'ppo'}, {'nn': 'nn'}, '_self_'], 'hydra': {'run': {'dir': 'outputs/${now:%Y-%m-%d/%H-%M-%S}'}}, 'experiment': {'env_ids': ['CartPole-v1'], 'seed': 42, 'max_episode_steps': 1000, 'num_rollout_steps': 128, 'num_train_envs': 64, 'num_envs': 64, 'total_timesteps': 10000000, 'save_ckpt': False, 'num_checkpoints': 20, 'print_interval': 100, 'stop_after_epochs': 500, 'capture_video': False, 'device': 0, 'cuda': True, 'torch_deterministic': True, 'resume': False, 'resume_update_idx': 0, 'resume_dir': 'None'}, 'evaluation': {'eval_seed': 3142, 'every': 8, 'num_eval': 5, 'num_test_envs': 5}, 'wandb': {'mode': 'online', 'project': 'DomainAgnosticRL', 'entity': None, 'name': None, 'group': None, 'tags': None, 'notes': None}, 'paths': {'dir': 'outputs/${now:%Y-%m-%d/%H-%M-%S}', 'log': 'outputs/${now:%Y-%m-%d/%H-%M-%S}/runs', 'video': 'outputs/${now:%Y-%m-%d/%H-%M-%S}/videos', 'checkpoints': 'outputs/${now:%Y-%m-%d/%H-%M-%S}/checkpoints', 'src': 'outputs/${now:%Y-%m-%d/%H-%M-%S}/src', 'scripts': 'outputs/${now:%Y-%m-%d/%H-%M-%S}/scripts'}, 'nn': {'lr': {'actor': 0.0001, 'critic': 0.0005}, 'weight_decay': {'actor': 0.0, 'critic': 0.0}, 'encoder': {'type': 'rnn', 'final_activation': 'identity', 'initializer': 'uniform', 'hidden_dim': 128}, 'decoder': {'type': 'rnn', 'final_activation': 'identity', 'initializer': 'uniform'}, 'shared_networks': {'type': 'transformer', 'hidden_dim': 128, 'expansion_factor': 128, 'output_dim': 128, 'dropout': 0.0, 'activation': 'tanh'}, 'transformer': {'num_heads': 1, 'num_encoder_layers': 1, 'dim_feedforward': 128, 'dropout': 0.2, 'activation': 'gelu'}}, 'ppo': {'clip_logstd': False, 'clip_logstd_max': 3, 'clip_logstd_min': -5, 'anneal_lr': False, 'update_epochs': 10, 'num_minibatches': 8, 'batch_size': 'None', 'minibatch_size': 'None', 'max_grad_norm': 0.5, 'norm_adv': True, 'const_coef': 0.0, 'clip_coef': 0.2, 'clip_vloss': True, 'ent_coef': 0.0, 'vf_coef': 0.5, 'gamma': 0.99, 'gae_lambda': 0.95}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = OmegaConf.load(cfg_path)\n",
    "nn_cfg = OmegaConf.load(nn_cfg_path)\n",
    "ppo_cfg = OmegaConf.load(ppo_cfg_path)\n",
    "cfg.nn = nn_cfg\n",
    "cfg.ppo = ppo_cfg\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': {'actor': 0.0001, 'critic': 0.0005}, 'weight_decay': {'actor': 0.0, 'critic': 0.0}, 'encoder': {'type': 'rnn', 'final_activation': 'identity', 'initializer': 'uniform', 'hidden_dim': 128}, 'decoder': {'type': 'rnn', 'final_activation': 'identity', 'initializer': 'uniform'}, 'shared_networks': {'type': 'transformer', 'hidden_dim': 128, 'expansion_factor': 128, 'output_dim': 128, 'dropout': 0.0, 'activation': 'tanh'}, 'transformer': {'num_heads': 1, 'num_encoder_layers': 1, 'dim_feedforward': 128, 'dropout': 0.2, 'activation': 'gelu'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clip_logstd': False, 'clip_logstd_max': 3, 'clip_logstd_min': -5, 'anneal_lr': False, 'update_epochs': 10, 'num_minibatches': 8, 'batch_size': 'None', 'minibatch_size': 'None', 'max_grad_norm': 0.5, 'norm_adv': True, 'const_coef': 0.0, 'clip_coef': 0.2, 'clip_vloss': True, 'ent_coef': 0.0, 'vf_coef': 0.5, 'gamma': 0.99, 'gae_lambda': 0.95}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cfg.experiment.device\n",
    "cfg.experiment.num_train_envs = 2\n",
    "cfg.experiment.num_envs = 2\n",
    "\n",
    "\n",
    "agent = FuncPPOAgentSep(cfg)\n",
    "total_num_params = sum([np.prod(p.size()) for p in agent.parameters()])\n",
    "actor_params = dict(agent.actor.named_parameters())\n",
    "critic_params = dict(agent.critic.named_parameters())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['obs_encoder.rnn.weight_ih_l0', 'obs_encoder.rnn.weight_hh_l0', 'obs_encoder.rnn.weight_ih_l0_reverse', 'obs_encoder.rnn.weight_hh_l0_reverse', 'obs_encoder.transformer_encoder.layers.0.self_attn.in_proj_weight', 'obs_encoder.transformer_encoder.layers.0.self_attn.in_proj_bias', 'obs_encoder.transformer_encoder.layers.0.self_attn.out_proj.weight', 'obs_encoder.transformer_encoder.layers.0.self_attn.out_proj.bias', 'obs_encoder.transformer_encoder.layers.0.linear1.weight', 'obs_encoder.transformer_encoder.layers.0.linear1.bias', 'obs_encoder.transformer_encoder.layers.0.linear2.weight', 'obs_encoder.transformer_encoder.layers.0.linear2.bias', 'obs_encoder.transformer_encoder.layers.0.norm1.weight', 'obs_encoder.transformer_encoder.layers.0.norm1.bias', 'obs_encoder.transformer_encoder.layers.0.norm2.weight', 'obs_encoder.transformer_encoder.layers.0.norm2.bias', 'policy_mean_decoder.rnn.weight_ih_l0', 'policy_mean_decoder.rnn.weight_hh_l0', 'policy_mean_decoder.rnn.weight_ih_l0_reverse', 'policy_mean_decoder.rnn.weight_hh_l0_reverse', 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight', 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias', 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight', 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias', 'policy_mean_decoder.transformer_encoder.layers.0.linear1.weight', 'policy_mean_decoder.transformer_encoder.layers.0.linear1.bias', 'policy_mean_decoder.transformer_encoder.layers.0.linear2.weight', 'policy_mean_decoder.transformer_encoder.layers.0.linear2.bias', 'policy_mean_decoder.transformer_encoder.layers.0.norm1.weight', 'policy_mean_decoder.transformer_encoder.layers.0.norm1.bias', 'policy_mean_decoder.transformer_encoder.layers.0.norm2.weight', 'policy_mean_decoder.transformer_encoder.layers.0.norm2.bias', 'policy_logstd_decoder.rnn.weight_ih_l0', 'policy_logstd_decoder.rnn.weight_hh_l0', 'policy_logstd_decoder.rnn.weight_ih_l0_reverse', 'policy_logstd_decoder.rnn.weight_hh_l0_reverse', 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight', 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias', 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight', 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias', 'policy_logstd_decoder.transformer_encoder.layers.0.linear1.weight', 'policy_logstd_decoder.transformer_encoder.layers.0.linear1.bias', 'policy_logstd_decoder.transformer_encoder.layers.0.linear2.weight', 'policy_logstd_decoder.transformer_encoder.layers.0.linear2.bias', 'policy_logstd_decoder.transformer_encoder.layers.0.norm1.weight', 'policy_logstd_decoder.transformer_encoder.layers.0.norm1.bias', 'policy_logstd_decoder.transformer_encoder.layers.0.norm2.weight', 'policy_logstd_decoder.transformer_encoder.layers.0.norm2.bias', 'policy_prob_decoder.rnn.weight_ih_l0', 'policy_prob_decoder.rnn.weight_hh_l0', 'policy_prob_decoder.rnn.weight_ih_l0_reverse', 'policy_prob_decoder.rnn.weight_hh_l0_reverse', 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight', 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias', 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight', 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias', 'policy_prob_decoder.transformer_encoder.layers.0.linear1.weight', 'policy_prob_decoder.transformer_encoder.layers.0.linear1.bias', 'policy_prob_decoder.transformer_encoder.layers.0.linear2.weight', 'policy_prob_decoder.transformer_encoder.layers.0.linear2.bias', 'policy_prob_decoder.transformer_encoder.layers.0.norm1.weight', 'policy_prob_decoder.transformer_encoder.layers.0.norm1.bias', 'policy_prob_decoder.transformer_encoder.layers.0.norm2.weight', 'policy_prob_decoder.transformer_encoder.layers.0.norm2.bias'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_policy_grad = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in actor_params.keys():\n",
    "    total_policy_grad.setdefault(key, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obs_encoder.rnn.weight_ih_l0': 0,\n",
       " 'obs_encoder.rnn.weight_hh_l0': 0,\n",
       " 'obs_encoder.rnn.weight_ih_l0_reverse': 0,\n",
       " 'obs_encoder.rnn.weight_hh_l0_reverse': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.self_attn.in_proj_weight': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.self_attn.in_proj_bias': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.self_attn.out_proj.weight': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.self_attn.out_proj.bias': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.linear1.weight': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.linear1.bias': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.linear2.weight': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.linear2.bias': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.norm1.weight': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.norm1.bias': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.norm2.weight': 0,\n",
       " 'obs_encoder.transformer_encoder.layers.0.norm2.bias': 0,\n",
       " 'policy_mean_decoder.rnn.weight_ih_l0': 0,\n",
       " 'policy_mean_decoder.rnn.weight_hh_l0': 0,\n",
       " 'policy_mean_decoder.rnn.weight_ih_l0_reverse': 0,\n",
       " 'policy_mean_decoder.rnn.weight_hh_l0_reverse': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.linear1.weight': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.linear1.bias': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.linear2.weight': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.linear2.bias': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.norm1.weight': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.norm1.bias': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.norm2.weight': 0,\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.norm2.bias': 0,\n",
       " 'policy_logstd_decoder.rnn.weight_ih_l0': 0,\n",
       " 'policy_logstd_decoder.rnn.weight_hh_l0': 0,\n",
       " 'policy_logstd_decoder.rnn.weight_ih_l0_reverse': 0,\n",
       " 'policy_logstd_decoder.rnn.weight_hh_l0_reverse': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.linear1.weight': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.linear1.bias': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.linear2.weight': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.linear2.bias': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.norm1.weight': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.norm1.bias': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.norm2.weight': 0,\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.norm2.bias': 0,\n",
       " 'policy_prob_decoder.rnn.weight_ih_l0': 0,\n",
       " 'policy_prob_decoder.rnn.weight_hh_l0': 0,\n",
       " 'policy_prob_decoder.rnn.weight_ih_l0_reverse': 0,\n",
       " 'policy_prob_decoder.rnn.weight_hh_l0_reverse': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.linear1.weight': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.linear1.bias': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.linear2.weight': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.linear2.bias': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.norm1.weight': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.norm1.bias': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.norm2.weight': 0,\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.norm2.bias': 0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_policy_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_grads(params, total_grad_dict, env_grad_dict):\n",
    "    for key in params.keys():\n",
    "        total_grad_dict[key] += env_grad_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = Box(low=-1.0, high=1.0, shape=(4,))\n",
    "obs = torch.randn([32, 17])\n",
    "actions = torch.randn([32, 4])\n",
    "old_log_probs = torch.randn([32,])\n",
    "advantages = torch.randn([32,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, log_prob, entropy, value = agent.get_action_and_value(\n",
    "    actor_params,\n",
    "    critic_params,\n",
    "    action_space,\n",
    "    obs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/deprecated.py:66: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.grad_and_value is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.grad_and_value instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('grad_and_value')\n"
     ]
    }
   ],
   "source": [
    "policy_grad, policy_loss, entropy_loss = agent.compute_grad_and_policy_loss(actor_params,\n",
    "                                   action_space,\n",
    "                                   obs,\n",
    "                                   actions,\n",
    "                                   old_log_probs,\n",
    "                                   advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obs_encoder.rnn.weight_ih_l0': tensor([[ 4.1840e-06],\n",
       "         [ 5.6890e-07],\n",
       "         [ 8.1663e-07],\n",
       "         [-5.8885e-08],\n",
       "         [ 4.9224e-07],\n",
       "         [ 1.2302e-07],\n",
       "         [ 1.2471e-06],\n",
       "         [-7.0023e-07],\n",
       "         [-5.0591e-08],\n",
       "         [ 1.0565e-06],\n",
       "         [ 1.2605e-07],\n",
       "         [-3.4430e-07],\n",
       "         [-2.7008e-06],\n",
       "         [-5.7405e-07],\n",
       "         [ 3.6546e-06],\n",
       "         [ 2.1551e-07],\n",
       "         [ 1.2894e-07],\n",
       "         [-8.7099e-08],\n",
       "         [-1.9651e-07],\n",
       "         [-4.0859e-07],\n",
       "         [ 6.2901e-10],\n",
       "         [-1.8148e-07],\n",
       "         [-2.0902e-07],\n",
       "         [ 4.3948e-07],\n",
       "         [ 9.8775e-08],\n",
       "         [-4.1217e-08],\n",
       "         [-8.3186e-09],\n",
       "         [-1.3312e-06],\n",
       "         [ 3.6558e-08],\n",
       "         [-8.2609e-07],\n",
       "         [ 1.0150e-07],\n",
       "         [ 7.4433e-07],\n",
       "         [-1.3070e-06],\n",
       "         [-2.5546e-07],\n",
       "         [ 5.6925e-07],\n",
       "         [ 6.8467e-07],\n",
       "         [-6.2044e-07],\n",
       "         [ 2.1496e-06],\n",
       "         [-7.5377e-07],\n",
       "         [ 4.9287e-08],\n",
       "         [-4.8800e-07],\n",
       "         [-1.8917e-07],\n",
       "         [ 7.3099e-07],\n",
       "         [-7.8186e-07],\n",
       "         [-3.0186e-07],\n",
       "         [-3.1618e-07],\n",
       "         [-1.3544e-08],\n",
       "         [ 2.2924e-07],\n",
       "         [-6.4584e-07],\n",
       "         [-4.4637e-07],\n",
       "         [ 3.0876e-07],\n",
       "         [-1.3676e-07],\n",
       "         [ 4.4702e-08],\n",
       "         [ 2.7936e-07],\n",
       "         [ 2.9449e-06],\n",
       "         [-7.5304e-07],\n",
       "         [-5.5519e-07],\n",
       "         [ 3.1189e-07],\n",
       "         [-5.1459e-07],\n",
       "         [-3.0737e-07],\n",
       "         [-6.6559e-07],\n",
       "         [ 1.1618e-06],\n",
       "         [ 7.2432e-07],\n",
       "         [-4.1739e-07],\n",
       "         [ 2.1728e-07],\n",
       "         [ 3.6682e-07],\n",
       "         [-1.7038e-06],\n",
       "         [-1.1340e-07],\n",
       "         [ 7.8993e-09],\n",
       "         [-1.7399e-07],\n",
       "         [ 8.6453e-08],\n",
       "         [ 3.4159e-07],\n",
       "         [-1.7044e-07],\n",
       "         [-7.9508e-07],\n",
       "         [-4.9145e-08],\n",
       "         [ 3.1399e-07],\n",
       "         [-1.7223e-07],\n",
       "         [ 1.9688e-07],\n",
       "         [-2.8357e-06],\n",
       "         [-3.8359e-07],\n",
       "         [-1.2201e-06],\n",
       "         [-1.5202e-07],\n",
       "         [-1.2361e-06],\n",
       "         [ 1.6380e-06],\n",
       "         [ 1.7447e-07],\n",
       "         [ 7.2216e-07],\n",
       "         [ 9.7847e-07],\n",
       "         [-5.9910e-08],\n",
       "         [-9.1899e-07],\n",
       "         [-5.0980e-07],\n",
       "         [-2.0966e-07],\n",
       "         [-1.5016e-07],\n",
       "         [ 5.9626e-07],\n",
       "         [ 7.7018e-08],\n",
       "         [ 5.8274e-08],\n",
       "         [ 9.0236e-07],\n",
       "         [-1.6006e-07],\n",
       "         [ 1.1866e-07],\n",
       "         [ 1.0393e-07],\n",
       "         [ 6.0319e-08],\n",
       "         [-1.0027e-07],\n",
       "         [ 3.4686e-07],\n",
       "         [-4.3110e-07],\n",
       "         [ 8.6426e-07],\n",
       "         [ 3.6857e-07],\n",
       "         [-7.7429e-07],\n",
       "         [-8.5625e-07],\n",
       "         [-2.8862e-07],\n",
       "         [-4.9926e-07],\n",
       "         [-8.5691e-07],\n",
       "         [-3.7628e-07],\n",
       "         [ 3.1940e-06],\n",
       "         [ 2.3808e-06],\n",
       "         [-4.4105e-08],\n",
       "         [ 6.8295e-08],\n",
       "         [-1.5308e-07],\n",
       "         [ 1.8035e-07],\n",
       "         [-1.1772e-07],\n",
       "         [ 2.8133e-07],\n",
       "         [-5.4505e-08],\n",
       "         [ 2.1044e-07],\n",
       "         [-3.3094e-07],\n",
       "         [-5.5980e-07],\n",
       "         [-2.2037e-07],\n",
       "         [ 3.3444e-07],\n",
       "         [-4.2791e-08],\n",
       "         [-2.9093e-07],\n",
       "         [ 3.7704e-07],\n",
       "         [ 1.8564e-05],\n",
       "         [-1.1985e-06],\n",
       "         [-5.6306e-06],\n",
       "         [-1.1533e-05],\n",
       "         [ 3.0929e-05],\n",
       "         [ 9.4795e-07],\n",
       "         [-3.0630e-06],\n",
       "         [ 1.7264e-05],\n",
       "         [-3.9479e-07],\n",
       "         [-2.6729e-06],\n",
       "         [-2.7029e-05],\n",
       "         [-1.4149e-06],\n",
       "         [ 1.1478e-05],\n",
       "         [-9.5869e-07],\n",
       "         [ 7.9563e-06],\n",
       "         [ 6.1512e-05],\n",
       "         [ 5.9064e-06],\n",
       "         [ 5.1844e-06],\n",
       "         [ 1.7429e-06],\n",
       "         [-1.7331e-05],\n",
       "         [ 1.8119e-05],\n",
       "         [ 2.2898e-06],\n",
       "         [ 2.0573e-06],\n",
       "         [-7.9816e-06],\n",
       "         [ 5.5720e-06],\n",
       "         [ 5.6298e-09],\n",
       "         [-1.5490e-06],\n",
       "         [-1.2891e-05],\n",
       "         [ 8.7450e-06],\n",
       "         [ 1.1952e-05],\n",
       "         [ 1.7411e-05],\n",
       "         [ 1.4806e-05],\n",
       "         [ 4.0281e-05],\n",
       "         [ 1.3821e-06],\n",
       "         [ 2.3624e-06],\n",
       "         [-1.3490e-05],\n",
       "         [ 1.7578e-05],\n",
       "         [-1.9413e-05],\n",
       "         [-1.2430e-06],\n",
       "         [ 8.8486e-06],\n",
       "         [-1.6588e-06],\n",
       "         [-1.0452e-05],\n",
       "         [-2.3144e-06],\n",
       "         [-2.0340e-05],\n",
       "         [-1.2952e-05],\n",
       "         [-1.4453e-05],\n",
       "         [-1.0064e-05],\n",
       "         [ 2.1691e-05],\n",
       "         [-1.1509e-05],\n",
       "         [ 5.6182e-07],\n",
       "         [-6.8719e-06],\n",
       "         [ 9.6963e-06],\n",
       "         [-8.1653e-06],\n",
       "         [ 1.7368e-05],\n",
       "         [ 1.6213e-05],\n",
       "         [-8.3727e-06],\n",
       "         [-2.2448e-05],\n",
       "         [-2.8520e-06],\n",
       "         [ 5.3157e-06],\n",
       "         [ 2.6474e-06],\n",
       "         [ 3.9224e-05],\n",
       "         [-1.0781e-05],\n",
       "         [ 4.7334e-06],\n",
       "         [ 3.2772e-07],\n",
       "         [ 3.0685e-05],\n",
       "         [ 3.4883e-06],\n",
       "         [ 3.0800e-05],\n",
       "         [ 3.3854e-06],\n",
       "         [-8.7084e-06],\n",
       "         [ 1.1449e-05],\n",
       "         [-1.1847e-05],\n",
       "         [-7.5932e-07],\n",
       "         [-5.5368e-06],\n",
       "         [ 1.9083e-05],\n",
       "         [ 8.4447e-06],\n",
       "         [ 7.5179e-06],\n",
       "         [ 1.6843e-05],\n",
       "         [-7.3538e-06],\n",
       "         [-3.3723e-06],\n",
       "         [-2.6574e-06],\n",
       "         [-3.7157e-05],\n",
       "         [-7.5797e-06],\n",
       "         [ 1.2871e-05],\n",
       "         [ 6.7558e-06],\n",
       "         [ 2.9560e-05],\n",
       "         [-7.5653e-06],\n",
       "         [ 4.7483e-06],\n",
       "         [ 6.6563e-07],\n",
       "         [ 3.7641e-05],\n",
       "         [ 2.0607e-05],\n",
       "         [ 3.6822e-05],\n",
       "         [-2.0942e-05],\n",
       "         [ 1.9352e-07],\n",
       "         [ 6.0583e-06],\n",
       "         [ 9.3723e-07],\n",
       "         [ 9.1639e-06],\n",
       "         [-3.2872e-08],\n",
       "         [ 7.2520e-06],\n",
       "         [-1.9425e-05],\n",
       "         [ 3.1764e-05],\n",
       "         [ 2.3942e-06],\n",
       "         [ 7.0346e-06],\n",
       "         [-2.3737e-05],\n",
       "         [ 3.3103e-06],\n",
       "         [ 7.4511e-06],\n",
       "         [-1.2426e-05],\n",
       "         [-1.6213e-05],\n",
       "         [ 6.0260e-06],\n",
       "         [-4.7041e-05],\n",
       "         [-1.9717e-05],\n",
       "         [-2.3967e-06],\n",
       "         [-3.4683e-05],\n",
       "         [-5.4706e-07],\n",
       "         [-1.5102e-05],\n",
       "         [-7.5078e-06],\n",
       "         [ 1.1913e-05],\n",
       "         [-3.7043e-07],\n",
       "         [ 2.1422e-06],\n",
       "         [ 2.0149e-06],\n",
       "         [ 5.3500e-06],\n",
       "         [ 4.5715e-06],\n",
       "         [ 1.5855e-05],\n",
       "         [ 4.1900e-05],\n",
       "         [-1.2152e-05],\n",
       "         [-2.3879e-05],\n",
       "         [-9.8849e-07],\n",
       "         [ 5.2178e-06],\n",
       "         [-4.0645e-06],\n",
       "         [ 4.4624e-04],\n",
       "         [-1.5946e-04],\n",
       "         [ 2.6402e-04],\n",
       "         [ 5.6523e-04],\n",
       "         [ 9.2943e-05],\n",
       "         [-3.0775e-04],\n",
       "         [-2.3988e-04],\n",
       "         [-2.4453e-04],\n",
       "         [ 3.5585e-04],\n",
       "         [ 6.6094e-05],\n",
       "         [-9.6632e-05],\n",
       "         [-2.6114e-05],\n",
       "         [-1.7552e-04],\n",
       "         [ 1.5867e-05],\n",
       "         [-9.4011e-06],\n",
       "         [-5.9327e-05],\n",
       "         [ 1.7818e-04],\n",
       "         [-3.1997e-05],\n",
       "         [ 1.5969e-05],\n",
       "         [-2.7057e-04],\n",
       "         [-2.1452e-04],\n",
       "         [ 1.3866e-04],\n",
       "         [ 1.3676e-05],\n",
       "         [ 1.3103e-04],\n",
       "         [-5.5330e-04],\n",
       "         [ 2.6931e-05],\n",
       "         [ 2.0946e-04],\n",
       "         [ 7.9603e-05],\n",
       "         [-8.5961e-05],\n",
       "         [ 1.9555e-04],\n",
       "         [ 2.9484e-05],\n",
       "         [ 3.8882e-04],\n",
       "         [ 2.4730e-04],\n",
       "         [ 4.7312e-04],\n",
       "         [ 2.4592e-04],\n",
       "         [-1.3069e-04],\n",
       "         [-1.3144e-04],\n",
       "         [-7.1622e-05],\n",
       "         [ 3.8904e-04],\n",
       "         [-2.7030e-04],\n",
       "         [ 2.9110e-04],\n",
       "         [ 4.8690e-05],\n",
       "         [-3.3900e-04],\n",
       "         [-2.5587e-04],\n",
       "         [-2.2770e-04],\n",
       "         [-2.6682e-04],\n",
       "         [ 2.6902e-05],\n",
       "         [ 4.4640e-05],\n",
       "         [ 1.6732e-04],\n",
       "         [ 2.8919e-05],\n",
       "         [ 1.1739e-04],\n",
       "         [-4.4921e-04],\n",
       "         [-5.0881e-04],\n",
       "         [-1.8167e-04],\n",
       "         [-4.6588e-04],\n",
       "         [-1.3806e-04],\n",
       "         [-1.4117e-04],\n",
       "         [-3.5440e-04],\n",
       "         [ 3.1701e-04],\n",
       "         [-2.5128e-04],\n",
       "         [-4.2932e-05],\n",
       "         [-9.1788e-05],\n",
       "         [ 3.1423e-04],\n",
       "         [ 2.7483e-04],\n",
       "         [ 1.3430e-04],\n",
       "         [-1.1669e-04],\n",
       "         [-1.3592e-04],\n",
       "         [-7.6336e-04],\n",
       "         [-1.8452e-06],\n",
       "         [ 1.2866e-04],\n",
       "         [-9.1112e-05],\n",
       "         [ 2.1593e-04],\n",
       "         [ 3.9759e-04],\n",
       "         [ 1.6477e-04],\n",
       "         [ 5.0032e-04],\n",
       "         [ 2.6123e-04],\n",
       "         [-2.7847e-04],\n",
       "         [-2.8565e-04],\n",
       "         [-1.3970e-04],\n",
       "         [-2.4499e-04],\n",
       "         [ 1.6740e-04],\n",
       "         [ 5.7963e-05],\n",
       "         [ 3.0431e-04],\n",
       "         [-8.3170e-05],\n",
       "         [ 9.3741e-05],\n",
       "         [-4.0943e-04],\n",
       "         [-1.4669e-04],\n",
       "         [ 2.8563e-04],\n",
       "         [-2.4122e-04],\n",
       "         [-1.4480e-04],\n",
       "         [-1.0208e-04],\n",
       "         [-2.5896e-04],\n",
       "         [ 5.8298e-06],\n",
       "         [ 1.3426e-04],\n",
       "         [-7.4184e-05],\n",
       "         [-8.9885e-05],\n",
       "         [-1.9665e-04],\n",
       "         [ 8.4015e-05],\n",
       "         [-2.0244e-05],\n",
       "         [-9.0184e-04],\n",
       "         [-1.5230e-04],\n",
       "         [ 7.0865e-05],\n",
       "         [-2.9532e-04],\n",
       "         [ 4.0718e-05],\n",
       "         [ 1.4099e-04],\n",
       "         [ 3.7506e-04],\n",
       "         [ 9.5681e-05],\n",
       "         [-2.5645e-04],\n",
       "         [ 5.4846e-04],\n",
       "         [-5.1004e-04],\n",
       "         [ 2.4189e-04],\n",
       "         [-1.0992e-04],\n",
       "         [ 1.9909e-04],\n",
       "         [ 3.0235e-04],\n",
       "         [ 1.1099e-04],\n",
       "         [ 1.4195e-04],\n",
       "         [ 7.3494e-05],\n",
       "         [ 1.4500e-04],\n",
       "         [-7.7762e-05],\n",
       "         [ 1.6487e-04],\n",
       "         [ 8.8295e-05],\n",
       "         [ 3.9590e-04],\n",
       "         [ 1.3330e-04],\n",
       "         [ 6.9540e-08],\n",
       "         [ 5.3490e-04],\n",
       "         [ 2.9394e-04],\n",
       "         [ 8.4469e-05],\n",
       "         [-1.7685e-04]], grad_fn=<TBackward0>),\n",
       " 'obs_encoder.rnn.weight_hh_l0': tensor([[ 6.6336e-08, -8.1561e-08, -1.2529e-07,  ...,  1.7469e-08,\n",
       "          -1.1085e-07,  2.8321e-08],\n",
       "         [ 4.6474e-08, -4.2014e-08, -5.7777e-08,  ..., -4.1000e-09,\n",
       "          -5.0938e-08,  9.2137e-09],\n",
       "         [-1.4736e-07,  1.3358e-07,  1.5576e-07,  ...,  2.3487e-08,\n",
       "           1.3170e-07, -2.1253e-08],\n",
       "         ...,\n",
       "         [-1.2834e-05,  1.3858e-05,  1.6699e-05,  ...,  2.7694e-07,\n",
       "           1.4238e-05, -3.0045e-06],\n",
       "         [-3.3178e-06,  2.1178e-06,  1.1996e-06,  ...,  1.9996e-06,\n",
       "           2.2889e-07,  4.4511e-07],\n",
       "         [ 2.7125e-06, -2.4733e-06, -3.6224e-06,  ..., -1.4869e-07,\n",
       "          -2.9167e-06,  5.9209e-07]], grad_fn=<AddBackward0>),\n",
       " 'obs_encoder.rnn.weight_ih_l0_reverse': tensor([[ 1.6357e-07],\n",
       "         [ 8.6715e-07],\n",
       "         [ 6.6219e-07],\n",
       "         [ 3.2642e-07],\n",
       "         [-2.8968e-07],\n",
       "         [ 1.1557e-06],\n",
       "         [ 3.1837e-07],\n",
       "         [-3.4741e-08],\n",
       "         [-8.1704e-07],\n",
       "         [-6.9703e-09],\n",
       "         [-3.6385e-08],\n",
       "         [ 2.8906e-07],\n",
       "         [ 3.3208e-07],\n",
       "         [ 1.3310e-07],\n",
       "         [ 7.2112e-07],\n",
       "         [ 7.2778e-08],\n",
       "         [-7.5285e-07],\n",
       "         [-9.4279e-08],\n",
       "         [ 8.0068e-08],\n",
       "         [ 3.0436e-08],\n",
       "         [-8.8219e-07],\n",
       "         [ 8.4614e-08],\n",
       "         [-2.1688e-08],\n",
       "         [-1.2322e-07],\n",
       "         [ 7.4382e-08],\n",
       "         [-9.3136e-07],\n",
       "         [ 1.2180e-06],\n",
       "         [ 3.2037e-07],\n",
       "         [ 2.7339e-08],\n",
       "         [ 2.8071e-08],\n",
       "         [ 7.4238e-07],\n",
       "         [ 3.7026e-07],\n",
       "         [-4.7210e-07],\n",
       "         [-1.2007e-06],\n",
       "         [-8.3819e-08],\n",
       "         [-7.3527e-08],\n",
       "         [ 1.6066e-06],\n",
       "         [ 8.5816e-07],\n",
       "         [-1.7124e-07],\n",
       "         [-1.4662e-07],\n",
       "         [-4.2666e-07],\n",
       "         [ 4.3253e-07],\n",
       "         [ 3.0986e-07],\n",
       "         [ 1.9346e-06],\n",
       "         [-1.7953e-07],\n",
       "         [-3.4761e-06],\n",
       "         [ 2.7961e-08],\n",
       "         [-1.0973e-06],\n",
       "         [ 1.4336e-06],\n",
       "         [ 2.1136e-06],\n",
       "         [ 1.9620e-06],\n",
       "         [ 5.5071e-07],\n",
       "         [ 6.3141e-07],\n",
       "         [ 7.9109e-08],\n",
       "         [-1.6060e-06],\n",
       "         [ 2.3140e-06],\n",
       "         [ 1.3257e-07],\n",
       "         [ 1.5964e-07],\n",
       "         [ 6.1174e-08],\n",
       "         [-1.0133e-07],\n",
       "         [-1.2002e-06],\n",
       "         [-7.7753e-08],\n",
       "         [ 1.4289e-06],\n",
       "         [ 1.5943e-07],\n",
       "         [ 7.1008e-07],\n",
       "         [-8.5898e-07],\n",
       "         [ 4.5806e-07],\n",
       "         [-1.5635e-06],\n",
       "         [ 1.4798e-06],\n",
       "         [-3.8941e-07],\n",
       "         [-5.4874e-08],\n",
       "         [ 1.4375e-07],\n",
       "         [-1.2243e-06],\n",
       "         [ 7.1008e-07],\n",
       "         [ 2.9657e-07],\n",
       "         [-4.7679e-07],\n",
       "         [ 2.9892e-06],\n",
       "         [ 6.7295e-08],\n",
       "         [ 6.0533e-07],\n",
       "         [-9.2665e-07],\n",
       "         [ 1.3222e-07],\n",
       "         [-6.5247e-07],\n",
       "         [-9.9969e-07],\n",
       "         [-1.1053e-06],\n",
       "         [-5.4962e-07],\n",
       "         [ 3.1031e-07],\n",
       "         [ 5.1977e-07],\n",
       "         [ 4.9846e-07],\n",
       "         [-9.2117e-08],\n",
       "         [-8.2631e-07],\n",
       "         [-5.6310e-08],\n",
       "         [-2.1079e-07],\n",
       "         [-5.1122e-08],\n",
       "         [-7.3581e-07],\n",
       "         [-2.9433e-07],\n",
       "         [ 1.5835e-07],\n",
       "         [-2.1714e-08],\n",
       "         [-3.0707e-07],\n",
       "         [-7.1689e-07],\n",
       "         [ 2.9644e-07],\n",
       "         [-7.7932e-09],\n",
       "         [-7.8047e-08],\n",
       "         [ 7.0331e-08],\n",
       "         [-1.1276e-07],\n",
       "         [-6.9027e-07],\n",
       "         [ 7.8069e-07],\n",
       "         [ 2.2345e-07],\n",
       "         [ 3.3857e-07],\n",
       "         [-8.6851e-07],\n",
       "         [ 4.9915e-07],\n",
       "         [ 6.8758e-07],\n",
       "         [-3.2742e-06],\n",
       "         [-5.1414e-07],\n",
       "         [ 2.1131e-06],\n",
       "         [ 7.0860e-07],\n",
       "         [-2.1324e-07],\n",
       "         [ 8.4041e-08],\n",
       "         [ 2.0295e-07],\n",
       "         [ 9.6361e-09],\n",
       "         [ 2.5035e-07],\n",
       "         [-2.9626e-08],\n",
       "         [ 1.0783e-06],\n",
       "         [-6.1061e-08],\n",
       "         [-3.1663e-07],\n",
       "         [ 8.7232e-09],\n",
       "         [ 3.7668e-07],\n",
       "         [-3.3431e-08],\n",
       "         [ 4.9561e-07],\n",
       "         [-9.5317e-06],\n",
       "         [ 1.9746e-05],\n",
       "         [-9.5118e-06],\n",
       "         [-1.9582e-06],\n",
       "         [-1.7507e-05],\n",
       "         [ 1.1454e-05],\n",
       "         [-6.1368e-06],\n",
       "         [ 1.0111e-05],\n",
       "         [-2.7812e-06],\n",
       "         [-1.4787e-06],\n",
       "         [ 2.2829e-07],\n",
       "         [-1.0672e-05],\n",
       "         [-5.0199e-06],\n",
       "         [-5.8223e-06],\n",
       "         [-1.2591e-06],\n",
       "         [-8.0315e-07],\n",
       "         [-1.6897e-05],\n",
       "         [ 5.5928e-06],\n",
       "         [-1.8417e-05],\n",
       "         [ 1.2669e-05],\n",
       "         [-2.1320e-05],\n",
       "         [ 5.7170e-06],\n",
       "         [-6.4224e-06],\n",
       "         [-2.1952e-06],\n",
       "         [ 2.5432e-06],\n",
       "         [-3.0787e-06],\n",
       "         [ 9.0536e-07],\n",
       "         [ 1.5615e-05],\n",
       "         [ 4.7674e-06],\n",
       "         [-1.7845e-05],\n",
       "         [-1.0664e-05],\n",
       "         [-6.2570e-06],\n",
       "         [-2.9029e-05],\n",
       "         [ 1.5716e-05],\n",
       "         [ 1.3114e-05],\n",
       "         [-2.8600e-05],\n",
       "         [ 6.0579e-06],\n",
       "         [-5.1052e-06],\n",
       "         [ 1.2501e-05],\n",
       "         [ 9.1766e-06],\n",
       "         [ 7.1577e-06],\n",
       "         [ 1.3441e-06],\n",
       "         [-1.8673e-05],\n",
       "         [ 3.6672e-05],\n",
       "         [-3.4610e-05],\n",
       "         [-1.6567e-05],\n",
       "         [ 8.0781e-07],\n",
       "         [ 4.2718e-05],\n",
       "         [ 3.1409e-05],\n",
       "         [-1.4679e-05],\n",
       "         [-2.9331e-05],\n",
       "         [-7.2926e-06],\n",
       "         [ 1.5969e-05],\n",
       "         [-1.1544e-05],\n",
       "         [ 3.8961e-05],\n",
       "         [-3.3090e-05],\n",
       "         [ 1.0557e-06],\n",
       "         [-1.1458e-06],\n",
       "         [-1.3413e-05],\n",
       "         [ 1.9764e-05],\n",
       "         [-5.0794e-05],\n",
       "         [ 1.2613e-05],\n",
       "         [ 2.2386e-05],\n",
       "         [ 2.5584e-06],\n",
       "         [ 1.7634e-05],\n",
       "         [-9.4555e-06],\n",
       "         [-3.2067e-05],\n",
       "         [-7.9004e-06],\n",
       "         [ 2.6054e-05],\n",
       "         [-8.4115e-07],\n",
       "         [ 3.0354e-05],\n",
       "         [ 9.2617e-08],\n",
       "         [ 2.3023e-05],\n",
       "         [ 1.2222e-05],\n",
       "         [ 6.6515e-06],\n",
       "         [ 3.6469e-05],\n",
       "         [ 2.7206e-05],\n",
       "         [ 5.1015e-06],\n",
       "         [-3.6549e-06],\n",
       "         [-2.0129e-05],\n",
       "         [-2.0198e-05],\n",
       "         [-1.3293e-06],\n",
       "         [-1.2177e-05],\n",
       "         [ 7.9428e-05],\n",
       "         [-1.4693e-05],\n",
       "         [-1.1357e-05],\n",
       "         [ 1.3720e-06],\n",
       "         [-1.0958e-05],\n",
       "         [ 2.1203e-05],\n",
       "         [ 1.5125e-05],\n",
       "         [-3.5348e-05],\n",
       "         [-6.2551e-05],\n",
       "         [ 2.6946e-05],\n",
       "         [-3.1578e-06],\n",
       "         [-3.5697e-05],\n",
       "         [ 1.4219e-05],\n",
       "         [-1.1075e-06],\n",
       "         [ 9.2573e-07],\n",
       "         [-7.9957e-06],\n",
       "         [-2.2917e-05],\n",
       "         [ 1.6949e-05],\n",
       "         [-1.9743e-08],\n",
       "         [-9.1612e-06],\n",
       "         [-1.7066e-07],\n",
       "         [ 1.0181e-05],\n",
       "         [-1.6216e-05],\n",
       "         [-3.2875e-06],\n",
       "         [-2.3315e-06],\n",
       "         [ 5.5704e-05],\n",
       "         [ 1.4678e-05],\n",
       "         [ 4.5961e-06],\n",
       "         [ 6.0618e-06],\n",
       "         [ 2.6975e-07],\n",
       "         [-2.5075e-05],\n",
       "         [ 7.1082e-06],\n",
       "         [-7.0420e-06],\n",
       "         [-3.7010e-06],\n",
       "         [-4.4283e-06],\n",
       "         [ 3.0274e-06],\n",
       "         [-4.8166e-06],\n",
       "         [ 2.8928e-06],\n",
       "         [ 1.7880e-05],\n",
       "         [ 1.9767e-05],\n",
       "         [-4.3439e-05],\n",
       "         [-2.7352e-06],\n",
       "         [ 8.9487e-06],\n",
       "         [-2.4789e-06],\n",
       "         [ 2.9821e-05],\n",
       "         [ 1.5830e-04],\n",
       "         [ 1.4732e-04],\n",
       "         [-2.3558e-04],\n",
       "         [ 3.8930e-04],\n",
       "         [ 1.3067e-04],\n",
       "         [-2.3333e-05],\n",
       "         [ 1.0644e-04],\n",
       "         [-1.1282e-04],\n",
       "         [ 1.9696e-05],\n",
       "         [-5.8710e-05],\n",
       "         [ 4.2491e-05],\n",
       "         [ 3.4284e-05],\n",
       "         [-6.4328e-05],\n",
       "         [-2.8478e-05],\n",
       "         [-1.2462e-04],\n",
       "         [-1.2669e-04],\n",
       "         [-1.6254e-04],\n",
       "         [ 4.4607e-05],\n",
       "         [ 5.0225e-06],\n",
       "         [-6.9395e-05],\n",
       "         [-1.0437e-04],\n",
       "         [-3.6743e-04],\n",
       "         [-3.1560e-04],\n",
       "         [-9.9939e-05],\n",
       "         [-2.8000e-04],\n",
       "         [-9.1530e-05],\n",
       "         [-1.3261e-04],\n",
       "         [ 3.0841e-05],\n",
       "         [-6.1669e-05],\n",
       "         [-1.2882e-04],\n",
       "         [ 1.1818e-05],\n",
       "         [ 4.2310e-04],\n",
       "         [-1.0688e-04],\n",
       "         [ 9.7878e-05],\n",
       "         [ 7.5678e-05],\n",
       "         [ 4.3795e-05],\n",
       "         [ 4.1423e-05],\n",
       "         [-2.6792e-04],\n",
       "         [ 2.5278e-04],\n",
       "         [-1.3026e-05],\n",
       "         [ 1.3329e-04],\n",
       "         [-1.2343e-04],\n",
       "         [ 1.6334e-04],\n",
       "         [-1.4103e-04],\n",
       "         [ 1.1516e-04],\n",
       "         [ 3.5319e-04],\n",
       "         [-2.4984e-07],\n",
       "         [ 2.7418e-04],\n",
       "         [ 3.0531e-04],\n",
       "         [-3.1348e-04],\n",
       "         [-1.2169e-04],\n",
       "         [ 9.7500e-05],\n",
       "         [ 7.3840e-05],\n",
       "         [ 2.2034e-05],\n",
       "         [ 1.1318e-04],\n",
       "         [ 7.4399e-05],\n",
       "         [ 3.6158e-05],\n",
       "         [-8.7007e-05],\n",
       "         [ 3.6749e-04],\n",
       "         [ 8.4487e-05],\n",
       "         [-7.5803e-05],\n",
       "         [-1.1356e-04],\n",
       "         [-1.2591e-04],\n",
       "         [ 1.0838e-04],\n",
       "         [ 1.6444e-04],\n",
       "         [-3.0319e-04],\n",
       "         [-6.4646e-06],\n",
       "         [-9.6549e-05],\n",
       "         [-8.4947e-05],\n",
       "         [-1.2564e-04],\n",
       "         [-2.0316e-04],\n",
       "         [-1.6591e-04],\n",
       "         [ 3.0452e-04],\n",
       "         [-1.3930e-04],\n",
       "         [-1.7690e-04],\n",
       "         [-2.1184e-04],\n",
       "         [ 3.9821e-05],\n",
       "         [-3.8730e-05],\n",
       "         [ 1.2390e-04],\n",
       "         [ 1.1474e-04],\n",
       "         [-7.5450e-05],\n",
       "         [-2.8072e-04],\n",
       "         [ 3.7444e-04],\n",
       "         [-1.1141e-05],\n",
       "         [-8.3380e-05],\n",
       "         [ 2.6215e-05],\n",
       "         [ 1.3077e-04],\n",
       "         [-4.0523e-05],\n",
       "         [ 4.6026e-05],\n",
       "         [-2.6932e-05],\n",
       "         [ 5.8306e-05],\n",
       "         [ 2.1728e-04],\n",
       "         [-4.3325e-05],\n",
       "         [-5.2072e-05],\n",
       "         [-2.3450e-04],\n",
       "         [ 1.1362e-04],\n",
       "         [ 1.4746e-04],\n",
       "         [-1.1553e-04],\n",
       "         [ 4.8588e-05],\n",
       "         [-1.7157e-04],\n",
       "         [-5.8143e-05],\n",
       "         [ 2.9229e-04],\n",
       "         [-2.2240e-04],\n",
       "         [-1.6320e-04],\n",
       "         [ 1.7531e-04],\n",
       "         [ 3.2800e-04],\n",
       "         [-2.6113e-05],\n",
       "         [-1.3298e-04],\n",
       "         [-3.1243e-05],\n",
       "         [-3.8248e-04],\n",
       "         [ 7.2498e-06],\n",
       "         [-1.0987e-04],\n",
       "         [-2.1280e-04],\n",
       "         [ 1.2156e-04],\n",
       "         [-1.7916e-05],\n",
       "         [ 8.7051e-05],\n",
       "         [-9.7796e-05],\n",
       "         [ 4.3758e-05],\n",
       "         [-1.8505e-04],\n",
       "         [ 5.4608e-06],\n",
       "         [ 1.8213e-04],\n",
       "         [-2.3421e-06],\n",
       "         [-5.3691e-05],\n",
       "         [ 2.5249e-04],\n",
       "         [ 8.5088e-05],\n",
       "         [-3.3471e-04],\n",
       "         [ 1.0718e-04],\n",
       "         [-4.8851e-05]], grad_fn=<TBackward0>),\n",
       " 'obs_encoder.rnn.weight_hh_l0_reverse': tensor([[-9.7995e-09, -4.1766e-08, -3.4574e-08,  ...,  1.5534e-08,\n",
       "           2.8507e-08, -4.1098e-08],\n",
       "         [-2.6666e-08, -1.2612e-07, -9.9581e-08,  ...,  4.6504e-08,\n",
       "           7.8937e-08, -1.1744e-07],\n",
       "         [ 2.6650e-08,  1.4468e-07,  1.2202e-07,  ..., -4.7375e-08,\n",
       "          -9.8225e-08,  1.2550e-07],\n",
       "         ...,\n",
       "         [-9.9069e-07, -6.6801e-06, -4.4878e-06,  ...,  2.1856e-06,\n",
       "           3.5449e-06, -4.9138e-06],\n",
       "         [-1.9111e-07,  7.5586e-07, -4.8811e-08,  ..., -5.6370e-08,\n",
       "           1.4118e-07, -2.3926e-07],\n",
       "         [-6.6464e-07, -3.9699e-06, -4.4307e-06,  ...,  1.2497e-06,\n",
       "           3.5553e-06, -3.7556e-06]], grad_fn=<AddBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.self_attn.in_proj_weight': tensor([[ 2.9157e-06, -7.7044e-06, -6.3631e-06,  ...,  1.2455e-06,\n",
       "          -2.0835e-06, -1.7158e-06],\n",
       "         [ 2.6018e-06, -1.6431e-05, -1.5353e-05,  ...,  4.2607e-06,\n",
       "          -7.8053e-07, -7.3300e-06],\n",
       "         [-8.8398e-07,  9.0700e-06,  9.0052e-06,  ..., -2.5903e-06,\n",
       "          -2.1778e-07,  4.7734e-06],\n",
       "         ...,\n",
       "         [ 1.7505e-05,  5.0382e-06, -9.5132e-08,  ..., -5.8306e-06,\n",
       "          -2.1148e-05,  1.3613e-05],\n",
       "         [ 1.0558e-05, -1.3187e-05, -1.1165e-05,  ..., -7.3021e-07,\n",
       "          -8.8918e-06,  1.1760e-06],\n",
       "         [-2.5874e-05,  3.4407e-05,  3.9018e-05,  ..., -4.3953e-07,\n",
       "           1.7651e-05,  6.0756e-06]], grad_fn=<TBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.self_attn.in_proj_bias': tensor([ 1.9865e-05,  9.2058e-06,  3.6580e-06, -1.1697e-05,  4.2502e-07,\n",
       "         -7.2202e-06,  1.4632e-06, -3.7502e-06,  2.1042e-07, -1.4868e-05,\n",
       "          7.4757e-06,  2.7057e-06, -5.2969e-06, -2.2078e-05, -9.6301e-06,\n",
       "         -8.7209e-06,  1.0422e-05, -1.2261e-06, -1.9580e-06, -2.7166e-06,\n",
       "         -8.2102e-07, -1.3707e-06, -5.5021e-06,  1.0965e-05, -2.0224e-05,\n",
       "         -1.3458e-06,  2.0372e-05,  1.0979e-05,  8.5986e-06,  5.9801e-06,\n",
       "         -1.2954e-06,  4.6874e-06,  1.8524e-06, -1.4632e-06,  2.1223e-05,\n",
       "         -2.9380e-06,  4.7614e-06, -9.5580e-06,  4.3498e-06, -2.2760e-06,\n",
       "          3.0246e-06, -4.9321e-06,  2.0262e-06, -3.5438e-06, -4.9276e-06,\n",
       "         -6.0418e-07, -2.9439e-08,  4.2397e-07, -1.0504e-05, -4.6778e-06,\n",
       "          9.2168e-06,  7.0791e-06,  7.8998e-06, -8.7796e-06,  1.8830e-06,\n",
       "          1.7418e-05, -5.0975e-06, -5.9295e-06, -3.8171e-06,  4.3242e-06,\n",
       "         -2.0936e-06, -3.9343e-06,  1.5865e-05, -1.2805e-05, -3.1088e-06,\n",
       "          6.9330e-06, -6.8577e-06,  6.8422e-06,  1.3493e-05,  5.8882e-06,\n",
       "         -4.3688e-06,  3.9227e-06, -1.1871e-05, -2.9975e-07,  2.1499e-06,\n",
       "         -5.4904e-06,  1.0344e-05, -4.7327e-07,  1.1261e-07, -9.2717e-06,\n",
       "         -6.5082e-06,  4.7022e-06,  3.2410e-06, -2.3149e-05,  9.0916e-06,\n",
       "         -1.4900e-05, -1.4640e-06, -7.4556e-07, -1.7400e-05,  1.1873e-05,\n",
       "          8.0277e-07, -7.5352e-06,  1.2936e-06,  1.0491e-06,  8.7944e-06,\n",
       "         -1.1456e-05,  5.0930e-06,  5.0375e-06, -7.2718e-06,  3.0087e-06,\n",
       "          1.6248e-05,  7.8436e-06,  7.9463e-06,  4.5032e-06,  1.1956e-05,\n",
       "          4.9463e-06,  4.3961e-06,  2.9215e-06,  7.8953e-06,  1.6251e-05,\n",
       "          7.8397e-06, -2.2557e-05, -2.7987e-06,  9.5020e-06,  6.7130e-07,\n",
       "         -2.2490e-06,  8.4023e-06, -1.9017e-06, -1.1954e-06, -2.7091e-06,\n",
       "          1.1779e-05, -2.5343e-06,  6.3644e-06, -5.8768e-06,  2.8288e-05,\n",
       "         -7.5552e-06, -1.8758e-05,  5.2771e-06,  2.2737e-13, -1.4211e-13,\n",
       "         -4.5475e-13,  0.0000e+00, -8.5265e-14,  1.3642e-12,  8.5265e-14,\n",
       "          1.1369e-13,  0.0000e+00,  2.2737e-13, -1.1369e-13,  4.5475e-13,\n",
       "         -2.8422e-14,  5.6843e-14,  0.0000e+00,  0.0000e+00, -2.2737e-13,\n",
       "         -1.1369e-13,  0.0000e+00, -4.5475e-13, -1.3642e-12, -3.9790e-13,\n",
       "         -3.4106e-13, -2.2737e-13,  0.0000e+00, -2.2737e-13,  2.2737e-13,\n",
       "          6.8212e-13, -2.2737e-13,  4.5475e-13,  1.1369e-13, -2.2737e-13,\n",
       "          0.0000e+00, -2.2737e-13,  0.0000e+00, -4.5475e-13, -1.1369e-13,\n",
       "          0.0000e+00, -5.6843e-14, -1.8474e-13, -2.2737e-13, -2.8422e-13,\n",
       "          0.0000e+00,  2.2737e-13,  2.2737e-13, -3.9790e-13,  1.7053e-13,\n",
       "         -3.4106e-13, -1.4211e-14,  6.8212e-13, -3.4106e-13, -2.2737e-13,\n",
       "          2.2737e-13, -9.0949e-13, -4.5475e-13,  4.5475e-13,  1.1369e-13,\n",
       "          1.2434e-14, -1.1369e-13, -4.5475e-13, -4.5475e-13,  5.6843e-14,\n",
       "          0.0000e+00,  6.8212e-13, -2.2737e-13, -2.8422e-13,  2.2737e-13,\n",
       "         -1.1369e-13,  0.0000e+00,  1.4211e-14,  0.0000e+00,  1.1369e-13,\n",
       "         -4.5475e-13,  0.0000e+00, -2.2737e-13,  4.5475e-13,  2.8422e-14,\n",
       "         -9.0949e-13,  0.0000e+00,  2.2737e-13,  9.0949e-13, -3.4106e-13,\n",
       "          2.8422e-13, -4.5475e-13,  3.5527e-14,  1.8474e-13,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  1.5916e-12,  1.1369e-13, -7.8160e-14,\n",
       "          2.1316e-14,  3.4106e-13,  0.0000e+00,  2.2737e-13,  2.2737e-13,\n",
       "         -1.4211e-14,  4.5475e-13, -4.5475e-13,  6.8212e-13,  1.1369e-13,\n",
       "         -5.6843e-14,  2.2737e-13,  4.5475e-13,  4.2633e-14, -4.5475e-13,\n",
       "         -1.7053e-13,  1.2790e-13, -2.8422e-14,  3.4106e-13, -1.1369e-13,\n",
       "         -2.2737e-13,  1.1369e-13, -9.0949e-13, -1.1369e-13,  2.2737e-13,\n",
       "         -1.1369e-13,  1.1369e-13, -4.5475e-13,  1.7053e-13,  0.0000e+00,\n",
       "          1.1369e-13, -1.1369e-13, -4.5475e-13,  3.4106e-13,  0.0000e+00,\n",
       "          0.0000e+00,  9.0444e-06, -1.6196e-05,  5.6803e-06, -1.8880e-05,\n",
       "         -2.6354e-05, -2.2550e-05, -1.3206e-05, -3.1900e-05,  8.3645e-06,\n",
       "         -8.2057e-06,  1.7536e-05, -1.0943e-05, -1.1179e-05, -2.9316e-05,\n",
       "          2.1615e-06,  5.4453e-06,  9.9561e-06, -5.3426e-05, -1.0789e-05,\n",
       "          2.9278e-05,  1.9084e-05,  6.0739e-07,  9.7555e-06, -4.7020e-06,\n",
       "          6.8097e-06, -8.0037e-06,  4.6018e-06,  2.4343e-05,  8.8051e-06,\n",
       "         -1.0795e-05,  4.1171e-05, -1.2533e-05,  2.2803e-05,  3.0425e-05,\n",
       "          5.8992e-08,  2.3543e-05,  5.1060e-06, -3.7400e-05, -1.2642e-05,\n",
       "          1.8201e-05,  5.9366e-06, -2.3704e-05,  1.0906e-05,  3.6085e-06,\n",
       "         -4.1341e-05,  2.4165e-05, -2.1145e-05,  1.2378e-05,  6.0732e-08,\n",
       "         -3.0837e-05, -2.8618e-05, -8.5269e-06, -9.9706e-07,  1.4266e-05,\n",
       "          2.1015e-05,  1.9395e-05, -3.5159e-07, -2.4199e-05,  1.6477e-05,\n",
       "          4.1988e-06,  1.7790e-05, -1.7766e-05, -8.8137e-06, -9.8094e-06,\n",
       "          1.8974e-05, -9.1765e-07, -1.1934e-05, -5.1872e-06,  4.6423e-09,\n",
       "          3.6828e-06, -2.7373e-05, -2.0429e-05,  2.2235e-05,  5.2113e-06,\n",
       "          3.5083e-06,  8.2042e-06, -7.0912e-06,  9.4565e-06,  3.6893e-05,\n",
       "          1.3044e-05,  9.0967e-06, -2.1037e-05,  6.8543e-07,  2.7900e-05,\n",
       "         -5.9121e-06,  2.0407e-06, -4.9334e-06, -3.0667e-05,  2.0615e-05,\n",
       "          2.0548e-05, -1.8434e-05, -2.4732e-05, -5.1539e-06,  1.5566e-05,\n",
       "          2.4005e-05, -1.0256e-05,  8.5232e-06, -1.8232e-05,  1.4826e-05,\n",
       "         -3.3008e-06,  3.1080e-05, -1.6168e-05, -2.9974e-05,  1.2922e-06,\n",
       "          1.5882e-05,  2.2655e-06,  1.7993e-05,  5.2638e-07,  2.1984e-05,\n",
       "          1.1201e-05, -3.3936e-05, -4.5519e-05,  2.1496e-05, -1.2125e-05,\n",
       "         -2.0483e-05, -1.8939e-05,  3.2716e-05,  3.3109e-05,  2.1618e-06,\n",
       "          1.6030e-05,  1.7029e-05,  1.5635e-05,  3.3164e-05, -1.3914e-05,\n",
       "          9.6796e-06, -2.7368e-05,  3.1018e-05,  4.1786e-06],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.self_attn.out_proj.weight': tensor([[-1.2398e-05,  4.9430e-05, -2.3933e-06,  ..., -1.7836e-06,\n",
       "           9.5655e-06, -1.9839e-05],\n",
       "         [-4.1584e-07, -2.1445e-05, -7.0946e-07,  ...,  1.6112e-06,\n",
       "          -2.1456e-06,  7.9671e-06],\n",
       "         [ 7.2272e-06, -2.2011e-05,  1.6897e-06,  ...,  8.5763e-07,\n",
       "          -4.5273e-06,  8.5166e-06],\n",
       "         ...,\n",
       "         [ 1.6748e-05, -4.6759e-05,  3.6800e-06,  ...,  3.5007e-06,\n",
       "          -8.4272e-06,  1.7311e-05],\n",
       "         [-7.3412e-06,  1.4510e-05, -2.0938e-06,  ..., -1.3240e-06,\n",
       "           3.2569e-06, -5.0365e-06],\n",
       "         [ 1.2424e-05, -1.9262e-05,  2.4491e-06,  ...,  7.7274e-07,\n",
       "          -5.0312e-06,  7.2935e-06]], grad_fn=<TBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.self_attn.out_proj.bias': tensor([-4.3028e-05,  4.7437e-06,  1.3272e-05, -9.0141e-05, -5.5536e-06,\n",
       "          1.1862e-06, -1.3050e-05,  1.8925e-05, -8.6277e-05,  3.9021e-07,\n",
       "         -1.3319e-05,  1.6960e-05,  7.8379e-05, -9.0882e-05, -2.8671e-05,\n",
       "          8.3170e-06,  1.2282e-05,  6.7971e-05, -1.5740e-07, -2.1311e-05,\n",
       "         -1.6635e-06, -1.0079e-05, -3.7358e-05,  1.5829e-05,  4.4034e-06,\n",
       "         -8.6243e-05,  3.7269e-06,  2.3952e-05,  3.2997e-05,  2.2614e-05,\n",
       "          2.0705e-05, -5.0983e-05, -2.7046e-05, -3.3287e-05, -1.9021e-05,\n",
       "          1.1532e-05,  5.0101e-05,  2.2130e-05, -8.7345e-05,  6.5045e-06,\n",
       "         -1.6681e-06,  2.3612e-06,  4.5783e-06, -1.4702e-05, -7.0420e-05,\n",
       "          9.5199e-06, -2.5452e-06, -4.0711e-05, -3.0293e-06,  1.2313e-06,\n",
       "          6.7104e-05,  3.3096e-05,  1.1187e-05, -3.6173e-05, -2.6950e-05,\n",
       "         -2.0306e-05,  1.8234e-06, -5.3396e-06, -7.8398e-06,  2.9190e-05,\n",
       "          3.4092e-05,  1.1388e-05,  2.4099e-05, -3.3857e-05, -8.1571e-05,\n",
       "          1.4779e-05,  4.5918e-05, -1.7836e-05,  1.9270e-07, -2.8848e-05,\n",
       "         -6.6430e-05,  1.7359e-05, -4.9758e-05, -1.8120e-05,  3.8857e-05,\n",
       "          1.8020e-05,  9.6411e-05,  6.3064e-06,  8.8089e-06, -1.0843e-05,\n",
       "          1.5458e-06,  3.1073e-05, -3.2043e-05, -1.6985e-05,  1.1632e-06,\n",
       "          6.7847e-05, -1.2229e-05, -4.3946e-05, -1.9799e-05,  2.6324e-05,\n",
       "          3.9018e-05, -1.2550e-05, -1.6134e-05,  2.2024e-05,  1.0623e-05,\n",
       "         -2.3571e-05, -7.3459e-06,  5.3472e-06, -4.8886e-05,  5.5110e-05,\n",
       "         -1.5133e-05, -1.8404e-06, -6.7548e-05,  9.0553e-06, -1.1131e-05,\n",
       "          6.7664e-06, -1.6327e-05, -4.2298e-06, -5.9322e-05, -7.5425e-05,\n",
       "         -5.8252e-06,  1.3363e-05,  1.1411e-06, -2.7196e-05,  4.5264e-05,\n",
       "         -1.0606e-05, -3.3056e-06,  2.2065e-05,  1.2028e-05, -2.8434e-05,\n",
       "         -5.9067e-06, -6.5549e-05, -4.6774e-05, -1.7692e-05,  2.3485e-05,\n",
       "          2.7613e-05, -1.3228e-05,  1.7176e-05], grad_fn=<ViewBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.linear1.weight': tensor([[-6.9178e-09, -8.7571e-07, -3.1386e-07,  ..., -2.4428e-07,\n",
       "           6.6301e-07, -1.5090e-07],\n",
       "         [-5.1577e-08,  1.1580e-07,  3.7775e-08,  ...,  3.1412e-08,\n",
       "          -4.8099e-08, -7.0762e-08],\n",
       "         [-1.0993e-07,  3.0443e-07,  1.9801e-07,  ...,  3.8300e-08,\n",
       "          -2.8203e-07, -6.6623e-08],\n",
       "         ...,\n",
       "         [ 1.9748e-07, -3.5262e-07, -2.6344e-07,  ..., -1.2105e-07,\n",
       "           1.4479e-07,  3.4922e-07],\n",
       "         [-5.4295e-08, -3.5761e-08,  6.0161e-08,  ...,  2.0987e-08,\n",
       "           1.0342e-07, -2.5131e-07],\n",
       "         [-7.9485e-08, -4.2883e-09,  1.5543e-07,  ..., -7.1344e-08,\n",
       "           8.4592e-08, -3.2457e-08]], grad_fn=<TBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.linear1.bias': tensor([-3.2771e-07, -7.4284e-08,  9.5720e-08,  1.6082e-07,  7.5179e-09,\n",
       "         -1.7212e-07,  1.5183e-07,  4.2337e-08, -3.9555e-08, -1.1668e-07,\n",
       "         -2.6070e-07,  2.3470e-07,  1.0378e-07, -4.1414e-07,  7.8382e-08,\n",
       "         -1.6320e-08,  3.8384e-07, -1.0168e-07,  1.9858e-08, -5.2031e-08,\n",
       "          6.0387e-08,  2.7093e-07, -2.8270e-07, -6.5731e-08, -1.7858e-08,\n",
       "          2.1508e-07,  9.9977e-08, -1.3526e-07, -1.6524e-07,  6.1806e-08,\n",
       "          2.5666e-08, -2.3858e-07,  9.7038e-08, -1.3252e-07,  4.0013e-10,\n",
       "         -6.7420e-08, -6.1675e-08,  1.4596e-07,  3.7884e-08,  7.5059e-08,\n",
       "          5.9729e-08, -8.8285e-08,  5.0877e-08,  7.1074e-09, -3.1648e-07,\n",
       "         -2.7213e-07, -1.2845e-07,  1.2412e-08, -1.0836e-07,  3.1602e-08,\n",
       "          1.1885e-07,  6.3898e-08,  2.3499e-08, -2.3303e-07,  6.8884e-08,\n",
       "          1.4130e-07,  5.9995e-08,  1.0855e-07, -1.1160e-08,  1.5283e-08,\n",
       "         -6.6528e-08, -1.4501e-08, -7.7723e-08, -1.1184e-07, -4.5787e-08,\n",
       "          3.2445e-08, -1.1379e-07, -1.3121e-07,  2.8769e-07,  2.4297e-07,\n",
       "         -1.7859e-07, -3.7211e-08,  2.7532e-07, -2.0242e-07, -8.8208e-08,\n",
       "         -7.2829e-08,  2.7032e-07,  3.7826e-08,  1.0898e-07, -7.3715e-08,\n",
       "         -3.6451e-07, -2.4310e-07,  8.7085e-08,  9.6225e-08,  2.5322e-07,\n",
       "         -2.6149e-07, -5.1875e-08,  5.1899e-08,  9.7568e-08, -2.1739e-08,\n",
       "         -1.9516e-07,  3.1909e-07,  2.1183e-10,  1.1306e-07, -3.5804e-07,\n",
       "          4.5815e-08, -3.2445e-08, -2.5759e-07,  1.7854e-07,  1.8717e-08,\n",
       "          1.2717e-07, -2.1294e-07,  3.0791e-07, -9.6581e-08, -9.6542e-08,\n",
       "          9.1468e-09, -2.4966e-08, -1.6546e-07,  1.2253e-08, -3.2640e-07,\n",
       "          1.4326e-07,  2.0398e-07, -2.0816e-07, -1.4235e-07, -1.4290e-08,\n",
       "         -7.2086e-08,  6.2689e-08,  9.9681e-08, -1.3912e-08, -2.7925e-07,\n",
       "          1.2938e-07,  1.9455e-07,  1.2500e-07,  3.1034e-07,  1.2937e-07,\n",
       "          1.5680e-10, -3.1538e-08,  4.6910e-08], grad_fn=<ViewBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.linear2.weight': tensor([[ 2.1414e-07, -4.0406e-08, -1.2300e-07,  ..., -1.8226e-08,\n",
       "           3.6830e-07,  4.5974e-07],\n",
       "         [-1.2608e-07,  8.6037e-08, -8.2771e-08,  ...,  3.0318e-08,\n",
       "          -2.3260e-07, -4.3887e-07],\n",
       "         [-2.0350e-07,  3.3341e-08,  3.2733e-07,  ..., -2.2337e-08,\n",
       "          -3.0914e-07, -1.6586e-07],\n",
       "         ...,\n",
       "         [-2.2579e-07,  8.8491e-08,  1.8001e-07,  ...,  5.5840e-09,\n",
       "          -5.6157e-07, -4.8587e-07],\n",
       "         [ 4.7546e-08, -3.7923e-09, -9.8195e-08,  ..., -1.1013e-08,\n",
       "           5.1350e-08,  1.1107e-07],\n",
       "         [-8.0295e-08, -8.5637e-08,  2.2510e-07,  ..., -2.3942e-08,\n",
       "          -9.5989e-08, -2.7869e-07]], grad_fn=<TBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.linear2.bias': tensor([-3.7964e-05,  4.1747e-05, -8.3583e-06, -5.7127e-05, -3.5197e-07,\n",
       "         -3.0417e-05,  4.2184e-05,  3.4984e-06, -3.4517e-05,  9.6052e-06,\n",
       "         -7.1785e-06,  3.3725e-05,  5.1826e-05, -9.0123e-05,  1.4696e-05,\n",
       "          1.8554e-06,  1.1764e-05, -4.7199e-05, -1.8022e-05, -2.5042e-05,\n",
       "         -8.8235e-06, -4.2148e-05, -5.5623e-05,  8.8828e-06, -2.1172e-06,\n",
       "         -2.8601e-05, -4.6594e-05,  5.5563e-06,  8.6792e-06,  2.8054e-05,\n",
       "          2.6261e-05,  2.1154e-05, -9.9292e-06,  2.5216e-06, -4.1805e-05,\n",
       "          3.9627e-05,  5.5077e-06,  2.4276e-05, -5.9714e-05, -1.5337e-05,\n",
       "         -1.9905e-05, -5.3059e-06,  1.3347e-05, -3.5619e-05, -5.5226e-05,\n",
       "          2.9160e-05, -7.4000e-06,  5.0966e-05, -1.3115e-05, -3.6020e-05,\n",
       "         -2.3005e-05,  2.4212e-05, -5.0341e-05, -1.8730e-05, -3.4995e-05,\n",
       "         -5.4181e-05,  2.2699e-05, -1.1204e-05,  1.5297e-06,  4.3783e-05,\n",
       "          3.6272e-05, -3.0655e-07, -4.4234e-06, -1.5749e-05, -4.7543e-05,\n",
       "          1.3493e-05,  4.7705e-06,  8.6602e-07, -2.7661e-06, -1.9259e-05,\n",
       "         -8.7760e-06,  1.1300e-05, -3.1611e-05,  1.5363e-05,  4.2965e-06,\n",
       "         -9.3664e-06,  5.7512e-05, -1.2223e-05,  7.2524e-06, -3.4117e-05,\n",
       "         -2.1611e-06,  1.0246e-05, -8.0124e-06, -2.1120e-06,  3.7761e-06,\n",
       "          5.7984e-05,  1.0429e-06, -9.0127e-05, -7.2845e-05,  1.4488e-05,\n",
       "         -5.1942e-05, -5.1483e-05, -1.4740e-05,  7.9003e-06, -3.7174e-07,\n",
       "         -2.0697e-05, -6.8744e-05, -1.1618e-06, -3.9075e-05,  8.2802e-05,\n",
       "         -1.8129e-05,  1.7533e-07, -9.8148e-05,  3.6761e-06, -5.1303e-06,\n",
       "         -1.4743e-05, -2.0816e-05,  2.7847e-05, -1.7715e-06, -3.3561e-05,\n",
       "          7.7308e-06, -1.5873e-05,  1.0073e-05, -1.4411e-05,  1.8612e-05,\n",
       "         -7.6116e-06, -4.8049e-06,  2.0037e-05,  8.4405e-06, -3.6902e-05,\n",
       "         -3.9202e-06, -4.1231e-05,  4.9863e-06, -3.0934e-06,  5.3024e-05,\n",
       "          2.2692e-06, -8.1592e-06,  3.3739e-05], grad_fn=<ViewBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.norm1.weight': tensor([-2.4416e-05,  5.9571e-06,  9.8727e-06, -3.2861e-05,  5.3019e-06,\n",
       "          1.8429e-05,  6.2568e-06,  2.4169e-05, -1.5177e-06, -1.4375e-06,\n",
       "         -1.0929e-05,  8.7464e-08,  2.8815e-06, -1.4126e-05,  3.5379e-06,\n",
       "          5.1059e-06, -5.6358e-06,  2.7926e-06,  2.3997e-06, -7.7831e-06,\n",
       "          1.2445e-05, -4.5788e-06, -2.6289e-05, -1.9401e-06,  2.7773e-05,\n",
       "          9.0184e-06, -1.1567e-05,  4.6665e-06,  2.5903e-06,  8.8724e-06,\n",
       "         -3.9908e-06,  7.1046e-06,  1.5266e-05,  2.5683e-06, -1.6514e-05,\n",
       "          1.3190e-05,  3.2814e-06,  3.4187e-06, -9.9853e-07,  1.9464e-05,\n",
       "         -7.5130e-06, -1.2196e-06, -1.4487e-05,  2.1240e-06,  1.4448e-05,\n",
       "          8.0455e-06, -1.6549e-06, -3.7794e-06, -1.0041e-05, -2.9926e-06,\n",
       "         -8.3238e-06, -2.5621e-05,  5.0444e-05,  2.2294e-05, -2.9290e-05,\n",
       "          6.9576e-06,  1.1338e-05, -1.3214e-05, -1.5753e-05, -6.0461e-06,\n",
       "          7.4095e-07, -9.2381e-07,  1.0992e-05, -1.2356e-05,  8.1957e-06,\n",
       "          2.2982e-05,  9.0560e-06, -4.2510e-05,  1.9114e-07,  1.5087e-06,\n",
       "         -4.7048e-06, -3.0797e-06,  2.4789e-07, -1.6178e-05,  4.7948e-05,\n",
       "          1.1159e-05, -1.7720e-05,  1.0807e-05, -1.0932e-06,  1.6818e-05,\n",
       "          1.4042e-05, -4.6271e-07, -7.7088e-06,  6.0830e-07,  5.3959e-06,\n",
       "         -3.9638e-06,  6.7294e-06, -1.0225e-05,  2.1363e-05, -5.0233e-06,\n",
       "         -6.7614e-06, -1.4696e-05, -3.5707e-06,  1.2552e-05, -1.0219e-05,\n",
       "          9.1235e-07,  2.8154e-06,  3.7597e-06,  2.0486e-06, -3.7315e-05,\n",
       "          6.9467e-06,  1.6053e-05, -1.0024e-05, -7.1982e-06,  2.8003e-06,\n",
       "          3.2607e-05, -2.9039e-06,  5.2363e-06,  3.2675e-05, -7.8752e-06,\n",
       "         -6.2659e-07,  5.4193e-06,  6.8976e-06, -1.8343e-05, -3.1733e-06,\n",
       "          7.1563e-06, -6.0318e-06, -4.5313e-06, -1.7822e-06,  4.3261e-06,\n",
       "          2.5564e-06, -8.2072e-06,  2.7254e-06,  2.0045e-05,  9.9764e-06,\n",
       "          1.3198e-05, -2.7585e-06, -2.8679e-06],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.norm1.bias': tensor([ 2.4738e-05, -2.8754e-07,  1.2228e-05, -5.4682e-06, -6.2280e-06,\n",
       "         -2.2368e-06,  1.5117e-05,  2.1492e-05, -7.9174e-06, -9.3225e-06,\n",
       "          2.2283e-05, -3.7008e-06,  1.6313e-05,  1.3413e-05,  8.9802e-06,\n",
       "          3.5521e-05,  1.9050e-05, -1.8696e-05,  2.1913e-05,  1.4462e-05,\n",
       "          1.8681e-05,  1.6922e-05,  3.9898e-06, -1.3921e-05,  1.7194e-05,\n",
       "         -2.6578e-06,  9.0132e-06,  1.5085e-05,  1.4747e-05, -1.5214e-05,\n",
       "          1.8135e-05,  1.0042e-05, -3.3997e-05,  1.7629e-07,  4.0381e-06,\n",
       "          2.4282e-05,  2.2702e-05, -5.9008e-06, -6.8083e-06,  1.0660e-05,\n",
       "         -6.3930e-06,  3.1848e-06, -7.9642e-06,  2.9270e-05, -1.0681e-05,\n",
       "         -5.3548e-06, -5.4123e-06, -2.8971e-05,  2.1165e-05,  1.5328e-05,\n",
       "          6.6917e-06, -1.9392e-05, -7.4861e-06,  1.4436e-05, -1.6832e-05,\n",
       "         -2.0073e-06,  9.7493e-07, -1.1438e-06, -1.8937e-05, -1.1603e-05,\n",
       "         -3.1141e-05, -2.1047e-05,  1.9984e-05, -5.7244e-06, -3.2930e-05,\n",
       "          2.5060e-06,  2.6406e-05, -9.7886e-06,  1.4037e-05,  1.0464e-05,\n",
       "         -1.4465e-05,  1.0560e-05, -9.0794e-06,  2.4849e-05, -4.2777e-06,\n",
       "          2.1953e-05,  1.0696e-05, -1.1034e-05, -9.8848e-06, -4.4161e-06,\n",
       "          1.9541e-05,  1.6386e-06, -2.9876e-05, -2.7215e-05, -2.4575e-05,\n",
       "          4.6184e-06,  9.8492e-06,  1.1552e-05,  2.6480e-05, -4.9614e-06,\n",
       "         -2.9034e-05, -2.9574e-05,  1.7104e-05, -1.5522e-06,  1.8551e-05,\n",
       "         -1.2457e-05, -1.0948e-06, -4.7083e-06,  1.5109e-05, -2.2497e-05,\n",
       "          1.2334e-05, -2.2578e-05,  1.2842e-05, -1.7203e-05, -7.3769e-06,\n",
       "          8.1361e-06, -1.1670e-05,  9.3719e-06,  3.8771e-05,  2.6932e-05,\n",
       "         -7.4930e-06, -1.4962e-05,  8.3241e-06, -1.3974e-05, -1.1724e-05,\n",
       "         -3.1596e-06, -1.8579e-06,  4.6810e-06, -1.0039e-05, -1.1626e-05,\n",
       "         -1.2908e-05,  9.7303e-06, -2.6750e-05,  1.8838e-05,  1.0352e-05,\n",
       "         -2.5641e-06, -1.1203e-05, -6.0785e-06],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.norm2.weight': tensor([-3.9934e-10, -4.7080e-10, -2.6480e-09, -9.9345e-10,  2.9137e-09,\n",
       "         -8.5787e-09,  1.0933e-09,  1.5341e-09,  9.2969e-10, -1.0132e-10,\n",
       "          1.0503e-09,  2.3940e-09, -1.9027e-09, -7.6978e-10, -9.4636e-10,\n",
       "          7.3718e-10, -1.6275e-09,  4.1770e-09, -1.5146e-10,  3.1920e-11,\n",
       "         -9.9992e-10, -1.7506e-09,  2.4093e-09,  3.7283e-09,  2.2639e-09,\n",
       "          7.1916e-10,  4.7668e-10,  8.1207e-10,  4.9779e-09, -2.2056e-10,\n",
       "          1.7882e-10, -5.6593e-10,  3.0531e-09, -1.9685e-09, -2.5087e-09,\n",
       "          1.1310e-09,  7.1153e-10,  3.4156e-09, -3.0876e-09, -1.1701e-09,\n",
       "         -5.9658e-10,  4.9551e-09, -9.6967e-10,  1.3345e-09,  2.5593e-09,\n",
       "          3.8779e-09,  1.4310e-09, -3.1378e-09,  1.2993e-08,  7.0657e-10,\n",
       "         -3.9623e-09,  5.8566e-10,  1.3078e-10,  6.4373e-10, -1.9908e-11,\n",
       "         -1.0957e-09,  8.3791e-10,  6.3026e-10,  9.2509e-10,  1.3276e-09,\n",
       "          1.1235e-09, -1.1154e-09,  1.8550e-09,  3.0772e-09, -2.4733e-09,\n",
       "          8.9989e-10,  7.2183e-09, -6.0318e-10, -5.0298e-10, -3.2999e-09,\n",
       "          1.6893e-09, -4.6707e-09,  1.8897e-09,  1.1357e-09,  3.9569e-09,\n",
       "         -5.4963e-09, -9.5345e-10,  4.1041e-10,  2.6890e-09,  4.3897e-09,\n",
       "          8.1682e-09,  7.5006e-10, -1.9543e-09, -6.3313e-09,  1.3760e-09,\n",
       "         -1.0620e-09, -6.0871e-11, -3.8413e-09,  1.1750e-09,  2.6843e-09,\n",
       "         -2.0476e-10,  9.0065e-09, -1.6682e-09, -2.5054e-09, -3.2194e-09,\n",
       "         -2.8667e-10, -1.4594e-09, -7.5979e-10,  5.9848e-09,  1.5834e-09,\n",
       "         -1.1853e-09, -7.3384e-09, -2.4436e-09,  7.6179e-10, -7.4378e-10,\n",
       "          4.7745e-09, -3.9773e-11, -3.0362e-10,  1.3179e-09,  1.0129e-10,\n",
       "         -3.1303e-09, -4.2154e-09,  6.4025e-10,  3.3797e-09, -9.6458e-09,\n",
       "         -1.7506e-09,  3.5564e-09,  9.6181e-10, -2.6073e-10, -2.6046e-09,\n",
       "          8.4661e-10, -8.6652e-09,  1.3019e-08, -8.4595e-11,  7.6870e-10,\n",
       "          7.3830e-10, -7.7904e-10,  7.8427e-11],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'obs_encoder.transformer_encoder.layers.0.norm2.bias': tensor([ 7.9700e-10,  1.5858e-09, -2.0977e-10, -4.8606e-10,  3.4601e-10,\n",
       "          1.9382e-09, -1.7339e-10,  1.6701e-09, -6.0013e-10, -1.7138e-09,\n",
       "          1.0413e-09, -2.4043e-09,  5.4691e-10, -1.6869e-09, -1.4580e-09,\n",
       "         -1.9153e-09, -6.7930e-10,  1.1782e-09,  2.3402e-09,  1.3373e-09,\n",
       "         -1.0416e-09,  6.9678e-10,  3.4242e-09, -4.4582e-09, -1.1414e-09,\n",
       "         -4.5327e-10, -7.7671e-11, -8.6854e-10,  1.8581e-09, -2.9172e-10,\n",
       "         -8.7612e-10,  5.3704e-10, -5.0886e-10, -9.0648e-10,  2.1094e-09,\n",
       "         -2.0625e-10, -1.2515e-09,  4.6602e-10,  7.1661e-10,  4.0206e-11,\n",
       "          8.3328e-10, -2.6693e-10,  8.6980e-10, -1.5779e-09, -4.1130e-10,\n",
       "         -5.7866e-10, -1.6944e-09,  4.2949e-10,  2.0280e-09, -1.1431e-09,\n",
       "          1.6565e-09, -1.1279e-09,  3.5064e-10, -1.1168e-09, -6.4481e-10,\n",
       "         -1.5016e-09,  1.9731e-09,  7.2434e-10,  1.3548e-12,  5.9614e-10,\n",
       "         -1.1823e-09,  2.6194e-10, -1.8126e-10, -1.8012e-09,  1.2525e-09,\n",
       "         -1.2622e-09, -1.9796e-09, -8.1271e-10,  3.5256e-10,  1.3871e-09,\n",
       "          4.4930e-11,  3.1964e-10, -3.3477e-10,  1.3910e-10,  1.0636e-09,\n",
       "         -4.4045e-10,  2.8097e-09,  1.1515e-09,  1.2364e-09, -2.1512e-09,\n",
       "         -6.9663e-11, -2.6222e-11, -3.0501e-11,  6.1634e-10, -9.0365e-11,\n",
       "         -4.6466e-10,  6.2723e-10, -1.2882e-09,  7.7813e-10, -1.1159e-10,\n",
       "          1.6606e-11,  4.8780e-10,  5.1348e-10,  7.8477e-10,  9.6249e-11,\n",
       "          6.3548e-10,  3.2832e-10, -4.2525e-10, -1.9072e-09,  2.7646e-11,\n",
       "         -1.3876e-09,  8.2850e-11, -1.0633e-09, -1.9347e-10,  1.4844e-09,\n",
       "          5.3636e-10,  1.1606e-10, -8.1279e-10, -3.5742e-09, -2.0507e-09,\n",
       "         -2.2843e-09,  1.4018e-09,  1.1492e-09,  3.9729e-10, -1.1075e-11,\n",
       "          1.5137e-09,  1.6369e-09,  8.1497e-10, -1.2603e-09, -4.3950e-10,\n",
       "         -2.8252e-10, -9.8355e-10,  4.0556e-10,  2.2701e-09,  1.3815e-09,\n",
       "          1.5210e-09,  7.4542e-10,  1.1189e-11],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'policy_mean_decoder.rnn.weight_ih_l0': tensor([[ 5.8865e-10,  4.8526e-09,  1.5642e-09,  ...,  6.3686e-10,\n",
       "          -2.4048e-09,  2.5142e-09],\n",
       "         [ 4.7509e-10,  7.3374e-09,  2.2977e-09,  ...,  7.0792e-10,\n",
       "          -3.5272e-09,  4.3143e-09],\n",
       "         [-6.6615e-10, -1.9577e-09, -8.3861e-10,  ...,  4.0930e-10,\n",
       "           7.6580e-10, -2.0147e-09],\n",
       "         ...,\n",
       "         [ 1.2854e-07,  9.2187e-07,  3.4346e-07,  ..., -1.8462e-07,\n",
       "          -2.8044e-07,  9.8695e-07],\n",
       "         [ 3.0345e-07,  7.9291e-06,  2.5426e-06,  ..., -5.1736e-07,\n",
       "          -2.3322e-06,  6.2819e-06],\n",
       "         [-3.8555e-07, -1.2452e-06, -4.1285e-07,  ..., -7.1531e-07,\n",
       "           1.2602e-06,  6.2573e-08]], grad_fn=<TBackward0>),\n",
       " 'policy_mean_decoder.rnn.weight_hh_l0': tensor([[-2.4795e-10, -3.3165e-09, -2.1843e-09,  ...,  2.4753e-09,\n",
       "          -3.7433e-09,  3.2545e-10],\n",
       "         [-2.0176e-10, -4.1277e-09, -2.9733e-09,  ...,  2.9187e-09,\n",
       "          -4.7248e-09,  4.3600e-10],\n",
       "         [-1.8893e-11,  1.1627e-09, -5.6370e-10,  ..., -5.0670e-10,\n",
       "          -8.9597e-11, -3.8529e-11],\n",
       "         ...,\n",
       "         [-7.2929e-09, -2.1838e-07, -3.9694e-08,  ...,  1.1439e-07,\n",
       "          -1.2673e-07,  2.9672e-08],\n",
       "         [ 5.6308e-08, -1.0567e-06, -4.6498e-07,  ...,  7.0097e-07,\n",
       "          -8.9575e-07,  4.3513e-09],\n",
       "         [ 4.1555e-08,  4.3976e-07,  2.8571e-07,  ..., -3.4591e-07,\n",
       "           5.6679e-07, -5.7495e-08]], grad_fn=<AddBackward0>),\n",
       " 'policy_mean_decoder.rnn.weight_ih_l0_reverse': tensor([[ 5.5825e-10, -5.7572e-09, -1.7322e-09,  ...,  1.5399e-09,\n",
       "           9.5430e-11, -5.9323e-09],\n",
       "         [-2.4007e-09, -2.0704e-09, -1.2599e-09,  ..., -1.4090e-09,\n",
       "           3.5070e-09,  3.8072e-11],\n",
       "         [-3.0796e-10, -5.0366e-09, -1.7378e-09,  ...,  1.1374e-09,\n",
       "           6.2611e-10, -4.9565e-09],\n",
       "         ...,\n",
       "         [ 1.2742e-07,  1.1752e-06,  4.3037e-07,  ..., -3.6436e-07,\n",
       "          -8.9406e-08,  1.3361e-06],\n",
       "         [ 1.6142e-07,  8.0917e-06,  2.5790e-06,  ..., -8.0497e-07,\n",
       "          -2.0224e-06,  6.7560e-06],\n",
       "         [-5.3915e-07, -2.1014e-06, -6.9755e-07,  ..., -9.5519e-07,\n",
       "           1.9104e-06, -2.7700e-07]], grad_fn=<TBackward0>),\n",
       " 'policy_mean_decoder.rnn.weight_hh_l0_reverse': tensor([[-1.1660e-10, -4.9031e-10, -5.4253e-10,  ..., -1.0460e-09,\n",
       "          -4.6548e-10, -1.1910e-09],\n",
       "         [-2.7290e-09, -1.2568e-09, -2.5463e-09,  ...,  6.8322e-10,\n",
       "           1.0419e-09, -1.4246e-09],\n",
       "         [-6.0505e-10, -6.7980e-10, -1.0335e-09,  ..., -5.1695e-10,\n",
       "          -1.5203e-10, -1.4825e-09],\n",
       "         ...,\n",
       "         [-5.7954e-08, -2.7845e-08,  1.7990e-08,  ..., -2.6984e-08,\n",
       "           6.9354e-08,  1.1371e-07],\n",
       "         [ 5.0424e-07,  5.0487e-07,  6.2834e-07,  ...,  5.2033e-07,\n",
       "           1.8601e-08,  7.0068e-07],\n",
       "         [-5.7357e-07, -4.1683e-07, -4.7110e-07,  ..., -2.9346e-07,\n",
       "           1.5487e-07, -2.4414e-07]], grad_fn=<AddBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight': tensor([[ 4.8881e-08,  1.9527e-08,  1.0850e-08,  ...,  8.3462e-08,\n",
       "          -3.9442e-08,  1.8807e-08],\n",
       "         [-9.7219e-08,  6.5325e-09, -7.9193e-08,  ..., -1.3369e-07,\n",
       "           1.3872e-08, -7.4960e-08],\n",
       "         [ 3.2425e-08, -1.2982e-08,  3.2576e-08,  ...,  2.1034e-08,\n",
       "          -1.6179e-08,  3.1681e-08],\n",
       "         ...,\n",
       "         [ 3.1587e-06, -7.2733e-07, -2.7528e-06,  ...,  4.6814e-06,\n",
       "          -6.8043e-06,  1.4863e-06],\n",
       "         [ 7.6877e-06, -6.3974e-06, -6.8772e-07,  ...,  1.4192e-05,\n",
       "          -1.6754e-05,  6.8366e-06],\n",
       "         [-4.5098e-06,  2.1233e-06,  1.1669e-06,  ..., -8.2537e-06,\n",
       "           1.0541e-05, -3.3059e-06]], grad_fn=<TBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias': tensor([ 6.3360e-08, -1.4305e-07,  2.9791e-08,  7.6192e-08, -5.6166e-08,\n",
       "          7.4737e-08, -2.8382e-07,  7.1806e-08, -2.0765e-08, -5.7939e-08,\n",
       "          1.1934e-07, -2.7265e-07,  1.4061e-08, -1.2190e-07, -1.3543e-08,\n",
       "          4.6568e-08,  1.2032e-07, -6.7073e-08, -3.7598e-07,  6.7935e-08,\n",
       "          7.7746e-08, -9.4406e-08, -2.9043e-07,  1.2439e-07,  1.4206e-07,\n",
       "         -8.3068e-08, -1.7085e-08,  1.6266e-07, -1.6565e-07,  1.7189e-07,\n",
       "         -3.6427e-07,  2.4320e-08,  1.6258e-07, -5.6396e-09,  8.4245e-09,\n",
       "          2.9251e-08, -1.0426e-07, -1.2447e-07, -4.4308e-08,  3.8732e-08,\n",
       "          2.0927e-07, -1.5916e-07, -1.7500e-07,  4.5265e-08,  9.4695e-08,\n",
       "         -4.2713e-08,  5.1338e-08,  6.5843e-08, -1.5522e-07,  6.9422e-08,\n",
       "         -1.6607e-07, -4.5419e-08,  1.5820e-07, -2.0137e-08, -7.9775e-08,\n",
       "          1.3022e-07,  1.4914e-07,  9.8771e-08,  1.7460e-07,  1.9165e-08,\n",
       "          3.4036e-08, -2.2082e-07,  1.8194e-07,  2.4111e-07,  2.1921e-10,\n",
       "          2.4649e-08, -3.3278e-08, -1.2032e-07, -8.7382e-08,  1.6057e-07,\n",
       "         -8.7049e-08,  1.2220e-07, -3.6418e-08, -3.9139e-08,  2.5414e-08,\n",
       "         -6.3106e-08,  2.1313e-07,  1.5945e-07,  1.3852e-07,  1.5491e-07,\n",
       "          8.0900e-08,  1.8861e-08, -9.7870e-08, -8.1658e-08,  2.8505e-07,\n",
       "         -5.4514e-08, -1.1391e-07,  6.3824e-08, -1.3852e-07,  2.1520e-07,\n",
       "         -5.7812e-08,  2.8612e-07,  2.6709e-08, -8.6579e-08,  9.3517e-08,\n",
       "          1.7556e-07,  1.0156e-07, -1.4960e-07, -8.6018e-08,  1.8198e-07,\n",
       "         -1.2894e-07, -1.2755e-07,  2.5608e-07, -1.4201e-07,  8.5224e-08,\n",
       "         -4.9734e-08,  3.3555e-08,  1.0362e-08, -1.1076e-07, -1.1993e-07,\n",
       "         -6.5784e-09, -3.1179e-07, -3.9411e-08,  9.2733e-08, -1.4649e-07,\n",
       "          2.0110e-08, -3.0428e-08, -1.7547e-07, -1.1567e-07,  4.9022e-08,\n",
       "          2.5047e-07,  2.1068e-07,  1.4781e-07,  1.2631e-07, -8.7166e-08,\n",
       "         -1.1217e-07,  1.2396e-08, -5.2124e-09, -7.1054e-14, -2.8422e-14,\n",
       "          6.7502e-14,  1.4211e-13,  1.2790e-13,  2.1316e-14, -4.7073e-14,\n",
       "          2.0428e-14,  7.1054e-14, -1.2434e-14,  1.4211e-14, -7.1054e-14,\n",
       "         -1.6342e-13,  9.9476e-14, -7.1054e-15,  1.4211e-14,  4.2633e-14,\n",
       "          1.5810e-13, -1.9895e-13, -4.2633e-14,  1.2790e-13, -7.1054e-14,\n",
       "          3.6415e-14, -2.8422e-14,  1.9984e-14,  8.1712e-14,  1.5987e-13,\n",
       "          3.7303e-14,  1.1369e-13,  8.5265e-14,  9.9476e-14, -1.1013e-13,\n",
       "          1.0658e-13,  6.0396e-14, -1.0658e-14,  9.9476e-14, -1.0658e-14,\n",
       "          1.9185e-13, -8.5265e-14,  7.1054e-15, -1.5632e-13,  1.4211e-13,\n",
       "         -3.5527e-14,  2.8422e-14,  3.5527e-14,  4.9738e-14,  2.8422e-14,\n",
       "          6.9278e-14, -1.4211e-14, -1.0658e-13,  9.2371e-14,  1.4211e-14,\n",
       "         -7.4607e-14,  9.9476e-14,  9.9476e-14,  3.0198e-14, -7.1054e-14,\n",
       "          1.4211e-13,  6.3949e-14, -4.9738e-14,  3.5527e-14,  2.3093e-14,\n",
       "          2.1316e-13, -1.0658e-13,  1.0658e-13,  8.5265e-14, -2.1316e-14,\n",
       "          2.1316e-14,  1.3323e-13, -2.8422e-14, -1.2790e-13, -5.3291e-14,\n",
       "         -1.9540e-14, -4.9738e-14,  3.5527e-14,  1.4211e-14, -1.0658e-13,\n",
       "          0.0000e+00, -2.8422e-14,  3.5527e-15,  1.0658e-13,  1.2967e-13,\n",
       "          2.1316e-14,  1.4211e-13,  4.9738e-14,  4.2633e-14, -3.5527e-15,\n",
       "          1.5632e-13, -7.4607e-14, -4.2633e-14, -2.7356e-13, -2.6645e-14,\n",
       "          5.6843e-14,  3.5527e-14,  1.1902e-13, -7.8160e-14,  9.9476e-14,\n",
       "          7.1054e-14, -9.9476e-14, -1.4211e-14, -8.0824e-14, -1.0658e-13,\n",
       "         -5.6843e-14, -7.1054e-14,  1.7053e-13,  6.3949e-14, -8.8818e-14,\n",
       "         -5.6843e-14,  1.7764e-15,  2.8422e-14, -1.2790e-13,  3.7303e-14,\n",
       "         -3.9080e-14,  0.0000e+00,  5.1514e-14,  0.0000e+00,  2.4869e-14,\n",
       "         -7.4607e-14,  1.7053e-13,  5.6843e-14, -2.0250e-13, -3.9080e-14,\n",
       "          1.0658e-13, -9.3259e-14, -1.4211e-14, -2.8422e-14, -8.1712e-14,\n",
       "         -7.1054e-14, -2.9897e-06,  1.9559e-06, -3.4949e-06, -6.1557e-07,\n",
       "         -2.3467e-06, -4.2929e-07, -2.8875e-07, -3.3495e-06, -1.9360e-06,\n",
       "          4.0287e-06,  6.3655e-06,  3.8670e-06,  2.6037e-06, -5.6015e-06,\n",
       "         -6.4512e-08,  3.9161e-06, -1.0304e-05,  5.7930e-06, -6.7163e-06,\n",
       "          5.0575e-06, -1.1805e-06,  4.1005e-07, -1.0220e-05,  3.1654e-06,\n",
       "          2.9354e-06, -6.3298e-06, -9.7932e-08,  6.5962e-06, -8.0025e-06,\n",
       "         -2.3076e-06,  4.6283e-06, -2.5907e-06, -3.4655e-06, -4.3107e-06,\n",
       "         -5.6733e-06,  3.1543e-06, -9.0569e-06, -8.8754e-06, -8.1656e-06,\n",
       "          4.7668e-07,  1.9515e-06,  7.9999e-06, -1.4112e-06,  5.9560e-06,\n",
       "         -4.9849e-06, -7.0662e-06, -2.9657e-06,  1.0732e-05,  3.7329e-07,\n",
       "          2.8902e-06, -3.0094e-06, -5.6158e-06, -3.8066e-06, -7.9169e-06,\n",
       "          1.1108e-06,  1.6615e-06,  3.3731e-06,  1.2764e-06, -1.7810e-07,\n",
       "          1.3596e-06,  1.0728e-05,  5.4169e-06,  3.2579e-06, -9.1579e-06,\n",
       "         -1.1163e-06, -1.5231e-06,  2.2931e-06, -6.2027e-06, -3.3210e-06,\n",
       "          4.3731e-06,  1.1140e-06,  1.0173e-05, -4.1721e-06,  6.0167e-06,\n",
       "         -2.9896e-06, -2.0381e-06, -2.1904e-06,  4.8387e-07,  2.6204e-06,\n",
       "         -1.2660e-06,  2.5250e-06,  5.9552e-06,  5.8690e-06,  2.2961e-06,\n",
       "         -7.6637e-07, -4.5530e-06,  9.3353e-07, -2.9714e-06, -7.7777e-07,\n",
       "          4.4058e-06, -8.8447e-06,  2.7741e-06,  6.8376e-06, -4.8722e-06,\n",
       "          2.6444e-06,  2.7261e-07,  6.4098e-06,  8.4739e-06,  1.1362e-06,\n",
       "          6.3493e-06, -2.8668e-06,  3.8796e-06, -9.5111e-06, -3.1452e-06,\n",
       "          3.0462e-07, -1.9416e-06,  3.6545e-06, -1.8407e-06, -5.8775e-06,\n",
       "         -1.9640e-07, -2.0046e-06,  8.4413e-06, -1.0249e-05,  9.8328e-06,\n",
       "         -6.6690e-07,  3.7032e-06, -9.3804e-07,  3.8588e-06,  3.4581e-06,\n",
       "         -8.4504e-07, -2.5634e-06, -8.4181e-06, -2.1345e-06,  2.8606e-06,\n",
       "         -5.3481e-06,  3.4436e-06,  9.7927e-06, -5.0999e-06],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight': tensor([[ 3.3658e-07, -3.2776e-07,  3.8074e-07,  ...,  1.0438e-08,\n",
       "           2.1052e-07,  5.7540e-07],\n",
       "         [-1.7228e-06,  6.7893e-07,  1.2458e-07,  ..., -5.5515e-06,\n",
       "          -6.4779e-07,  8.0907e-07],\n",
       "         [ 1.1740e-06, -1.6851e-06,  1.9633e-06,  ...,  8.7370e-08,\n",
       "           9.2912e-07,  3.1024e-06],\n",
       "         ...,\n",
       "         [ 1.2446e-06, -1.4085e-06,  1.5484e-06,  ...,  5.0157e-07,\n",
       "           8.9994e-07,  2.2324e-06],\n",
       "         [-1.5594e-06,  3.0276e-06, -4.4847e-06,  ...,  2.9297e-06,\n",
       "          -1.3836e-06, -7.4986e-06],\n",
       "         [ 3.8556e-07, -1.8214e-06,  3.0733e-06,  ..., -3.4544e-06,\n",
       "           6.8056e-07,  5.4030e-06]], grad_fn=<TBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias': tensor([-2.1193e-07, -1.3711e-06, -2.7925e-06,  2.5017e-05,  5.5283e-07,\n",
       "         -1.2125e-05, -1.2199e-06,  3.2995e-07, -3.8386e-06,  1.0758e-06,\n",
       "          5.2636e-07,  2.2806e-05, -1.2761e-06, -3.7247e-06,  3.0478e-06,\n",
       "         -3.5100e-06,  1.1708e-05, -5.4827e-07, -4.6994e-06, -3.9313e-06,\n",
       "          5.5384e-06, -6.7068e-06,  3.7452e-06, -8.3557e-06,  7.4916e-06,\n",
       "         -1.2372e-05, -9.4592e-06, -2.3742e-05, -1.6981e-05, -8.5719e-06,\n",
       "          3.2456e-06, -3.9016e-06, -9.6197e-06, -4.5073e-06, -2.3742e-06,\n",
       "          1.4149e-07, -1.1599e-05, -2.5870e-06, -1.3991e-05, -3.8391e-06,\n",
       "         -6.9821e-06,  4.9019e-06, -3.6848e-06,  4.5242e-06,  3.0276e-06,\n",
       "          1.1251e-05,  1.1799e-06, -1.0096e-05,  2.1158e-05, -6.3964e-06,\n",
       "          1.2158e-06, -6.0573e-06,  7.4459e-07,  1.7606e-06, -1.3172e-05,\n",
       "         -2.1572e-06,  4.9563e-06,  6.0881e-06, -8.3660e-06, -9.6882e-06,\n",
       "          9.7801e-07, -1.0102e-05,  3.9408e-06,  3.7881e-06,  2.9647e-06,\n",
       "          8.0668e-06,  2.4687e-05, -1.0038e-05, -9.3920e-08, -3.0292e-06,\n",
       "         -2.6655e-07, -1.3947e-05, -1.5953e-05,  8.1609e-06,  1.8905e-06,\n",
       "          1.1673e-05,  9.2997e-06, -9.6818e-07, -5.7351e-06,  1.8214e-06,\n",
       "          6.1373e-06,  1.0590e-05, -3.6993e-06, -1.4581e-05, -1.4852e-06,\n",
       "          7.7526e-06, -1.0510e-06,  3.7172e-06,  9.0955e-06, -1.4035e-05,\n",
       "          1.1556e-06,  1.0067e-06,  7.0578e-06, -1.6290e-05,  4.3307e-06,\n",
       "         -1.7439e-06,  2.9430e-06, -3.6636e-06, -2.2764e-05,  1.1079e-05,\n",
       "         -1.5987e-06, -2.5470e-05, -9.5157e-06, -1.0262e-05, -4.2177e-07,\n",
       "         -2.2483e-06,  4.8188e-06, -2.7610e-06, -8.2045e-06, -1.6731e-05,\n",
       "          9.3483e-06,  1.8654e-05,  1.6026e-07,  8.9374e-06, -1.0865e-05,\n",
       "          7.8784e-06, -6.7635e-06,  4.6576e-06,  1.0684e-06,  8.2310e-06,\n",
       "          4.7707e-07,  4.8849e-06, -1.7830e-05,  3.8849e-07,  7.2761e-07,\n",
       "         -1.7215e-06,  8.1998e-06, -6.3406e-06], grad_fn=<ViewBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.linear1.weight': tensor([[-2.3076e-08, -2.9349e-08, -4.5752e-08,  ..., -1.7022e-07,\n",
       "           6.3849e-08,  1.3032e-07],\n",
       "         [ 6.3297e-09, -6.4914e-08, -3.0072e-08,  ..., -8.1554e-08,\n",
       "          -6.1580e-09,  5.9285e-08],\n",
       "         [ 2.6501e-09, -4.8222e-08,  1.1472e-08,  ..., -3.6131e-09,\n",
       "          -2.6752e-08, -2.5917e-08],\n",
       "         ...,\n",
       "         [ 1.1390e-08,  4.3870e-08,  2.7407e-09,  ..., -5.8641e-08,\n",
       "           6.6904e-10,  7.7397e-09],\n",
       "         [-7.5443e-09,  4.2960e-08, -4.2082e-08,  ..., -1.2544e-07,\n",
       "           4.3943e-08,  1.0899e-07],\n",
       "         [ 1.6572e-08,  2.1021e-08,  1.0282e-08,  ..., -9.6094e-09,\n",
       "          -1.3104e-08, -3.3714e-08]], grad_fn=<TBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.linear1.bias': tensor([ 1.2181e-07,  4.4582e-08, -1.7727e-08, -2.6565e-09,  3.8314e-08,\n",
       "         -9.3319e-08,  5.6449e-08, -3.6029e-08, -1.9950e-08, -2.6353e-08,\n",
       "          4.2698e-08,  4.0036e-08,  7.5089e-08, -6.0718e-08,  1.5359e-07,\n",
       "          2.3420e-08,  2.1862e-08,  3.8793e-09,  2.7355e-08,  3.0074e-08,\n",
       "          4.4868e-08, -1.8940e-08, -6.8146e-08,  8.7047e-09, -1.8932e-09,\n",
       "          4.9828e-09, -5.0436e-08, -6.0221e-09, -9.5404e-08, -7.2918e-08,\n",
       "         -3.7444e-08,  6.7304e-08,  3.1439e-08,  1.9608e-09, -4.2472e-08,\n",
       "         -7.7557e-08,  3.3262e-08, -2.0074e-08,  1.0925e-08,  8.8084e-08,\n",
       "          4.4815e-08, -1.8331e-08,  1.2402e-08, -3.0092e-08,  3.4093e-08,\n",
       "          2.0335e-08,  7.6296e-08, -2.7073e-09, -3.1317e-08, -1.1523e-08,\n",
       "          4.2146e-08, -6.8882e-08,  8.8057e-08,  2.4825e-08, -1.2359e-07,\n",
       "         -4.3389e-08, -4.5169e-08,  7.0272e-09, -8.0405e-09,  5.7013e-09,\n",
       "          9.8468e-08, -1.9065e-08, -5.4266e-08,  8.7166e-08,  4.0684e-08,\n",
       "          3.4934e-08, -3.3179e-08,  5.2128e-08,  1.0674e-08,  1.0116e-07,\n",
       "          6.2556e-08, -6.3325e-08,  4.2132e-08, -4.3371e-08, -2.3019e-08,\n",
       "          3.1693e-08,  3.1503e-08,  4.3268e-08,  1.2732e-08, -6.3077e-08,\n",
       "         -1.2030e-08, -1.7374e-08, -6.9802e-10,  9.0131e-09, -2.7624e-08,\n",
       "          5.9758e-08, -3.5780e-08,  3.7965e-08,  2.0883e-08, -1.4742e-08,\n",
       "          6.5571e-08,  3.2021e-08,  3.4159e-08, -3.2225e-08,  4.0365e-08,\n",
       "          6.3885e-08,  9.7025e-09, -3.0821e-08,  2.5404e-08,  3.0971e-08,\n",
       "         -3.3323e-08, -5.9922e-08, -3.4109e-08, -1.8213e-09,  3.0180e-08,\n",
       "         -3.6584e-08, -8.2671e-09,  3.1796e-08, -6.1788e-08,  3.7823e-08,\n",
       "         -4.7461e-08,  1.5686e-08, -4.3818e-08,  4.3578e-08,  2.6808e-08,\n",
       "          5.9976e-09,  2.4541e-08, -2.7386e-08, -6.0566e-08, -2.5235e-08,\n",
       "         -2.2790e-08,  1.2470e-08, -1.7015e-08, -3.1506e-08, -2.2553e-08,\n",
       "          1.4644e-08,  1.0133e-07, -4.7050e-09], grad_fn=<ViewBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.linear2.weight': tensor([[ 3.3875e-09, -8.5974e-09,  1.2196e-09,  ...,  5.6340e-09,\n",
       "           7.0652e-09,  1.6471e-09],\n",
       "         [ 4.1339e-08, -9.0533e-09, -6.4536e-08,  ..., -4.1267e-08,\n",
       "           9.2592e-08, -3.7965e-08],\n",
       "         [ 9.1638e-09, -5.1370e-09, -1.5672e-08,  ..., -5.3760e-09,\n",
       "           2.4527e-08, -1.1718e-08],\n",
       "         ...,\n",
       "         [ 1.7467e-08, -1.3284e-08, -1.3926e-08,  ...,  2.7564e-09,\n",
       "           2.5363e-08,  3.2474e-09],\n",
       "         [-2.3526e-08,  2.1684e-08,  3.3017e-08,  ..., -3.3092e-10,\n",
       "          -5.3971e-08,  1.5832e-08],\n",
       "         [ 2.2496e-08,  2.4226e-08, -2.2635e-08,  ..., -3.0121e-08,\n",
       "           2.6160e-08, -1.2493e-08]], grad_fn=<TBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.linear2.bias': tensor([-1.3455e-06, -1.4659e-05, -4.4315e-06,  1.9791e-05,  9.3063e-06,\n",
       "         -1.4197e-05, -2.0183e-06,  1.3972e-05, -3.1893e-07,  2.1352e-06,\n",
       "         -6.1729e-06,  2.4578e-05,  3.7676e-06,  1.7373e-06,  1.7693e-06,\n",
       "         -2.7364e-06,  9.8848e-06,  2.9167e-05, -6.4660e-06, -5.9332e-06,\n",
       "          2.8557e-06, -7.9824e-06,  6.3723e-06, -2.7624e-07,  8.4609e-06,\n",
       "          1.1115e-05, -9.7948e-06, -1.4109e-05, -1.8372e-05, -7.1142e-06,\n",
       "         -3.9633e-06, -7.2548e-06, -9.1122e-06,  1.0310e-05,  2.1576e-06,\n",
       "          5.6869e-06, -6.0975e-06, -6.2843e-06,  4.0333e-06, -3.8140e-06,\n",
       "         -1.3330e-06,  1.2074e-05, -4.0208e-06,  6.8962e-06, -6.3195e-06,\n",
       "          1.3181e-05,  1.5278e-06, -1.0603e-05,  4.1210e-05,  4.6645e-09,\n",
       "         -6.2373e-06,  1.3555e-05,  9.6770e-07,  6.3490e-07, -9.2091e-06,\n",
       "         -2.2594e-06,  1.1849e-05,  6.0359e-06, -1.3349e-05,  1.8456e-06,\n",
       "         -2.7335e-06, -7.1085e-06,  6.2940e-06,  5.8988e-06, -5.8350e-06,\n",
       "          7.7647e-06,  1.9257e-05, -9.5501e-06,  1.2051e-06, -3.5552e-06,\n",
       "          1.9758e-06, -1.6536e-05,  8.4022e-06,  7.8240e-06,  5.5892e-06,\n",
       "          1.6017e-05,  1.3885e-05, -7.2030e-08, -1.1442e-05,  2.7359e-06,\n",
       "         -9.0964e-06, -1.3933e-05, -1.2379e-08, -3.8795e-06, -1.2091e-06,\n",
       "          9.7755e-06,  1.8241e-06,  4.4176e-06,  7.8179e-06,  4.9947e-06,\n",
       "          5.5201e-08,  5.2447e-07,  2.5145e-06, -2.1434e-05, -2.4979e-06,\n",
       "          5.4450e-07,  1.3938e-06, -2.6058e-06, -2.6631e-05,  1.0456e-05,\n",
       "         -4.9908e-06, -2.5617e-05, -1.1928e-05, -3.6197e-06, -1.3103e-06,\n",
       "          4.5441e-06,  3.5355e-06, -1.2417e-06, -8.7424e-06, -1.6323e-05,\n",
       "          1.3839e-05, -2.2441e-06, -1.9676e-06,  5.6160e-06,  1.7945e-06,\n",
       "          4.8413e-06,  9.7478e-06,  6.4263e-06,  1.3556e-06,  5.6099e-06,\n",
       "          1.2074e-06,  1.2417e-05, -1.3029e-05,  4.2644e-06, -2.2476e-06,\n",
       "         -3.2005e-06,  8.7792e-06, -4.4244e-06], grad_fn=<ViewBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.norm1.weight': tensor([-1.5561e-06,  2.3966e-06,  3.5831e-07, -2.7419e-07, -5.4070e-06,\n",
       "          2.8129e-06,  2.3182e-06, -5.5148e-07, -3.0037e-08,  4.1129e-07,\n",
       "          1.6220e-06, -1.7680e-06,  2.7022e-06,  3.5255e-06,  4.2357e-06,\n",
       "          1.4848e-06, -3.5559e-06,  2.5735e-06,  3.7186e-06,  6.0728e-07,\n",
       "          6.8843e-07, -8.0851e-06, -1.3758e-06,  1.3714e-07, -6.8645e-07,\n",
       "         -4.5181e-08,  1.0388e-06, -3.9168e-06,  2.2435e-07,  1.9239e-06,\n",
       "          1.3901e-06,  3.6806e-06, -8.2827e-07, -8.0900e-07,  2.6001e-06,\n",
       "         -8.1600e-07,  1.4938e-07,  2.7828e-06, -3.7592e-06, -1.0906e-06,\n",
       "          1.1034e-07,  7.0782e-07, -1.5792e-06,  9.0629e-07, -1.7188e-06,\n",
       "          5.0537e-06, -1.8584e-06,  1.3095e-06,  3.1935e-06,  7.1929e-07,\n",
       "          2.2649e-06, -1.4745e-06, -4.3580e-06,  3.2772e-06,  8.6579e-07,\n",
       "         -1.6009e-06, -6.3451e-07, -3.6582e-06,  9.9251e-07,  5.1038e-07,\n",
       "         -7.1510e-06, -2.0236e-06,  2.4576e-07,  4.5527e-07,  1.8777e-06,\n",
       "          4.9474e-06, -2.6299e-06,  3.6229e-06, -8.9625e-06,  5.5282e-06,\n",
       "         -1.3264e-05,  3.9583e-06,  2.3543e-07,  3.4808e-07, -1.1271e-06,\n",
       "          3.6956e-07,  6.5600e-09, -3.2567e-08,  6.4729e-07,  1.8038e-07,\n",
       "          3.6374e-06,  5.9702e-06, -9.6433e-07,  2.0598e-07, -1.1931e-07,\n",
       "         -5.8687e-06, -3.9097e-06, -2.2078e-07,  2.2195e-06, -1.5605e-06,\n",
       "         -2.1025e-07, -1.5924e-06,  6.0825e-06, -6.9510e-06, -3.0462e-06,\n",
       "          6.6394e-07,  3.0925e-06, -3.8043e-06, -1.0914e-06,  3.3273e-06,\n",
       "          3.0947e-06,  1.9377e-07, -3.4771e-06,  5.2643e-07,  1.8840e-06,\n",
       "          4.0287e-06, -1.6375e-07, -1.5459e-06,  5.6090e-06, -5.4316e-06,\n",
       "         -7.4539e-06, -2.0158e-06, -2.6014e-06, -1.3145e-07, -2.1407e-06,\n",
       "         -4.0965e-06,  1.0296e-07, -7.5674e-07, -1.1813e-06,  1.8897e-06,\n",
       "          6.6824e-09, -6.0468e-07,  4.3438e-07,  1.5083e-06,  3.0416e-06,\n",
       "         -5.9365e-07, -2.9693e-06, -1.4532e-06],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.norm1.bias': tensor([-2.7580e-06, -1.2917e-06,  1.0287e-06,  9.0853e-07, -6.1106e-06,\n",
       "          3.8473e-06,  1.4893e-06,  9.4891e-07,  4.0219e-06,  2.9711e-06,\n",
       "         -2.2004e-06,  3.9279e-06,  1.9061e-06, -4.8577e-06,  2.2354e-06,\n",
       "         -5.2115e-06, -5.9575e-06,  2.9471e-06, -3.0522e-06,  2.9227e-06,\n",
       "          3.1281e-06, -5.5711e-06,  7.6559e-06,  1.1606e-06, -1.3792e-06,\n",
       "         -5.9268e-07,  5.8684e-07,  3.1370e-06,  6.7136e-07, -3.6781e-06,\n",
       "         -4.6870e-06, -3.9565e-06, -6.5742e-07, -1.0463e-06, -3.8144e-06,\n",
       "          1.4328e-07, -7.5781e-07, -2.4917e-06,  3.3505e-06, -1.0356e-06,\n",
       "         -1.7186e-06, -1.1325e-06,  2.0796e-06, -7.9855e-07,  5.1288e-06,\n",
       "          2.8145e-06,  6.3041e-06, -1.5293e-06,  3.9434e-06, -1.4909e-06,\n",
       "         -1.6251e-06, -1.8055e-06,  3.7993e-06, -3.0752e-08, -5.0407e-06,\n",
       "          1.2570e-06,  4.1321e-09, -3.0745e-06,  8.8802e-07, -1.6778e-06,\n",
       "          6.6934e-06,  5.6908e-06, -1.4274e-06,  1.5632e-06,  2.2920e-06,\n",
       "          3.0744e-06, -7.1235e-06, -7.4452e-07, -3.8202e-06,  4.3973e-06,\n",
       "         -7.0435e-06,  2.8051e-06,  5.9910e-07, -6.7140e-07,  4.0944e-06,\n",
       "         -1.7139e-07, -1.0581e-06, -9.7305e-07, -2.7982e-07, -2.9684e-07,\n",
       "          4.2855e-06, -5.2102e-06, -1.1498e-06,  2.1273e-06,  1.2261e-06,\n",
       "         -3.7820e-06, -2.8895e-06,  4.0931e-07, -2.7894e-06, -6.3235e-07,\n",
       "          6.0991e-07, -1.3712e-06, -5.2516e-06, -4.3301e-06, -2.6864e-06,\n",
       "         -1.5809e-06, -3.4142e-06,  3.5567e-06, -1.8404e-06, -2.8810e-06,\n",
       "         -2.7506e-06,  3.7290e-07, -3.1389e-06,  2.4508e-07,  9.3233e-07,\n",
       "         -4.0772e-06, -1.2946e-06, -4.6698e-06, -4.6406e-06,  5.6129e-06,\n",
       "          4.0083e-06, -4.1789e-06, -2.9384e-06,  3.6828e-06,  2.4052e-06,\n",
       "         -6.8009e-06, -3.7329e-06, -1.7017e-07, -1.6201e-06,  2.6065e-06,\n",
       "          1.5803e-06, -6.3461e-07, -3.3477e-06, -2.1390e-06, -2.1851e-06,\n",
       "         -2.8727e-07,  3.6161e-06, -1.9865e-06],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.norm2.weight': tensor([-1.4685e-11, -9.0330e-10, -7.9941e-11,  5.2873e-10,  4.5384e-10,\n",
       "         -1.3051e-10, -5.5583e-11, -6.5857e-13, -3.6749e-11,  1.2935e-10,\n",
       "         -2.4755e-11,  2.4918e-10, -3.1008e-10,  7.6151e-10, -1.8293e-10,\n",
       "         -4.2629e-11,  3.6848e-11,  2.7661e-12, -2.1451e-10, -3.6834e-11,\n",
       "          1.9554e-11, -2.3405e-10,  2.2636e-10, -1.6323e-10,  2.7204e-12,\n",
       "          1.1168e-09, -5.4025e-11, -4.3160e-11,  6.5497e-10, -2.0945e-10,\n",
       "          3.6308e-11,  1.1600e-10, -4.6726e-10,  4.9621e-11, -2.1936e-10,\n",
       "          2.2610e-10, -9.0246e-11, -3.0720e-10,  1.0680e-09, -2.5016e-10,\n",
       "          6.9681e-11,  5.8537e-12,  1.5528e-10,  6.0047e-10,  3.0677e-12,\n",
       "          4.0020e-11,  2.8116e-11, -5.5498e-10,  9.9334e-10,  3.3305e-10,\n",
       "         -2.5952e-11, -1.8797e-10, -3.3275e-10,  2.9715e-10, -1.1070e-10,\n",
       "          1.1400e-10, -1.3924e-10, -6.2882e-10, -4.0011e-11,  2.2459e-12,\n",
       "         -1.4828e-10, -1.3267e-10, -1.5682e-10,  3.0154e-11, -7.4485e-11,\n",
       "         -3.0798e-10, -5.7994e-11,  1.7937e-11, -3.1793e-11,  6.2139e-11,\n",
       "          1.0017e-10, -9.8135e-11,  6.8434e-11, -5.7914e-10, -1.2175e-09,\n",
       "          1.2131e-09, -3.2223e-10,  3.5019e-10,  1.1431e-10, -2.8271e-13,\n",
       "         -3.9515e-10,  7.5732e-10, -8.1868e-11, -1.0336e-10, -7.9093e-11,\n",
       "         -5.6535e-10, -1.7492e-11, -5.2579e-10,  2.2261e-10,  4.7000e-11,\n",
       "          1.4366e-10, -1.4521e-10,  3.6656e-12,  8.3747e-10, -2.8164e-11,\n",
       "         -5.2148e-13, -2.0210e-11, -3.5259e-10, -3.0574e-10, -7.3676e-11,\n",
       "          2.8744e-10,  1.1418e-10,  1.7553e-10,  2.9779e-10,  1.7639e-10,\n",
       "          2.5664e-10, -2.0093e-10, -1.1494e-10,  6.8327e-11, -1.2034e-10,\n",
       "          8.1794e-11, -1.9513e-10, -4.7158e-11,  1.7363e-09, -2.2881e-11,\n",
       "         -1.2058e-09,  2.9259e-10,  8.0154e-11, -5.7832e-11,  3.6632e-10,\n",
       "         -6.3893e-11, -1.3404e-10,  2.0348e-10, -1.4696e-10,  1.8418e-10,\n",
       "          8.0830e-12, -1.4274e-10,  5.5122e-10],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'policy_mean_decoder.transformer_encoder.layers.0.norm2.bias': tensor([ 7.7518e-11,  8.5148e-11, -3.5301e-11, -5.0601e-10,  3.5479e-10,\n",
       "         -3.6605e-10, -6.2811e-10, -8.8912e-11, -9.6485e-11, -7.3262e-11,\n",
       "         -2.1280e-10, -1.2238e-10, -1.5588e-10, -5.0433e-10,  1.2157e-10,\n",
       "          1.4349e-10,  4.6178e-11, -4.0652e-10, -1.3929e-10,  1.1213e-10,\n",
       "          1.7404e-11,  5.0793e-10,  1.0209e-09, -1.5920e-10, -2.6104e-11,\n",
       "          4.0807e-10,  4.9996e-10,  1.4920e-10, -8.5776e-10, -2.4099e-10,\n",
       "          6.3479e-11,  2.9168e-10, -3.2892e-10,  2.1982e-10, -7.0361e-11,\n",
       "          3.3531e-10,  1.2312e-10, -3.4289e-10,  5.3778e-10,  2.9328e-10,\n",
       "          7.4080e-11, -2.1288e-12, -1.6515e-10,  5.9506e-10,  4.6765e-10,\n",
       "         -3.3743e-11,  1.1392e-11,  2.0876e-10,  4.0433e-10,  3.6839e-10,\n",
       "          5.0586e-10,  9.2449e-11, -4.4632e-10, -1.8408e-10, -2.4344e-10,\n",
       "         -2.6053e-10,  2.5153e-11,  4.8719e-10, -8.3042e-11,  8.1875e-11,\n",
       "          4.4908e-10,  2.1448e-10,  1.0242e-10,  8.1683e-11,  1.6032e-10,\n",
       "         -5.9489e-10, -1.1486e-10, -1.5905e-10,  1.3493e-10, -2.9694e-10,\n",
       "          6.9395e-10, -2.3219e-10, -2.1037e-10, -3.8881e-10,  5.5095e-10,\n",
       "         -6.1757e-10, -1.9597e-10, -4.3855e-10, -8.8721e-11, -2.7503e-10,\n",
       "         -1.6662e-10, -5.8001e-10, -4.6217e-10, -7.4028e-11,  1.5672e-10,\n",
       "          7.9650e-11,  4.1046e-10, -3.4167e-10, -4.2578e-10,  2.2858e-10,\n",
       "          6.5185e-10,  4.0208e-10,  3.7374e-11, -3.9191e-10, -3.8039e-11,\n",
       "          6.4699e-10, -1.2036e-10, -4.1268e-10, -1.5945e-10,  2.2936e-11,\n",
       "          1.5889e-10, -3.4715e-11,  1.5377e-10, -3.6897e-10, -2.6315e-10,\n",
       "         -1.4997e-10,  1.1949e-10, -1.6656e-10,  3.1504e-10,  2.0490e-10,\n",
       "          5.0865e-11,  3.2210e-10,  3.1773e-11,  9.8382e-10,  3.0020e-10,\n",
       "         -9.0131e-10,  1.0032e-09,  6.1812e-10,  5.7796e-12,  1.0330e-09,\n",
       "         -3.4110e-11, -4.7869e-10,  4.1875e-10,  1.6334e-10, -1.5638e-10,\n",
       "         -1.0695e-11, -5.3078e-10,  4.0739e-10],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'policy_logstd_decoder.rnn.weight_ih_l0': tensor([[-1.6525e-09, -9.8688e-09, -3.0424e-09,  ..., -6.4568e-09,\n",
       "           1.3289e-08, -4.0453e-10],\n",
       "         [ 5.5396e-10, -3.8130e-09, -1.0389e-09,  ...,  1.0908e-09,\n",
       "          -1.3651e-10, -3.8658e-09],\n",
       "         [-1.5090e-09,  6.0296e-09,  1.1512e-09,  ...,  1.4447e-09,\n",
       "          -2.9515e-09,  2.6108e-09],\n",
       "         ...,\n",
       "         [ 7.9786e-07, -2.0700e-07,  3.0159e-07,  ..., -2.1391e-06,\n",
       "           2.5397e-06,  2.1166e-06],\n",
       "         [-3.0281e-06,  1.1593e-05,  2.7047e-06,  ...,  1.8235e-07,\n",
       "          -4.4973e-06,  9.3521e-06],\n",
       "         [-2.1605e-07,  1.0786e-05,  2.9968e-06,  ...,  3.1373e-06,\n",
       "          -7.6743e-06,  4.4308e-06]], grad_fn=<TBackward0>),\n",
       " 'policy_logstd_decoder.rnn.weight_hh_l0': tensor([[-5.3724e-10,  4.7160e-09,  1.4552e-08,  ...,  4.4689e-09,\n",
       "          -5.1916e-09, -6.6597e-09],\n",
       "         [-1.8604e-10,  3.5234e-10,  1.0111e-09,  ..., -1.2500e-09,\n",
       "           8.8820e-10, -1.5463e-12],\n",
       "         [ 3.2200e-09, -2.7438e-09, -6.9071e-09,  ...,  6.1314e-10,\n",
       "           4.4729e-10,  1.3983e-09],\n",
       "         ...,\n",
       "         [-8.0700e-07,  6.5327e-07,  1.5540e-06,  ...,  1.3358e-07,\n",
       "          -3.2388e-07, -4.3171e-07],\n",
       "         [ 1.3173e-06, -1.4254e-06, -3.5708e-06,  ...,  6.3046e-07,\n",
       "           2.6819e-08,  8.7222e-07],\n",
       "         [ 7.0499e-07, -1.2811e-06, -3.7189e-06,  ..., -5.5217e-07,\n",
       "           7.5561e-07,  1.1863e-06]], grad_fn=<AddBackward0>),\n",
       " 'policy_logstd_decoder.rnn.weight_ih_l0_reverse': tensor([[ 8.4089e-10, -1.1800e-08, -3.5291e-09,  ...,  1.4002e-09,\n",
       "           2.9400e-09, -1.0517e-08],\n",
       "         [ 6.0770e-11, -7.4065e-09, -2.2021e-09,  ..., -1.0920e-09,\n",
       "           3.6584e-09, -4.1763e-09],\n",
       "         [-3.5916e-10,  5.0448e-09,  1.4111e-09,  ...,  9.3504e-10,\n",
       "          -3.1445e-09,  2.6521e-09],\n",
       "         ...,\n",
       "         [ 8.6208e-07, -4.8795e-06, -1.0703e-06,  ..., -2.4626e-06,\n",
       "           4.7229e-06, -1.0126e-06],\n",
       "         [-2.6531e-06,  4.4667e-06,  7.0052e-07,  ..., -3.9519e-07,\n",
       "          -1.2781e-06,  4.8487e-06],\n",
       "         [-3.0091e-07,  6.2944e-06,  1.6887e-06,  ...,  2.4240e-06,\n",
       "          -5.3641e-06,  2.2245e-06]], grad_fn=<TBackward0>),\n",
       " 'policy_logstd_decoder.rnn.weight_hh_l0_reverse': tensor([[ 4.2175e-09, -4.9792e-10,  2.2107e-09,  ...,  2.1408e-09,\n",
       "          -4.0867e-09, -1.8959e-09],\n",
       "         [ 2.8965e-09, -4.8847e-10, -4.4852e-10,  ...,  1.2989e-09,\n",
       "          -2.1163e-09, -1.6099e-09],\n",
       "         [-3.2083e-09,  7.6457e-10,  1.1506e-09,  ..., -1.4835e-09,\n",
       "           1.9860e-09,  1.8043e-09],\n",
       "         ...,\n",
       "         [ 8.8130e-07, -3.4723e-07, -8.2993e-07,  ...,  6.1186e-07,\n",
       "          -3.8614e-07, -6.0298e-07],\n",
       "         [ 4.0842e-07,  2.9951e-07, -3.0024e-07,  ..., -6.4350e-07,\n",
       "          -9.6833e-08, -7.4971e-08],\n",
       "         [-1.0059e-06,  3.8560e-07,  6.9521e-07,  ..., -6.9537e-07,\n",
       "           4.2758e-07,  6.6010e-07]], grad_fn=<AddBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight': tensor([[-1.2410e-07, -2.3068e-07, -8.6276e-07,  ..., -2.8704e-07,\n",
       "           3.5919e-07,  3.9672e-07],\n",
       "         [ 1.3127e-07,  3.3603e-08,  1.3155e-07,  ..., -2.3481e-08,\n",
       "          -5.0812e-08, -4.3291e-08],\n",
       "         [-2.2401e-07, -2.6614e-07, -9.1586e-07,  ..., -3.2693e-07,\n",
       "           4.1734e-07,  4.7834e-07],\n",
       "         ...,\n",
       "         [-1.2274e-05, -9.4534e-06, -2.9189e-05,  ..., -1.5504e-05,\n",
       "           1.7184e-05,  2.1842e-05],\n",
       "         [ 3.1834e-06,  3.7060e-06,  1.2022e-05,  ...,  9.1849e-06,\n",
       "          -7.1162e-06, -1.1176e-05],\n",
       "         [-9.2947e-06,  1.1385e-06, -3.1577e-06,  ...,  2.8559e-06,\n",
       "           1.5163e-06, -1.0959e-06]], grad_fn=<TBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias': tensor([ 4.2037e-07, -4.7137e-08,  4.4584e-07, -1.4330e-07,  3.0228e-07,\n",
       "          2.0770e-07,  2.5436e-07, -7.4948e-07,  3.6656e-07,  5.7574e-08,\n",
       "         -1.5831e-08,  5.6875e-07, -1.0209e-07,  1.8832e-07, -4.5605e-08,\n",
       "          3.6444e-07, -3.7791e-08, -7.0571e-07,  2.9921e-08, -8.3283e-09,\n",
       "         -1.0049e-07,  1.4387e-07, -1.0427e-07,  8.3805e-08, -3.9456e-08,\n",
       "          6.6757e-07, -3.1922e-07, -9.0327e-08,  5.8276e-07,  2.0243e-08,\n",
       "          3.7870e-07,  3.4839e-07, -3.9798e-07, -9.2098e-08, -3.3421e-07,\n",
       "         -1.4958e-07, -9.5710e-07,  4.7975e-07, -1.2180e-07, -3.0305e-07,\n",
       "         -2.4472e-07,  9.0518e-07, -1.0534e-07,  3.4532e-08,  1.6292e-07,\n",
       "         -4.1628e-08,  1.2676e-07,  4.1582e-07,  2.5152e-07,  3.2136e-07,\n",
       "         -2.2731e-07, -7.2496e-08,  1.6350e-07, -1.5827e-07, -8.3672e-08,\n",
       "          6.5632e-08,  2.2120e-07,  2.4203e-07,  4.4623e-07, -4.9706e-07,\n",
       "         -1.8380e-07,  3.7749e-07, -4.2596e-07, -2.9770e-07,  1.8607e-07,\n",
       "         -2.4259e-07, -1.8156e-07,  4.0017e-07, -5.4065e-07, -6.3467e-09,\n",
       "          1.9485e-07,  4.3211e-07,  6.2584e-08,  1.7838e-07, -3.6750e-07,\n",
       "          1.0573e-06,  1.0096e-07,  5.8936e-07,  5.0411e-07, -1.8625e-07,\n",
       "         -4.0063e-07, -2.2143e-07,  2.4841e-07, -5.2906e-08, -9.5528e-08,\n",
       "          7.3778e-07,  1.9714e-07, -3.6488e-07, -2.1571e-07,  6.0817e-07,\n",
       "         -4.9595e-07, -2.5354e-07, -7.6824e-08,  2.3101e-07,  1.4365e-07,\n",
       "          1.3173e-07,  2.0998e-07, -4.0439e-07,  5.0247e-08,  3.9052e-07,\n",
       "         -4.4985e-07, -5.8171e-07,  4.1222e-08, -6.3691e-08, -2.5990e-07,\n",
       "          2.5872e-07,  2.3381e-07,  4.4443e-07, -2.4204e-07,  2.1127e-07,\n",
       "          7.6782e-07,  3.2452e-07,  7.2697e-08,  3.1792e-07, -1.6765e-07,\n",
       "          7.8183e-08, -8.6060e-09,  4.7108e-07,  3.8823e-07, -1.5850e-07,\n",
       "          9.8792e-08,  1.7648e-07,  5.1215e-08, -4.7996e-09,  1.4806e-07,\n",
       "          9.0663e-08,  2.7297e-07,  1.0937e-07, -1.9895e-13, -2.8422e-14,\n",
       "          2.8422e-14, -5.6843e-14,  0.0000e+00,  5.6843e-14,  7.1054e-15,\n",
       "          1.4211e-14,  4.2633e-14,  1.7053e-13, -2.8422e-14,  8.5265e-14,\n",
       "          1.9895e-13,  4.9738e-14,  0.0000e+00,  5.6843e-14,  7.1054e-15,\n",
       "         -1.1369e-13,  2.8422e-14, -1.4211e-13, -8.5265e-14,  5.6843e-14,\n",
       "          1.1369e-13,  1.4211e-14,  2.8422e-14,  2.8422e-14,  6.0396e-14,\n",
       "         -8.5265e-14,  0.0000e+00,  1.0303e-13, -4.2633e-14,  5.6843e-14,\n",
       "          1.7764e-14,  2.8422e-14,  2.8422e-14,  1.4211e-13, -5.6843e-14,\n",
       "          2.8422e-14,  4.2633e-14,  7.1054e-14,  1.0658e-13, -8.5265e-14,\n",
       "          0.0000e+00, -4.2633e-14,  2.8422e-14,  1.1369e-13,  2.2737e-13,\n",
       "          1.1369e-13, -1.0192e-13, -7.1054e-14, -7.1054e-15, -1.1369e-13,\n",
       "          8.5265e-14,  9.9476e-14,  3.5527e-14,  6.7502e-14,  4.4409e-15,\n",
       "          2.8422e-14, -2.8422e-14, -1.1369e-13, -2.8422e-14,  2.8422e-14,\n",
       "          7.8160e-14, -4.2633e-14, -4.2633e-14,  2.5580e-13,  3.5527e-14,\n",
       "          5.6843e-14,  5.3291e-15, -1.4211e-14, -4.2633e-14,  0.0000e+00,\n",
       "          2.8422e-14, -1.4211e-14,  5.6843e-14,  2.8422e-14, -5.6843e-14,\n",
       "         -5.6843e-14,  4.2633e-14,  2.8422e-14, -2.8422e-14,  5.6843e-14,\n",
       "         -4.6185e-14,  2.8422e-14, -8.5265e-14, -5.6843e-14, -1.3500e-13,\n",
       "          0.0000e+00, -4.2633e-14, -1.3500e-13, -2.8422e-14, -1.4211e-14,\n",
       "          8.5265e-14,  2.1316e-14, -2.8422e-14,  2.8422e-14,  0.0000e+00,\n",
       "         -5.6843e-14, -9.9476e-14, -9.9476e-14,  2.8422e-14,  5.6843e-14,\n",
       "          1.0658e-13,  5.6843e-14,  8.5265e-14,  1.4211e-13,  5.6843e-14,\n",
       "          2.8422e-14, -1.1013e-13, -4.2633e-14,  0.0000e+00,  2.1316e-14,\n",
       "         -5.6843e-14, -9.9476e-14,  0.0000e+00,  9.9476e-14, -2.8422e-14,\n",
       "         -5.6843e-14,  4.2633e-14,  3.5527e-15,  4.2633e-14,  1.4211e-14,\n",
       "         -5.6843e-14,  1.1369e-13, -9.2371e-14,  4.2633e-14, -2.8422e-14,\n",
       "          0.0000e+00, -1.4605e-05,  4.7582e-07,  1.9878e-05,  3.2988e-08,\n",
       "          2.0403e-05,  1.8159e-05,  4.1777e-06,  8.7255e-06,  1.6186e-05,\n",
       "          1.0609e-05,  7.6255e-06, -9.4777e-06,  1.9387e-06, -2.7097e-06,\n",
       "         -9.3262e-07, -4.2166e-06, -1.4718e-05, -2.5792e-05, -1.2162e-08,\n",
       "          3.9920e-06,  1.6548e-05, -1.2710e-05, -8.9655e-06,  2.2487e-05,\n",
       "         -2.2507e-06, -1.2715e-05,  7.7912e-06, -2.1450e-05,  6.7514e-07,\n",
       "          9.7501e-06,  3.5640e-07,  5.5887e-08,  3.0420e-06,  2.6855e-06,\n",
       "          3.4722e-06, -2.6063e-07,  8.5152e-06,  4.0016e-06, -2.9511e-05,\n",
       "          1.1955e-05, -2.6789e-05, -2.6017e-07, -2.5899e-06, -6.8452e-06,\n",
       "          1.3001e-05,  6.8155e-06,  1.2033e-05, -1.5395e-05,  2.3961e-05,\n",
       "          9.6854e-06, -1.8983e-05,  1.3048e-06,  1.2172e-05,  6.5680e-06,\n",
       "         -2.4357e-06, -2.2116e-05, -5.9938e-06,  2.2077e-06,  1.2372e-06,\n",
       "         -1.4955e-05, -1.2148e-05, -2.4360e-05,  4.7163e-06, -2.0834e-06,\n",
       "          1.2092e-06,  4.0349e-06, -1.3668e-06,  1.4528e-07,  3.5581e-06,\n",
       "         -2.1112e-07,  7.9200e-06,  1.1055e-05,  6.4706e-06, -3.1327e-06,\n",
       "         -6.9431e-06,  4.8533e-06, -2.4946e-06,  1.7030e-05, -3.7014e-06,\n",
       "          3.2094e-07, -4.1439e-06,  1.1490e-05,  5.2657e-06, -3.4215e-05,\n",
       "          1.3847e-05, -1.6076e-05, -1.0296e-05, -1.2596e-05, -2.7434e-05,\n",
       "         -2.1680e-06, -2.0249e-05, -1.6026e-07,  8.5928e-06,  3.3238e-05,\n",
       "         -9.3242e-06, -7.8534e-06, -2.3235e-05,  3.0818e-06, -2.4251e-05,\n",
       "          7.6331e-07,  1.9846e-05, -1.4005e-05, -9.0939e-06, -5.9273e-06,\n",
       "          1.1149e-05,  6.8635e-06,  8.4195e-06,  7.0811e-06, -2.4077e-05,\n",
       "         -1.0531e-06, -1.0947e-05, -7.8076e-07,  1.1162e-05, -1.4165e-05,\n",
       "          5.6082e-06,  2.3658e-05,  1.4436e-05, -5.6100e-06, -9.1734e-06,\n",
       "         -1.9335e-06, -9.6866e-06,  5.2335e-06,  3.8544e-05, -1.1145e-05,\n",
       "          4.1661e-06,  2.1105e-05, -1.0847e-05, -1.0481e-06],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight': tensor([[-1.4504e-06,  1.4811e-06, -6.5192e-08,  ..., -4.5844e-06,\n",
       "          -2.6540e-06, -1.1032e-06],\n",
       "         [ 1.6490e-05, -1.6652e-05, -1.9772e-06,  ...,  2.7308e-05,\n",
       "           9.4279e-06,  3.2033e-06],\n",
       "         [ 1.7063e-06, -3.5892e-06, -5.0765e-06,  ..., -5.7324e-06,\n",
       "           6.7021e-07, -3.1435e-06],\n",
       "         ...,\n",
       "         [-5.4296e-08,  3.6643e-06,  6.3528e-06,  ...,  1.4253e-05,\n",
       "           1.5162e-06,  5.7461e-06],\n",
       "         [-5.6182e-06,  4.9129e-07, -6.0420e-06,  ..., -2.3159e-05,\n",
       "          -3.4811e-06, -6.8537e-06],\n",
       "         [ 1.4647e-05, -1.6889e-05, -5.5590e-06,  ...,  1.2730e-05,\n",
       "           8.2144e-06, -1.1240e-06]], grad_fn=<TBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias': tensor([ 4.0607e-06, -2.5922e-05,  5.8171e-07,  3.1424e-05,  3.5290e-06,\n",
       "         -3.6427e-05, -6.0266e-06,  2.0101e-05,  7.9943e-06, -9.9375e-07,\n",
       "         -9.8410e-06, -1.7356e-05, -6.1724e-06, -7.5550e-06,  1.2425e-05,\n",
       "         -9.0805e-06, -4.7166e-06,  4.5433e-05, -9.0648e-06, -1.6175e-05,\n",
       "          3.4105e-06, -1.5402e-05,  9.1280e-06, -1.3076e-05,  1.9292e-05,\n",
       "         -2.4410e-05, -8.2221e-06, -1.6098e-05, -3.3013e-05, -2.6034e-05,\n",
       "          1.0090e-05,  1.1728e-06,  2.6736e-06,  1.9976e-05,  1.3203e-05,\n",
       "          2.8602e-06, -1.2345e-05,  1.0749e-05, -2.6303e-05, -2.1914e-06,\n",
       "         -5.7946e-07,  8.3351e-06,  3.6156e-06,  9.1593e-06, -1.3730e-05,\n",
       "          2.6634e-05,  1.9560e-06, -2.0650e-05,  5.0683e-05, -1.0839e-05,\n",
       "          6.4588e-07, -8.6705e-06,  2.5876e-06,  4.0445e-06, -2.1674e-05,\n",
       "          9.7398e-07,  2.4661e-05,  7.6391e-06,  9.5625e-06, -1.7005e-05,\n",
       "         -2.9687e-06, -9.6813e-06,  1.2509e-05,  7.9454e-06, -2.5270e-05,\n",
       "          1.1684e-05,  3.9754e-05, -1.0981e-05,  3.2052e-06, -1.4643e-05,\n",
       "          1.7115e-06, -2.4346e-05, -2.8423e-05,  1.6775e-05, -4.1887e-05,\n",
       "          3.7690e-05,  2.0521e-05, -3.5690e-07,  9.5841e-06,  8.2563e-06,\n",
       "         -4.1676e-05, -2.8327e-06,  1.3036e-06, -4.0389e-05,  3.5147e-06,\n",
       "          1.6361e-05,  5.2503e-06,  1.4109e-05,  1.5293e-05, -1.6279e-05,\n",
       "         -7.0645e-07, -5.9533e-06,  5.1959e-06, -4.5174e-05,  3.1242e-06,\n",
       "          2.4017e-06, -5.5591e-06, -3.1801e-06, -4.7178e-05,  8.2120e-06,\n",
       "         -1.0041e-05, -3.0400e-05, -1.9567e-05,  1.2941e-05,  4.1857e-07,\n",
       "         -1.0206e-05, -7.7008e-07, -8.4120e-06, -5.9285e-06, -4.0583e-05,\n",
       "          8.5993e-06,  1.6522e-05, -3.9878e-06,  1.8865e-05, -4.7917e-05,\n",
       "          1.7011e-05,  3.1582e-05,  1.8197e-06, -2.1825e-07,  1.0515e-05,\n",
       "          4.5340e-07, -2.2398e-06, -5.1230e-05, -2.3223e-06,  1.5344e-05,\n",
       "         -5.7218e-06,  1.2605e-05, -1.8999e-05], grad_fn=<ViewBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.linear1.weight': tensor([[ 6.3348e-08,  7.7608e-08, -1.4273e-08,  ...,  8.5287e-08,\n",
       "           1.9054e-08,  8.7637e-10],\n",
       "         [-4.1693e-08, -1.4822e-07,  2.1625e-07,  ...,  9.4071e-08,\n",
       "          -9.0410e-08, -2.4773e-08],\n",
       "         [ 1.4921e-09,  6.7841e-08,  1.2876e-08,  ...,  1.7014e-07,\n",
       "           1.2007e-07,  7.2231e-08],\n",
       "         ...,\n",
       "         [-2.2779e-09,  5.1860e-08, -6.6571e-08,  ...,  4.9187e-08,\n",
       "           8.1565e-08,  4.2948e-08],\n",
       "         [ 8.6503e-09,  3.0656e-08, -5.2597e-08,  ..., -5.4080e-08,\n",
       "           1.9514e-08,  2.7652e-08],\n",
       "         [-5.0888e-09, -4.5442e-09,  1.9670e-08,  ...,  3.2915e-08,\n",
       "          -2.7046e-09, -1.4545e-09]], grad_fn=<TBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.linear1.bias': tensor([ 2.8012e-08, -7.1496e-08,  9.9708e-08, -4.9783e-08, -2.0853e-09,\n",
       "         -1.4972e-08,  1.9587e-08, -4.4335e-09,  1.1403e-07,  7.9303e-08,\n",
       "          2.4628e-08,  9.3223e-08,  5.5458e-08, -9.7190e-09, -6.2111e-08,\n",
       "         -1.0195e-07, -1.4617e-08,  6.7868e-08,  1.5606e-07, -8.4113e-08,\n",
       "          4.6349e-08,  7.8788e-09, -9.4740e-08,  1.8273e-07,  1.8755e-08,\n",
       "         -4.7710e-08,  2.5772e-08, -1.1149e-07,  2.1692e-08,  2.1156e-08,\n",
       "         -3.8063e-08, -3.7814e-09, -1.4091e-07,  2.7360e-08,  1.1805e-08,\n",
       "          2.1268e-08,  7.4330e-08, -8.5991e-08,  1.4754e-08, -1.5091e-07,\n",
       "          1.1687e-07, -1.5947e-07, -1.3157e-07, -9.6063e-08,  4.9762e-09,\n",
       "         -2.5708e-08,  2.9917e-08, -5.0348e-08,  3.3415e-08, -3.1989e-10,\n",
       "         -6.7855e-08, -1.8097e-08,  1.1744e-07,  4.3125e-08,  5.6487e-08,\n",
       "         -1.1217e-08,  1.0170e-08, -7.5528e-08,  9.7168e-08, -1.0614e-08,\n",
       "         -2.1594e-08,  1.6237e-07, -1.6660e-07, -7.4650e-08,  5.9774e-08,\n",
       "         -4.6865e-08,  1.4932e-07,  5.0869e-09,  3.1575e-09, -1.0064e-07,\n",
       "         -8.0224e-08, -5.9446e-08,  1.1088e-07,  2.9629e-08,  1.1981e-07,\n",
       "          1.2396e-07,  5.4200e-08, -3.7120e-08, -7.0582e-08,  2.0114e-08,\n",
       "          1.4285e-07,  2.5714e-08, -8.8123e-09,  5.1497e-08,  9.4073e-08,\n",
       "         -6.7133e-08,  4.1948e-08, -6.6977e-10,  1.4991e-07, -7.8879e-08,\n",
       "          7.1588e-08, -4.1594e-08,  1.1390e-07,  2.2537e-07, -3.3134e-08,\n",
       "         -1.3127e-08, -2.2494e-07, -7.1319e-09,  5.5028e-08, -1.0079e-07,\n",
       "         -6.7797e-08, -3.5904e-08, -7.1858e-08, -4.5426e-08,  1.6227e-07,\n",
       "          8.8262e-08, -9.8457e-08,  5.7277e-08,  7.6758e-08, -1.9274e-08,\n",
       "         -9.6518e-08, -1.6967e-08,  2.7424e-08,  9.8578e-09, -1.0848e-07,\n",
       "          8.3251e-08, -3.5729e-09,  1.3913e-07,  5.0801e-09,  1.1902e-07,\n",
       "          8.3485e-08,  1.3876e-07, -4.4831e-08, -8.5393e-09,  4.6870e-08,\n",
       "          6.8363e-08,  1.9041e-08, -3.2829e-09], grad_fn=<ViewBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.linear2.weight': tensor([[ 8.9914e-09, -2.3710e-08, -1.2726e-08,  ...,  8.8130e-11,\n",
       "           2.0076e-08,  9.2880e-09],\n",
       "         [-2.6367e-07,  2.4323e-07,  4.2806e-08,  ...,  9.7558e-09,\n",
       "          -2.4874e-08, -1.4013e-07],\n",
       "         [-7.8451e-08,  6.6948e-08,  1.2216e-08,  ...,  2.5352e-09,\n",
       "          -3.3998e-09, -4.0970e-08],\n",
       "         ...,\n",
       "         [ 3.5617e-09, -4.4664e-09, -1.0308e-08,  ..., -2.6579e-09,\n",
       "           1.8448e-09, -4.1157e-09],\n",
       "         [ 1.4112e-07, -1.0616e-07,  8.5213e-08,  ...,  5.6558e-09,\n",
       "          -1.0183e-08,  3.4673e-08],\n",
       "         [-7.2526e-08,  5.7556e-08, -3.3099e-08,  ...,  6.8819e-09,\n",
       "           3.8041e-09, -5.2197e-09]], grad_fn=<TBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.linear2.bias': tensor([ 2.8673e-06, -2.7583e-05, -7.7040e-06,  4.5521e-05, -6.3600e-06,\n",
       "         -1.1357e-05, -2.8465e-06,  8.7297e-06,  1.4933e-05, -8.4813e-07,\n",
       "         -1.0590e-05,  4.6710e-05,  1.2019e-05, -6.7107e-06,  1.5575e-05,\n",
       "         -8.6269e-06, -1.1951e-05,  5.0556e-05, -1.0554e-05, -9.9856e-06,\n",
       "          1.8984e-06,  5.5124e-06,  1.2333e-05, -1.8284e-05,  1.8103e-05,\n",
       "          5.4859e-06,  1.6301e-05, -1.2808e-05,  1.0675e-05, -9.4638e-06,\n",
       "         -3.3246e-06, -1.1574e-05, -1.3005e-05,  1.3617e-05,  1.7479e-05,\n",
       "          4.7079e-06, -1.7071e-05, -1.6662e-05, -1.6142e-05, -1.5834e-05,\n",
       "         -3.0724e-06, -9.1627e-06, -5.0166e-06,  1.3190e-05, -9.8260e-06,\n",
       "         -5.2272e-06,  9.6185e-07, -2.0779e-05, -1.0043e-05, -1.2393e-05,\n",
       "          5.5617e-06,  1.0756e-05,  1.8500e-06,  3.5279e-06, -1.1387e-05,\n",
       "          5.5001e-06,  1.8203e-05,  7.7129e-06, -1.3785e-05,  3.8093e-06,\n",
       "         -3.7213e-06, -9.1915e-06,  6.1220e-06,  1.1936e-05, -1.2597e-05,\n",
       "          2.4227e-05,  4.9532e-05, -2.1795e-05,  3.3454e-06, -1.5239e-05,\n",
       "          1.4092e-07, -1.5028e-05, -3.2600e-05,  1.6960e-05,  1.0844e-05,\n",
       "          3.0640e-05,  1.1851e-05, -4.2009e-07, -1.5858e-05,  8.6485e-06,\n",
       "         -3.5140e-05, -1.8486e-06,  6.4494e-06, -1.7738e-06, -6.9127e-07,\n",
       "          1.6763e-05, -4.0360e-06,  3.1857e-06,  2.3199e-06, -3.2767e-05,\n",
       "         -6.8205e-06,  2.8723e-05,  6.0309e-06, -2.4745e-05,  1.2540e-05,\n",
       "         -6.0746e-07,  2.2420e-06, -4.6570e-06, -4.3681e-05,  8.4623e-06,\n",
       "         -4.2661e-06, -4.6815e-05, -9.5793e-06, -4.7066e-05, -1.8749e-06,\n",
       "         -1.4035e-05, -4.0263e-06, -1.4277e-06, -1.1817e-05,  1.4982e-05,\n",
       "          1.7092e-05,  1.7988e-05, -6.4468e-06,  4.4442e-06, -3.8062e-05,\n",
       "          1.7576e-05,  2.3390e-05,  1.0959e-05,  6.6127e-06, -1.0398e-06,\n",
       "         -3.3165e-06,  2.6004e-05, -4.4709e-05, -1.8846e-06,  1.5875e-05,\n",
       "          9.6623e-07,  1.2648e-05, -4.2233e-06], grad_fn=<ViewBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.norm1.weight': tensor([-3.9861e-06,  1.0048e-06, -1.6103e-06, -1.1617e-06,  1.3038e-05,\n",
       "         -2.3298e-06,  3.9895e-06,  5.7599e-06, -5.7180e-06,  5.0701e-07,\n",
       "         -4.2429e-07,  4.5982e-05,  4.9843e-06,  2.8028e-05, -4.2446e-06,\n",
       "          5.3961e-06, -8.7879e-06,  9.8473e-06,  5.6799e-06, -2.9406e-05,\n",
       "          4.0915e-07, -6.6047e-06,  2.0899e-06,  1.3002e-07,  1.4917e-07,\n",
       "         -6.9098e-06, -1.3983e-05, -1.2809e-06, -2.8378e-06,  1.3099e-06,\n",
       "         -5.8298e-06,  3.8166e-07, -1.8041e-05,  1.8370e-05,  3.1291e-06,\n",
       "         -1.2079e-06,  6.3491e-06, -1.4132e-05, -1.0950e-05, -6.3933e-06,\n",
       "          5.1644e-06, -9.9887e-06, -4.2002e-06,  4.7155e-06,  1.0041e-05,\n",
       "          5.2448e-07, -9.0932e-06,  1.7982e-06,  1.3497e-07, -6.7322e-06,\n",
       "          1.4483e-06, -1.6064e-05, -7.9686e-06, -1.5470e-06, -1.3010e-07,\n",
       "          4.0321e-06, -5.7286e-07,  1.5601e-05,  2.9585e-06,  4.0640e-06,\n",
       "          5.2724e-06, -2.9729e-06, -2.6264e-05, -1.3782e-06, -4.6542e-07,\n",
       "          7.2977e-06, -6.8600e-06,  6.5734e-06,  4.4494e-07,  2.0800e-06,\n",
       "         -1.2505e-05,  3.2826e-06, -7.5648e-06,  6.1208e-06,  9.8857e-06,\n",
       "          7.5453e-07, -3.2440e-05,  1.2382e-06,  7.5460e-06,  1.1502e-06,\n",
       "          6.7758e-06,  7.9335e-07,  3.7453e-06, -1.1794e-06,  2.7595e-06,\n",
       "          8.0775e-06, -3.5676e-06, -2.6256e-06, -1.3050e-05,  1.5426e-06,\n",
       "          3.4982e-06,  3.6373e-06,  3.8038e-06, -6.0037e-06, -3.1282e-06,\n",
       "          7.2638e-06, -4.2351e-06, -9.9505e-07,  1.3659e-07,  3.0292e-05,\n",
       "          7.4263e-07,  4.2227e-06, -1.4082e-06,  3.4257e-06, -2.6105e-06,\n",
       "          2.1312e-05, -9.9399e-07,  2.9946e-06, -1.7855e-06,  1.9598e-05,\n",
       "         -4.7382e-06, -2.1335e-06, -9.8245e-06,  2.6394e-07,  5.4658e-07,\n",
       "         -1.4457e-06, -3.2944e-06, -9.2083e-06, -3.9408e-06, -1.9606e-05,\n",
       "         -9.4934e-06, -2.5222e-06, -1.4405e-05,  6.7216e-06,  1.4655e-05,\n",
       "          2.6203e-06,  2.3877e-06,  9.0926e-06],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.norm1.bias': tensor([ 5.2756e-06, -2.1283e-06,  1.2052e-06,  1.1486e-06,  1.0962e-05,\n",
       "         -1.6918e-06, -8.8131e-06, -8.4695e-06, -1.4289e-05, -9.8619e-06,\n",
       "         -1.0486e-06,  2.6326e-05,  4.1104e-06,  2.1661e-05,  6.7047e-06,\n",
       "          1.5495e-05,  1.3769e-05,  1.1884e-05, -8.0563e-06,  1.3479e-05,\n",
       "         -1.1689e-05, -3.2589e-06, -3.4300e-06,  2.6264e-06,  2.1954e-06,\n",
       "          1.8248e-06, -7.9040e-06,  3.0640e-06,  1.5804e-05, -3.1195e-06,\n",
       "         -8.2394e-06,  1.0395e-05, -9.5629e-06,  1.6889e-05,  9.8120e-07,\n",
       "          2.4904e-06,  7.3462e-06, -1.0254e-05,  1.7643e-05, -6.1742e-06,\n",
       "         -4.3931e-06,  1.1000e-05, -2.7626e-06, -1.3468e-05,  1.5317e-05,\n",
       "          5.6020e-06, -1.0192e-05,  3.1115e-06,  1.7610e-06, -5.5355e-06,\n",
       "          7.4595e-06,  1.4296e-05,  1.8196e-05, -2.6757e-06,  8.9285e-07,\n",
       "          9.3482e-06, -2.3827e-06,  1.4860e-05, -3.5791e-06,  4.6186e-06,\n",
       "         -3.6587e-06, -5.6080e-06,  1.3166e-05, -3.0721e-06,  7.0453e-06,\n",
       "         -4.8938e-06,  5.9546e-06,  1.0867e-05, -7.7252e-06,  1.2961e-06,\n",
       "          1.5102e-05, -6.4182e-06, -1.7290e-05,  5.3101e-06, -4.3147e-06,\n",
       "          2.9597e-06,  1.4104e-05, -7.3772e-06, -5.3153e-06, -4.2499e-06,\n",
       "         -2.5824e-05, -1.6469e-06, -2.5631e-06,  6.7894e-07, -1.6335e-05,\n",
       "          9.8704e-06, -4.2115e-06, -8.2511e-06,  1.1803e-05,  5.7016e-06,\n",
       "         -1.0423e-06, -5.3108e-06,  5.8653e-06, -9.9773e-06,  1.0367e-05,\n",
       "          7.1649e-06,  2.6055e-05, -1.5634e-06, -1.0648e-05, -1.6715e-05,\n",
       "          3.7450e-07,  2.8744e-06, -3.6372e-06,  1.3431e-05, -8.8113e-07,\n",
       "         -1.6593e-05, -2.2083e-06,  7.0337e-06,  2.7548e-06,  1.2135e-05,\n",
       "         -9.4426e-06,  1.4310e-06,  7.5048e-06,  1.9526e-06,  5.4124e-07,\n",
       "          8.8495e-06,  4.8649e-06, -1.0853e-05,  6.9979e-06,  2.3042e-05,\n",
       "         -1.2044e-05,  1.2305e-05,  1.4892e-05,  5.3578e-06,  7.3654e-06,\n",
       "         -4.3408e-06,  7.8895e-06,  9.1936e-06],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.norm2.weight': tensor([-1.5541e-10, -3.9502e-10, -4.8062e-10,  5.2079e-10,  1.0619e-10,\n",
       "         -3.2383e-10, -4.9442e-10, -1.1362e-09,  4.6488e-10,  1.4910e-09,\n",
       "         -3.0842e-10,  3.1590e-11,  1.6184e-10, -3.4610e-10,  8.3710e-11,\n",
       "         -2.9903e-10, -8.4546e-11,  1.1460e-09,  6.0379e-11,  7.6496e-10,\n",
       "          1.6123e-10,  1.0109e-10,  2.7070e-10, -8.2724e-10,  5.6056e-10,\n",
       "         -4.0428e-10,  7.4581e-10, -1.0265e-09,  2.1496e-10,  1.2153e-09,\n",
       "         -6.0096e-11, -4.3233e-10, -1.7963e-10, -1.5441e-09, -2.8393e-10,\n",
       "          3.6610e-10,  3.2787e-10,  5.3586e-10, -1.4697e-09,  2.8653e-10,\n",
       "         -3.9429e-11,  1.5219e-10,  2.4721e-10, -1.2446e-10, -6.1093e-10,\n",
       "          1.1888e-10,  6.4618e-10,  1.0422e-09, -5.3912e-11, -1.2683e-09,\n",
       "         -1.7421e-10, -2.4347e-10, -3.1569e-10,  1.2775e-10, -8.0872e-11,\n",
       "          7.1082e-10, -1.0360e-10, -5.4605e-11,  1.4191e-10, -2.5289e-10,\n",
       "         -3.0350e-10, -2.7450e-10, -2.8217e-10,  2.3807e-10,  5.1805e-10,\n",
       "         -5.8222e-10,  6.8582e-10, -7.1033e-11,  1.4119e-10,  1.3654e-10,\n",
       "          1.2163e-09, -7.9433e-10, -4.4028e-10,  1.5694e-10, -9.0184e-10,\n",
       "          2.5849e-10,  4.0515e-11,  2.2912e-11, -4.9540e-11, -3.9827e-10,\n",
       "          8.8293e-10, -1.5784e-09, -4.0622e-10,  3.7095e-10, -4.2609e-10,\n",
       "         -1.0956e-09, -1.4985e-10, -1.5349e-10,  5.4365e-10,  1.8399e-10,\n",
       "         -8.7741e-10, -4.2849e-10, -4.3507e-10,  8.6010e-12,  1.3011e-09,\n",
       "         -2.2354e-10,  5.2409e-10,  1.8501e-10, -9.1512e-11, -4.6706e-10,\n",
       "         -1.0314e-09,  6.0254e-11,  1.0681e-09,  3.2672e-11, -8.9052e-10,\n",
       "         -1.6316e-10, -4.8338e-11,  1.3494e-10,  3.2683e-10, -1.9000e-09,\n",
       "         -1.5980e-12,  8.2440e-10, -6.1499e-11, -6.2013e-11,  4.8681e-12,\n",
       "          6.4864e-10,  2.5612e-10,  4.2668e-10,  5.9138e-12,  1.5583e-09,\n",
       "          5.0883e-11, -1.8776e-10, -1.2609e-10, -1.8608e-10, -7.6882e-10,\n",
       "         -2.5505e-09,  4.7522e-10, -4.5799e-10],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'policy_logstd_decoder.transformer_encoder.layers.0.norm2.bias': tensor([ 4.1383e-10, -9.4485e-10, -7.7742e-10, -5.9303e-10,  2.5520e-10,\n",
       "          1.3867e-09, -5.6236e-10, -4.4219e-10, -5.5641e-10, -7.7093e-10,\n",
       "         -1.7600e-10,  1.2647e-09,  3.5686e-10,  3.4394e-10, -3.3307e-10,\n",
       "          5.4485e-10,  1.1380e-09,  4.5468e-10,  2.3440e-11,  6.5269e-10,\n",
       "         -5.4852e-11, -3.7956e-10,  2.3636e-10, -7.6507e-10,  2.3507e-10,\n",
       "         -3.2289e-10,  7.4510e-10, -1.0221e-09,  1.0865e-10, -1.0140e-09,\n",
       "          1.4321e-10,  5.0917e-10,  8.0397e-10,  7.9740e-10,  1.6269e-11,\n",
       "         -3.1849e-10, -2.8133e-10, -9.8138e-11,  7.4896e-10, -3.4606e-10,\n",
       "          3.9315e-10,  2.7746e-10, -5.3093e-10,  3.3650e-10, -4.2569e-10,\n",
       "         -6.0823e-10, -5.9322e-10, -8.6255e-10,  2.0432e-10,  5.8806e-10,\n",
       "          1.2071e-09,  2.6872e-10, -7.8727e-10,  7.0702e-10,  1.1573e-10,\n",
       "         -3.5500e-10,  3.4464e-10, -1.0529e-10, -1.2160e-09,  9.0870e-10,\n",
       "          1.1492e-09,  5.5924e-10,  3.0659e-10, -7.5533e-10, -4.6961e-10,\n",
       "          9.4975e-10,  5.4063e-10,  2.0528e-10, -1.9945e-10, -3.4147e-10,\n",
       "         -1.0025e-09, -9.8842e-10, -3.1477e-10, -2.5982e-10, -1.0059e-09,\n",
       "         -7.9931e-10, -8.5536e-11,  1.0908e-10,  4.7809e-10, -4.7067e-10,\n",
       "          1.0995e-09, -1.5162e-09,  2.2528e-10, -4.4513e-10,  5.3191e-10,\n",
       "         -3.0192e-10,  3.9464e-10,  6.0437e-11, -6.4126e-10, -1.0909e-09,\n",
       "          6.2617e-10, -6.3619e-11, -4.0064e-10,  2.4154e-10,  1.2201e-09,\n",
       "          2.8638e-10, -4.3169e-10,  2.5765e-10, -6.7468e-11,  1.2561e-10,\n",
       "         -3.1772e-10, -7.3534e-13,  1.0540e-09,  3.5610e-10, -3.0453e-10,\n",
       "         -6.3598e-10,  5.0389e-11, -1.1978e-10, -5.5131e-10, -2.9449e-10,\n",
       "          5.7623e-10,  1.0291e-09,  5.1071e-10,  2.4592e-10, -1.3281e-11,\n",
       "         -3.9420e-10,  1.2736e-09,  9.5360e-10, -1.3979e-10, -1.4437e-09,\n",
       "         -3.2849e-10,  7.0693e-10,  6.4664e-11,  4.5234e-10,  8.1201e-10,\n",
       "         -1.2843e-09,  4.7900e-10, -5.7062e-10],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " 'policy_prob_decoder.rnn.weight_ih_l0': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'policy_prob_decoder.rnn.weight_hh_l0': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'policy_prob_decoder.rnn.weight_ih_l0_reverse': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'policy_prob_decoder.rnn.weight_hh_l0_reverse': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.linear1.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.linear1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.linear2.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.linear2.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.norm1.weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.norm1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.norm2.weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'policy_prob_decoder.transformer_encoder.layers.0.norm2.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchopt\n",
    "optimizer = torchopt.adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = optimizer.init(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, vmap, grad, grad_and_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, but got <class 'generator'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn([\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m action_space \u001b[39m=\u001b[39m Discrete(\u001b[39m4\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m output \u001b[39m=\u001b[39m functional_call(agent, params, (action_space, data, \u001b[39mNone\u001b[39;49;00m))\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/functional_call.py:138\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    134\u001b[0m     parameters_and_buffers \u001b[39m=\u001b[39m {\n\u001b[1;32m    135\u001b[0m         k: v \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m parameter_and_buffer_dicts \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m d\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    136\u001b[0m     }\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mstateless\u001b[39m.\u001b[39m_functional_call(\n\u001b[1;32m    144\u001b[0m     module,\n\u001b[1;32m    145\u001b[0m     parameters_and_buffers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     strict\u001b[39m=\u001b[39mstrict,\n\u001b[1;32m    150\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, but got <class 'generator'>"
     ]
    }
   ],
   "source": [
    "data = torch.randn([1, 4]).to(device)\n",
    "action_space = Discrete(4)\n",
    "output = functional_call(agent, params, (action_space, data, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0], device='cuda:1'), tensor([-1.3850], device='cuda:1', grad_fn=<SqueezeBackward1>), tensor([1.3863], device='cuda:1', grad_fn=<NegBackward0>), tensor([[0.0026]], device='cuda:1', grad_fn=<DivBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import make_functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m func, params \u001b[39m=\u001b[39m make_functional(agent)\n\u001b[1;32m      2\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn([\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m action_space \u001b[39m=\u001b[39m Discrete(\u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/deprecated.py:98\u001b[0m, in \u001b[0;36mmake_functional\u001b[0;34m(model, disable_autograd_tracking)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_functional\u001b[39m(model: nn\u001b[39m.\u001b[39mModule, disable_autograd_tracking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     97\u001b[0m     warn_deprecated(\u001b[39m'\u001b[39m\u001b[39mmake_functional\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtorch.func.functional_call\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m _nn_impl\u001b[39m.\u001b[39;49mmake_functional(model, disable_autograd_tracking)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/make_functional.py:413\u001b[0m, in \u001b[0;36mmake_functional\u001b[0;34m(model, disable_autograd_tracking)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buffers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    409\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmake_functional(model): `model` has buffers. Please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmake_functional_with_buffers(model) instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m     )\n\u001b[0;32m--> 413\u001b[0m \u001b[39mreturn\u001b[39;00m FunctionalModule\u001b[39m.\u001b[39;49m_create_from(\n\u001b[1;32m    414\u001b[0m     model, disable_autograd_tracking\u001b[39m=\u001b[39;49mdisable_autograd_tracking\n\u001b[1;32m    415\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/make_functional.py:331\u001b[0m, in \u001b[0;36mFunctionalModule._create_from\u001b[0;34m(model, disable_autograd_tracking)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    327\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_from\u001b[39m(\n\u001b[1;32m    328\u001b[0m     model: nn\u001b[39m.\u001b[39mModule, disable_autograd_tracking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39m\"\u001b[39m\u001b[39mFunctionalModule\u001b[39m\u001b[39m\"\u001b[39m, Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]:\n\u001b[1;32m    330\u001b[0m     \u001b[39m# TODO: We don't need to copy the model to create a stateless copy\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     model_copy \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39;49mdeepcopy(model)\n\u001b[1;32m    332\u001b[0m     params, param_names, names_map \u001b[39m=\u001b[39m extract_weights(model_copy)\n\u001b[1;32m    333\u001b[0m     \u001b[39mif\u001b[39;00m disable_autograd_tracking:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[39m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_tensor.py:86\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__deepcopy__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, memo)\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_leaf:\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly Tensors created explicitly by the user \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(graph leaves) support the deepcopy protocol at the moment\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39min\u001b[39;00m memo:\n\u001b[1;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m memo[\u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m)]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment"
     ]
    }
   ],
   "source": [
    "func, params = make_functional(agent)\n",
    "data = torch.randn([1, 4]).to(device)\n",
    "action_space = Discrete(4)\n",
    "output = func(params, action_space, data, None)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, dist, entropy, value = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:1')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397824"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor.obs_encoder.rnn.weight_ih_l0 torch.Size([192, 1]) None\n",
      "actor.obs_encoder.rnn.weight_hh_l0 torch.Size([192, 64]) None\n",
      "actor.obs_encoder.rnn.weight_ih_l0_reverse torch.Size([192, 1]) None\n",
      "actor.obs_encoder.rnn.weight_hh_l0_reverse torch.Size([192, 64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.self_attn.in_proj_bias torch.Size([192]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.self_attn.out_proj.bias torch.Size([64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.linear1.weight torch.Size([64, 64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.linear1.bias torch.Size([64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.linear2.weight torch.Size([64, 64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.linear2.bias torch.Size([64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.norm1.weight torch.Size([64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.norm1.bias torch.Size([64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.norm2.weight torch.Size([64]) None\n",
      "actor.obs_encoder.transformer_encoder.layers.0.norm2.bias torch.Size([64]) None\n",
      "actor.policy_mean_decoder.rnn.weight_ih_l0 torch.Size([192, 64]) None\n",
      "actor.policy_mean_decoder.rnn.weight_hh_l0 torch.Size([192, 64]) None\n",
      "actor.policy_mean_decoder.rnn.weight_ih_l0_reverse torch.Size([192, 64]) None\n",
      "actor.policy_mean_decoder.rnn.weight_hh_l0_reverse torch.Size([192, 64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias torch.Size([192]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias torch.Size([64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.linear1.weight torch.Size([64, 64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.linear1.bias torch.Size([64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.linear2.weight torch.Size([64, 64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.linear2.bias torch.Size([64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.norm1.weight torch.Size([64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.norm1.bias torch.Size([64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.norm2.weight torch.Size([64]) None\n",
      "actor.policy_mean_decoder.transformer_encoder.layers.0.norm2.bias torch.Size([64]) None\n",
      "actor.policy_logstd_decoder.rnn.weight_ih_l0 torch.Size([192, 64]) None\n",
      "actor.policy_logstd_decoder.rnn.weight_hh_l0 torch.Size([192, 64]) None\n",
      "actor.policy_logstd_decoder.rnn.weight_ih_l0_reverse torch.Size([192, 64]) None\n",
      "actor.policy_logstd_decoder.rnn.weight_hh_l0_reverse torch.Size([192, 64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias torch.Size([192]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias torch.Size([64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.linear1.weight torch.Size([64, 64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.linear1.bias torch.Size([64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.linear2.weight torch.Size([64, 64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.linear2.bias torch.Size([64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.norm1.weight torch.Size([64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.norm1.bias torch.Size([64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.norm2.weight torch.Size([64]) None\n",
      "actor.policy_logstd_decoder.transformer_encoder.layers.0.norm2.bias torch.Size([64]) None\n",
      "actor.policy_prob_decoder.rnn.weight_ih_l0 torch.Size([192, 64]) None\n",
      "actor.policy_prob_decoder.rnn.weight_hh_l0 torch.Size([192, 64]) None\n",
      "actor.policy_prob_decoder.rnn.weight_ih_l0_reverse torch.Size([192, 64]) None\n",
      "actor.policy_prob_decoder.rnn.weight_hh_l0_reverse torch.Size([192, 64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias torch.Size([192]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias torch.Size([64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.linear1.weight torch.Size([64, 64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.linear1.bias torch.Size([64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.linear2.weight torch.Size([64, 64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.linear2.bias torch.Size([64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.norm1.weight torch.Size([64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.norm1.bias torch.Size([64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.norm2.weight torch.Size([64]) None\n",
      "actor.policy_prob_decoder.transformer_encoder.layers.0.norm2.bias torch.Size([64]) None\n",
      "critic.obs_encoder.rnn.weight_ih_l0 torch.Size([192, 1]) None\n",
      "critic.obs_encoder.rnn.weight_hh_l0 torch.Size([192, 64]) None\n",
      "critic.obs_encoder.rnn.weight_ih_l0_reverse torch.Size([192, 1]) None\n",
      "critic.obs_encoder.rnn.weight_hh_l0_reverse torch.Size([192, 64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.self_attn.in_proj_bias torch.Size([192]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.self_attn.out_proj.bias torch.Size([64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.linear1.weight torch.Size([64, 64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.linear1.bias torch.Size([64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.linear2.weight torch.Size([64, 64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.linear2.bias torch.Size([64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.norm1.weight torch.Size([64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.norm1.bias torch.Size([64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.norm2.weight torch.Size([64]) None\n",
      "critic.obs_encoder.transformer_encoder.layers.0.norm2.bias torch.Size([64]) None\n",
      "critic.value_decoder.rnn.weight_ih_l0 torch.Size([192, 64]) None\n",
      "critic.value_decoder.rnn.weight_hh_l0 torch.Size([192, 64]) None\n",
      "critic.value_decoder.rnn.weight_ih_l0_reverse torch.Size([192, 64]) None\n",
      "critic.value_decoder.rnn.weight_hh_l0_reverse torch.Size([192, 64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.self_attn.in_proj_bias torch.Size([192]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.self_attn.out_proj.bias torch.Size([64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.linear1.weight torch.Size([64, 64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.linear1.bias torch.Size([64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.linear2.weight torch.Size([64, 64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.linear2.bias torch.Size([64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.norm1.weight torch.Size([64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.norm1.bias torch.Size([64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.norm2.weight torch.Size([64]) None\n",
      "critic.value_decoder.transformer_encoder.layers.0.norm2.bias torch.Size([64]) None\n"
     ]
    }
   ],
   "source": [
    "for name, param in agent.named_parameters():\n",
    "    print(name, param.shape, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CartPole-v1', 'LunarLander-v2', 'HalfCheetah-v4', 'Ant-v4']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_ids = list(cfg.experiment.env_ids)\n",
    "env_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_different_envs \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m test_different_envs \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m j, env_id \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(env_ids):\n\u001b[1;32m      4\u001b[0m     train_envs \u001b[39m=\u001b[39m make_batched_env(j, env_id, cfg, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     test_envs \u001b[39m=\u001b[39m make_batched_env(j, env_id, cfg, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env_ids' is not defined"
     ]
    }
   ],
   "source": [
    "train_different_envs = []\n",
    "test_different_envs = []\n",
    "for j, env_id in enumerate(env_ids):\n",
    "    train_envs = make_batched_env(j, env_id, cfg, mode='train')\n",
    "    test_envs = make_batched_env(j, env_id, cfg, mode='test')\n",
    "    train_different_envs.append(train_envs)\n",
    "    test_different_envs.append(test_envs)\n",
    "    print(f\"{j+1}/{len(env_ids)}environment {env_id} is loaded...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cfg = cfg.experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set PPO storage for each environment\n",
    "envs_storages = dict()\n",
    "for i, envs in enumerate(train_different_envs):\n",
    "    obs = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs) \\\n",
    "        + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs) \\\n",
    "        + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "    rewards = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "    dones = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "    values = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "    storage = dict()\n",
    "    storage[\"obs\"] = obs\n",
    "    storage['actions'] = actions\n",
    "    storage['logprobs'] = logprobs\n",
    "    storage['rewards'] = rewards\n",
    "    storage['dones'] = dones\n",
    "    storage['values'] = values\n",
    "    storage['avg_returns'] = deque(maxlen=20)\n",
    "    envs_storages[i] = storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(exp_cfg.num_envs * exp_cfg.num_rollout_steps)\n",
    "minibatch_size = int(batch_size // cfg.ppo.num_minibatches)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obsv = []\n",
    "next_donev = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for envs in train_different_envs:\n",
    "    next_obs = envs.reset()\n",
    "    next_obs = torch.Tensor(next_obs).to(device)\n",
    "    next_done = torch.zeros(exp_cfg.num_envs).to(device)\n",
    "    next_obsv.append(next_obs)\n",
    "    next_donev.append(next_done)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_update_idx = 1\n",
    "total_num_updates = exp_cfg.total_timesteps // batch_size\n",
    "total_num_updates = total_num_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39062\n"
     ]
    }
   ],
   "source": [
    "print(total_num_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_updates = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(update_idx, envs_storages, next_obsv, next_donev, envs_returns, envs_lengths):\n",
    "    for i, envs in enumerate(train_different_envs):\n",
    "        for step in range(0, cfg.experiment.num_rollout_steps):\n",
    "            envs_storages[i][\"obs\"][step] = next_obsv[i]\n",
    "            envs_storages[i][\"dones\"][step] = next_donev[i]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(envs.single_action_space, next_obsv[i])\n",
    "\n",
    "            envs_storages[i][\"values\"][step] = value.flatten()\n",
    "            envs_storages[i][\"actions\"][step] = action\n",
    "            envs_storages[i][\"logprobs\"][step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            next_obs, reward, done, infos = envs.step(action.cpu().numpy())\n",
    "            envs_storages[i][\"rewards\"][step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "            next_obsv[i] = next_obs\n",
    "            next_donev[i] = next_done\n",
    "                \n",
    "            for k, d in enumerate(done):\n",
    "                if d:\n",
    "                    # print(f\"[{update_idx}/{total_num_updates}] Train. env_name:{env_ids[i]}, global_step={global_step}, episodic_return={infos['r'][k]}\")\n",
    "                    envs_returns[env_ids[i]+'/train'] = infos[\"r\"][k]\n",
    "                    envs_lengths[env_ids[i]+'/train'] = infos[\"l\"][k]\n",
    "        print(f\"[{update_idx}/{total_num_updates}] Train. env_name:{env_ids[i]}, global_step={global_step}, episodic_return={infos['r'][k]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "    b_inds = np.arange(batch_size)\n",
    "    clipfracs = []\n",
    "    total_epochs_envs_loss = 0.0\n",
    "    total_epochs_value_loss = 0.0\n",
    "    total_epochs_policy_loss = 0.0\n",
    "    total_epochs_entropy_loss = 0.0\n",
    "    \n",
    "    for epoch in range(cfg.ppo.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            total_envs_loss = 0.0\n",
    "            total_value_loss = 0.0\n",
    "            total_policy_loss = 0.0\n",
    "            total_entropy_loss = 0.0\n",
    "            \n",
    "            for i, envs in enumerate(train_different_envs):\n",
    "                end = start + minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "                # pg_loss, entropy_loss, v_loss = calculate_ppo_loss(params, i, envs, mb_inds, clipfracs)\n",
    "                grad_value_func = grad_and_value(calculate_ppo_loss, has_aux=True)\n",
    "                grad_weights, losses = grad_value_func(params, i, envs, mb_inds, clipfracs)\n",
    "                print(grad_weights)\n",
    "                print(losses)\n",
    "                pg_loss, entropy_loss, v_loss = losses\n",
    "                loss = pg_loss - cfg.ppo.ent_coef * entropy_loss + v_loss * cfg.ppo.vf_coef + cfg.ppo.const_coef \n",
    "                total_value_loss += v_loss\n",
    "                total_policy_loss += pg_loss\n",
    "                total_entropy_loss += entropy_loss\n",
    "                total_envs_loss += loss\n",
    "            \n",
    "            print(total_envs_loss)\n",
    "            agent.optim_zero_grad()\n",
    "            total_envs_loss.backward()\n",
    "            agent.optim_step()\n",
    "            total_epochs_envs_loss += total_envs_loss\n",
    "            total_epochs_value_loss += total_value_loss\n",
    "            total_epochs_policy_loss += total_policy_loss\n",
    "            total_epochs_entropy_loss += total_entropy_loss\n",
    "            \n",
    "    total_epochs_envs_loss = total_epochs_envs_loss / cfg.ppo.update_epochs / cfg.ppo.num_minibatches\n",
    "    total_epochs_value_loss = total_epochs_value_loss / cfg.ppo.update_epochs / cfg.ppo.num_minibatches\n",
    "    total_epochs_policy_loss = total_epochs_policy_loss / cfg.ppo.update_epochs / cfg.ppo.num_minibatches\n",
    "    total_epochs_entropy_loss = total_epochs_entropy_loss / cfg.ppo.update_epochs / cfg.ppo.num_minibatches\n",
    "    \n",
    "    return total_epochs_envs_loss, total_epochs_value_loss, total_epochs_policy_loss, total_epochs_entropy_loss\n",
    "\n",
    "def calculate_ppo_loss(params, env_index, envs, mb_inds, clipfracs):\n",
    "    i = env_index\n",
    "    mb_obs = envs_storages[i]['b_obs'][mb_inds]\n",
    "    mb_actions = envs_storages[i]['b_actions'][mb_inds]\n",
    "    mb_logprobs = envs_storages[i]['b_logprobs'][mb_inds]\n",
    "    _, newlogprob, entropy, newvalue = functional_call(agent, params, (envs.action_space, mb_obs, mb_actions))\n",
    "    #agent.get_action_and_value(envs.single_action_space, mb_obs, mb_actions)\n",
    "    logratio = newlogprob - mb_logprobs\n",
    "    ratio = logratio.exp()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "        old_approx_kl = (-logratio).mean()\n",
    "        approx_kl = ((ratio - 1) - logratio).mean()\n",
    "        clipfracs += [((ratio - 1.0).abs() > cfg.ppo.clip_coef).float().mean().item()]\n",
    "\n",
    "    mb_advantages = envs_storages[i]['b_advantages'][mb_inds]\n",
    "    if cfg.ppo.norm_adv:\n",
    "        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "    # Policy loss\n",
    "    pg_loss1 = -mb_advantages * ratio\n",
    "    pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - cfg.ppo.clip_coef, 1 + cfg.ppo.clip_coef)\n",
    "    pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "    # Value loss\n",
    "    mb_returns = envs_storages[i]['b_returns'][mb_inds].view(-1)\n",
    "    mb_values = envs_storages[i]['b_values'][mb_inds].view(-1)\n",
    "    newvalue = newvalue.view(-1)\n",
    "    if cfg.ppo.clip_vloss:\n",
    "        v_loss_unclipped = (newvalue - mb_returns) ** 2\n",
    "        v_clipped = mb_values + torch.clamp(\n",
    "            newvalue - mb_values,\n",
    "            -cfg.ppo.clip_coef,\n",
    "            cfg.ppo.clip_coef,\n",
    "        )\n",
    "        v_loss_clipped = (v_clipped - mb_returns) ** 2\n",
    "        v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "        v_loss = 0.5 * v_loss_max.mean()\n",
    "    else:\n",
    "        v_loss = 0.5 * ((newvalue - mb_returns) ** 2).mean()\n",
    "\n",
    "    # regularization term                            \n",
    "    entropy_loss = entropy.mean()\n",
    "    return pg_loss, entropy_loss, v_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     envs_storages[i][\u001b[39m'\u001b[39m\u001b[39mb_returns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m envs_storages[i][\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m     envs_storages[i][\u001b[39m'\u001b[39m\u001b[39mb_values\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m envs_storages[i][\u001b[39m'\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m total_envs_loss, total_value_loss, total_policy_loss, total_entropy_loss \u001b[39m=\u001b[39m train_agent()\n",
      "Cell \u001b[0;32mIn[57], line 34\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(total_envs_loss)\n\u001b[1;32m     33\u001b[0m agent\u001b[39m.\u001b[39moptim_zero_grad()\n\u001b[0;32m---> 34\u001b[0m total_envs_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     35\u001b[0m agent\u001b[39m.\u001b[39moptim_step()\n\u001b[1;32m     36\u001b[0m total_epochs_envs_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m total_envs_loss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "for update_idx in range(start_update_idx, total_num_updates + 1):\n",
    "    envs_returns = dict()\n",
    "    envs_lengths = dict()\n",
    "    rollout_start_time = time.time()\n",
    "    rollout(update_idx, envs_storages, next_obsv, next_donev, envs_returns, envs_lengths)\n",
    "    global_step += 1 * exp_cfg.num_envs * exp_cfg.num_rollout_steps\n",
    "    \n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        for i, envs in enumerate(train_different_envs):\n",
    "            next_value = agent.get_value(next_obsv[i]).reshape(1, -1)\n",
    "            envs_storages[i]['advantages'] = torch.zeros_like(envs_storages[i]['rewards']).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(exp_cfg.num_rollout_steps)):\n",
    "                if t == exp_cfg.num_rollout_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_donev[i]\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - envs_storages[i][\"dones\"][t + 1]\n",
    "                    nextvalues =  envs_storages[i]['values'][t + 1]\n",
    "                delta = envs_storages[i]['rewards'][t] + cfg.ppo.gamma * nextvalues * nextnonterminal - envs_storages[i]['values'][t]\n",
    "                envs_storages[i]['advantages'][t] = lastgaelam = delta + cfg.ppo.gamma * cfg.ppo.gae_lambda * nextnonterminal * lastgaelam\n",
    "            envs_storages[i]['returns'] = envs_storages[i]['advantages'] + envs_storages[i]['values']\n",
    "    gae_end_time = time.time()\n",
    "    \n",
    "    for i, envs in enumerate(train_different_envs):\n",
    "        envs_storages[i][\"b_obs\"] = envs_storages[i][\"obs\"].reshape((-1,) + envs.single_observation_space.shape)\n",
    "        envs_storages[i]['b_logprobs'] = envs_storages[i]['logprobs'].reshape(-1)\n",
    "        envs_storages[i]['b_actions'] = envs_storages[i]['actions'].reshape((-1,) + envs.single_action_space.shape)\n",
    "        envs_storages[i]['b_advantages'] = envs_storages[i]['advantages'].reshape(-1)\n",
    "        envs_storages[i]['b_returns'] = envs_storages[i]['returns'].reshape(-1)\n",
    "        envs_storages[i]['b_values'] = envs_storages[i]['values'].reshape(-1)\n",
    "    \n",
    "    total_envs_loss, total_value_loss, total_policy_loss, total_entropy_loss = train_agent()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/deprecated.py:101: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional_with_buffers is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('make_functional_with_buffers', 'torch.func.functional_call')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctorch\u001b[39;00m \u001b[39mimport\u001b[39;00m make_functional_with_buffers, vmap, grad\n\u001b[0;32m----> 3\u001b[0m actor_fmodel, actor_params, buffers \u001b[39m=\u001b[39m make_functional_with_buffers(agent\u001b[39m.\u001b[39;49mactor)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/deprecated.py:102\u001b[0m, in \u001b[0;36mmake_functional_with_buffers\u001b[0;34m(model, disable_autograd_tracking)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_functional_with_buffers\u001b[39m(model: nn\u001b[39m.\u001b[39mModule, disable_autograd_tracking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    101\u001b[0m     warn_deprecated(\u001b[39m'\u001b[39m\u001b[39mmake_functional_with_buffers\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtorch.func.functional_call\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m _nn_impl\u001b[39m.\u001b[39;49mmake_functional_with_buffers(model, disable_autograd_tracking)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/make_functional.py:474\u001b[0m, in \u001b[0;36mmake_functional_with_buffers\u001b[0;34m(model, disable_autograd_tracking)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_functional_with_buffers\u001b[39m(\n\u001b[1;32m    419\u001b[0m     model: nn\u001b[39m.\u001b[39mModule, disable_autograd_tracking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    420\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[FunctionalModuleWithBuffers, Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]:\n\u001b[1;32m    421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"make_functional_with_buffers(model, disable_autograd_tracking=False) -> func, params, buffers\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \n\u001b[1;32m    423\u001b[0m \u001b[39m    Given a ``torch.nn.Module``, make_functional_with_buffers extracts the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \n\u001b[1;32m    473\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     \u001b[39mreturn\u001b[39;00m FunctionalModuleWithBuffers\u001b[39m.\u001b[39;49m_create_from(\n\u001b[1;32m    475\u001b[0m         model, disable_autograd_tracking\u001b[39m=\u001b[39;49mdisable_autograd_tracking\n\u001b[1;32m    476\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/make_functional.py:280\u001b[0m, in \u001b[0;36mFunctionalModuleWithBuffers._create_from\u001b[0;34m(model, disable_autograd_tracking)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_from\u001b[39m(\n\u001b[1;32m    277\u001b[0m     model: nn\u001b[39m.\u001b[39mModule, disable_autograd_tracking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    278\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39m\"\u001b[39m\u001b[39mFunctionalModuleWithBuffers\u001b[39m\u001b[39m\"\u001b[39m, Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]:\n\u001b[1;32m    279\u001b[0m     \u001b[39m# TODO: We don't need to copy the model to create a stateless copy\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     model_copy \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39;49mdeepcopy(model)\n\u001b[1;32m    281\u001b[0m     params, param_names, param_names_map \u001b[39m=\u001b[39m extract_weights(model_copy)\n\u001b[1;32m    282\u001b[0m     buffers, buffer_names, buffer_names_map \u001b[39m=\u001b[39m extract_buffers(model_copy)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_tensor.py:86\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__deepcopy__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, memo)\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_leaf:\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly Tensors created explicitly by the user \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(graph leaves) support the deepcopy protocol at the moment\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39min\u001b[39;00m memo:\n\u001b[1;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m memo[\u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m)]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment"
     ]
    }
   ],
   "source": [
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "\n",
    "actor_fmodel, actor_params, buffers = make_functional_with_buffers(agent.actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7ee91ee17640122f02738ad5b71799946a97252eaa170610250681b99b684d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
