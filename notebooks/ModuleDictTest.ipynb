{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env_ids = [\"HalfCheetah-v4\", \"CartPole-v1\", \"BreakoutNoFrameskip-v4\"]\n",
    "env_list = []\n",
    "for env_id in env_ids:\n",
    "    env = gym.make(env_id)\n",
    "    if env_id == \"BreakoutNoFrameskip-v4\":\n",
    "        env.observation_space.dtype = np.float32 \n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.GrayScaleObservation(env)\n",
    "        env = gym.wrappers.FrameStack(env, 4)\n",
    "        \n",
    "    env_list.append(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_list[2].observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Box, Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    # torch.nn.init.normal_(layer.weight, std)\n",
    "    # torch.nn.init.normal_(layer.bias, std)\n",
    "    return layer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.cnn1 = layer_init(nn.Conv2d(4, 32, 8, stride=4, padding=1))\n",
    "        self.cnn2 = layer_init(nn.Conv2d(32, 64, 4, stride=2, padding=1))\n",
    "        self.cnn3 = layer_init(nn.Conv2d(64, 64, 3, stride=1, padding=1))\n",
    "        self.cnn4 = layer_init(nn.Conv2d(64, 256, 3, stride=1, padding=1))\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "        x = F.relu(self.cnn3(x))\n",
    "        x = F.relu(self.cnn4(x))\n",
    "        x = self.pooling(x)\n",
    "        x = x.view(-1, 256)\n",
    "        # x = x.squeeze()\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FrameStack<GrayScaleObservation<ResizeObservation<OrderEnforcing<PassiveEnvChecker<AtariEnv<BreakoutNoFrameskip-v4>>>>>>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/anaconda3/envs/HMRL/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be <class 'numpy.float32'>, actual type: uint8\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "img_obs, info = env_list[2].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lives': 5, 'episode_frame_number': 0, 'frame_number': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn= CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3897537/2495989917.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  img_ten = torch.tensor(img_obs).unsqueeze(0).to(torch.float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 84, 84])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_ten = torch.tensor(img_obs).unsqueeze(0).to(torch.float)\n",
    "img_ten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = cnn(img_ten)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadNet(nn.Module):\n",
    "    def __init__(self, env_ids, env_list) -> None:\n",
    "        super().__init__()\n",
    "        self.env_ids = env_ids\n",
    "        self.env_list = env_list\n",
    "        encoder_dict = dict()\n",
    "        decoder_dict = dict()\n",
    "        for env_id, env in zip(env_ids, env_list):\n",
    "            obs_dim = env.observation_space.shape\n",
    "            if len(obs_dim) < 2:\n",
    "                obs_dim = np.prod(obs_dim)\n",
    "                obs_encoder = nn.Linear(obs_dim, 256)\n",
    "            elif len(obs_dim) == 3:\n",
    "                obs_encoder = CNN()\n",
    "            if isinstance(env.action_space, Box):\n",
    "                act_dim = np.prod(env.action_space.shape)\n",
    "            elif isinstance(env.action_space, Discrete):\n",
    "                act_dim = env.action_space.n\n",
    "            act_decoder = nn.Linear(256, act_dim)\n",
    "            encoder_dict[env_id] = obs_encoder\n",
    "            decoder_dict[env_id] = act_decoder\n",
    "        self.encoder_dict = nn.ModuleDict(encoder_dict)\n",
    "        self.mlp = nn.Linear(256, 256)\n",
    "        self.decoder_dict = nn.ModuleDict(decoder_dict)\n",
    "    \n",
    "    def forward(self, env_id, action_space, x):\n",
    "        h = self.encoder_dict[env_id](x) \n",
    "        h = self.mlp(h)\n",
    "        out = self.decoder_dict[env_id](h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = MultiHeadNet(env_ids, env_list).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [env.reset()[0] for env in env_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [policy(env_id, None, torch.tensor(obs).to(torch.float32)) for env_id, obs in zip(env_ids, observations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6])\n",
      "torch.Size([2])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "for a in actions:\n",
    "    print(a.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HMRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8719e50d026025b94f58105440f85a6bf990a5dd7b0a32985b3fd4edbc65bfcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
