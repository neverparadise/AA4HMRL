{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/anaconda3/envs/HMRL/lib/python3.9/site-packages/lightning_fabric/__init__.py:36: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n",
      "/home/kukjin/anaconda3/envs/HMRL/lib/python3.9/site-packages/torchmetrics/utilities/imports.py:24: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _PYTHON_LOWER_3_8 = LooseVersion(_PYTHON_VERSION) < LooseVersion(\"3.8\")\n",
      "/home/kukjin/anaconda3/envs/HMRL/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:242: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  interpolation: int = Image.BILINEAR,\n",
      "/home/kukjin/anaconda3/envs/HMRL/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:286: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  interpolation: int = Image.NEAREST,\n",
      "/home/kukjin/anaconda3/envs/HMRL/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:319: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  interpolation: int = Image.BICUBIC,\n",
      "/home/kukjin/anaconda3/envs/HMRL/lib/python3.9/site-packages/pytorch_lightning/__init__.py:36: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "from typing import Dict\n",
    "import random\n",
    "import hydra\n",
    "from hydra.utils import get_original_cwd\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import pprint\n",
    "# import gymnasium as gym\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.linalg import matrix_norm\n",
    "from tensordict.tensordict import TensorDict\n",
    "from modules.hyper_agent import HyperAgent\n",
    "from modules.utils import set_seed, make_envpool_env\n",
    "from itertools import combinations\n",
    "import logging\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.spaces import Box as GymBox\n",
    "from gym.spaces import Discrete as GymDiscrete\n",
    "from gymnasium.spaces import Box as GymnasiumBox\n",
    "from gymnasium.spaces import Discrete as GymnasiumDiscrete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "cfg_path = \"/home/kukjin/Projects/HMRL/configs/hyper_trainer.yaml\"\n",
    "nn_cfg_path = \"/home/kukjin/Projects/HMRL/configs/nn/hyper_policy.yaml\"\n",
    "ppo_cfg_path = \"/home/kukjin/Projects/HMRL/configs/ppo/ppo.yaml\"\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "nn_cfg = OmegaConf.load(nn_cfg_path)\n",
    "ppo_cfg = OmegaConf.load(ppo_cfg_path)\n",
    "cfg.nn = nn_cfg\n",
    "cfg.ppo = ppo_cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchopt\n",
    "from torch.func import grad, grad_and_value, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"HalfCheetah-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train env 1: HalfCheetah-v4 is loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/anaconda3/envs/HMRL/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = make_envpool_env(0, env_id, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.hyper_agent import HyperAgent\n",
    "device = \"cuda:1\"\n",
    "hyperagent = HyperAgent(cfg).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cfg = cfg.experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "5\n",
      "390464\n"
     ]
    }
   ],
   "source": [
    "batch_size = int(exp_cfg.num_envs * exp_cfg.num_rollout_steps)\n",
    "minibatch_size = int(batch_size // cfg.ppo.num_minibatches)\n",
    "total_num_params = sum([np.prod(p.size()) for p in hyperagent.parameters()])\n",
    "print(batch_size)\n",
    "print(minibatch_size)\n",
    "print(total_num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from modules.hyper_agent import HyperAgent\n",
    "device = \"cuda:1\"\n",
    "hyperagent = HyperAgent(cfg).to(device)\n",
    "\n",
    "obs = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs) \\\n",
    "    + env.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs) \\\n",
    "    + env.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "rewards = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "dones = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "values = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "storage = TensorDict({\n",
    "            \"obs\": obs,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"dones\": dones,\n",
    "            }, batch_size=[exp_cfg.num_rollout_steps, exp_cfg.num_envs])\n",
    "\n",
    "hyper_rollout_steps = 4\n",
    "# 17 + 64 + 64 + 6 + 64 + 64 + 6\n",
    "obs_dim = np.prod(env.observation_space.shape)\n",
    "mid_dim = 64\n",
    "act_dim = np.prod(env.action_space.shape)\n",
    "weight_action_dim = obs_dim + 4*mid_dim + 2*act_dim\n",
    "hyper_storage = TensorDict({\n",
    "                \"trajectories\": torch.stack([storage.clone()for i in range(hyper_rollout_steps)], dim=0),\n",
    "                \"weight_actions\": torch.zeros((hyper_rollout_steps, weight_action_dim)).to(device),\n",
    "                \"logprobs\": torch.zeros((hyper_rollout_steps, 1)).to(device),\n",
    "                \"dones\": torch.zeros((hyper_rollout_steps, 1)).to(device),\n",
    "                \"values\": torch.zeros((hyper_rollout_steps, 1)).to(device),\n",
    "                }, batch_size=[hyper_rollout_steps,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        dones: Tensor(shape=torch.Size([4, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        logprobs: Tensor(shape=torch.Size([4, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        trajectories: LazyStackedTensorDict(\n",
       "            fields={\n",
       "                actions: Tensor(shape=torch.Size([4, 128, 1, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                dones: Tensor(shape=torch.Size([4, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                obs: Tensor(shape=torch.Size([4, 128, 1, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                rewards: Tensor(shape=torch.Size([4, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "            batch_size=torch.Size([4, 128, 1]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        values: Tensor(shape=torch.Size([4, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        weight_actions: Tensor(shape=torch.Size([4, 285]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([4]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs: {}\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1])\n",
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "next_obs = env.reset()\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(exp_cfg.num_envs).to(device)\n",
    "print(next_obs.shape)\n",
    "print(next_done.shape)\n",
    "actions = np.array([env.action_space.sample() for i in range(next_obs.shape[0])])\n",
    "print(actions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "action = torch.zeros_like(torch.tensor(actions))\n",
    "rewards = torch.zeros([action.shape[0], ])\n",
    "print(rewards.shape)\n",
    "obs_ary = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs) \\\n",
    "    + env.single_observation_space.shape).to(device)\n",
    "actions_ary = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs) \\\n",
    "    + env.single_action_space.shape).to(device)\n",
    "rewards_ary = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "obs_ary[0] = next_obs\n",
    "actions_ary[0] = action\n",
    "rewards_ary[0] = rewards\n",
    "init_traj = TensorDict({\n",
    "            \"obs\": obs_ary,\n",
    "            \"actions\": actions_ary,\n",
    "            \"rewards\": rewards_ary,\n",
    "            }, batch_size=[exp_cfg.num_rollout_steps, exp_cfg.num_envs])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6.6114e-04, -2.7240e-04,  9.5442e-04, -8.1753e-04, -2.0646e-04,\n",
      "          -2.9157e-04, -2.6721e-05,  9.7685e-04,  5.3563e-04,  1.0997e-03,\n",
      "          -5.5710e-04,  6.9071e-04, -4.0816e-04,  2.7745e-04, -2.9971e-04,\n",
      "          -2.3282e-04,  6.5928e-05]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]], device='cuda:1')\n",
      "torch.Size([2, 1, 17])\n",
      "torch.Size([2, 1, 6])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(obs_ary[0:2])\n",
    "\n",
    "print(obs_ary[0:2].shape)\n",
    "print(actions_ary[0:2].shape)\n",
    "print(rewards_ary[0:2].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.6114e-04, -2.7240e-04,  9.5442e-04, -8.1753e-04, -2.0646e-04,\n",
       "          -2.9157e-04, -2.6721e-05,  9.7685e-04,  5.3563e-04,  1.0997e-03,\n",
       "          -5.5710e-04,  6.9071e-04, -4.0816e-04,  2.7745e-04, -2.9971e-04,\n",
       "          -2.3282e-04,  6.5928e-05]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]], device='cuda:1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_traj[0:2]['obs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_action, log_prob, entropy, value, policy = hyperagent.get_action_value_policy(\n",
    "    init_traj,\n",
    "    env,\n",
    "    64,\n",
    "    None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_updates = exp_cfg.total_timesteps // batch_size\n",
    "start_update_idx = 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs_returns = dict()\n",
    "envs_lengths = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        actions: Tensor(shape=torch.Size([128, 1, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        dones: Tensor(shape=torch.Size([128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        obs: Tensor(shape=torch.Size([128, 1, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        rewards: Tensor(shape=torch.Size([128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([128, 1]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_obs, reward, done, infos = env.step(action.cpu().numpy())\n",
    "# traj, reward, done, infos = env.step(action.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"][0][\"rewards\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], device='cuda:1')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"][0]['dones']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(current_hyper_timesteps, env, policy, hyper_storage, \n",
    "            next_obs, hyperdone, envs_returns, envs_lengths, global_step):\n",
    "    policy = policy.to(device)\n",
    "    action_scale = torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32).to(device)\n",
    "    action_bias = torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32).to(device)\n",
    "    next_done = hyperdone\n",
    "    for step in range(0, cfg.experiment.num_rollout_steps):\n",
    "        global_step += 1 * exp_cfg.num_envs\n",
    "        hyper_storage[\"trajectories\"][current_hyper_timesteps][\"obs\"][step] = next_obs\n",
    "        hyper_storage[\"trajectories\"][current_hyper_timesteps][\"dones\"][step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = policy(next_obs, action_scale, action_bias)\n",
    "        hyper_storage[\"trajectories\"][current_hyper_timesteps][\"actions\"][step] = action\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, done, infos = env.step(action.cpu().numpy())\n",
    "        hyper_storage[\"trajectories\"][current_hyper_timesteps][\"rewards\"][step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "        for k, d in enumerate(done):\n",
    "            if d:\n",
    "                print(f\"global_step: {global_step}. episodic_return={infos['r'][k]}\")\n",
    "                print(f\"global_step: {global_step}.  episodic_length={infos['l'][k]}\")\n",
    "                envs_returns['episodic_return'] = infos[\"r\"][k]\n",
    "                envs_lengths['episodic_length'] = infos[\"l\"][k]\n",
    "    trajectories = hyper_storage[\"trajectories\"][current_hyper_timesteps]\n",
    "    gammas = torch.tensor([cfg.ppo.gamma ** i for i in range(cfg.experiment.num_rollout_steps)]).to(device)\n",
    "    reward_lst = hyper_storage[\"trajectories\"][current_hyper_timesteps][\"rewards\"].view(-1)\n",
    "    reward = torch.dot(gammas, reward_lst)\n",
    "    hyperdone = 1.0 if 1.0 in hyper_storage[\"trajectories\"][0]['dones'] else 0.0\n",
    "    hyperdone = torch.tensor(hyperdone).to(device)\n",
    "    # global_step += 1 * exp_cfg.num_envs * exp_cfg.num_rollout_steps\n",
    "    \n",
    "    return trajectories, reward, hyperdone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # \"weight_actions\": torch.zeros((hyper_rollout_steps, weight_action_dim)).to(device),\n",
    "    #             \"logprobs\": torch.zeros((hyper_rollout_steps, 1)).to(device),\n",
    "    #             \"dones\": torch.zeros((hyper_rollout_steps, 1)).to(device),\n",
    "    #             \"values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs: {}\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1])\n",
      "torch.Size([128, 1, 6])\n",
      "torch.Size([128, 1])\n",
      "global_step: 104. episodic_return=-116.12046813964844\n",
      "global_step: 104.  episodic_length=1000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m mb_advantages \u001b[39m=\u001b[39m hyper_storage[\u001b[39m\"\u001b[39m\u001b[39madvantages\u001b[39m\u001b[39m\"\u001b[39m][mb_index]\n\u001b[1;32m    117\u001b[0m mb_returns \u001b[39m=\u001b[39m hyper_storage[\u001b[39m\"\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m\"\u001b[39m][mb_index]\n\u001b[0;32m--> 119\u001b[0m weight_action, new_logprob, entropy, new_value, policy \u001b[39m=\u001b[39m hyperagent\u001b[39m.\u001b[39;49mget_action_value_policy(\n\u001b[1;32m    120\u001b[0m                                                     mb_traj,\n\u001b[1;32m    121\u001b[0m                                                     env,\n\u001b[1;32m    122\u001b[0m                                                     \u001b[39m64\u001b[39;49m,\n\u001b[1;32m    123\u001b[0m                                                     mb_weight_action\n\u001b[1;32m    124\u001b[0m                                                     )\n\u001b[1;32m    125\u001b[0m logratio \u001b[39m=\u001b[39m new_logprob\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m mb_log_prob\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    126\u001b[0m ratio \u001b[39m=\u001b[39m logratio\u001b[39m.\u001b[39mexp()\n",
      "File \u001b[0;32m~/Projects/HMRL/modules/hyper_agent.py:113\u001b[0m, in \u001b[0;36mHyperAgent.get_action_value_policy\u001b[0;34m(self, trajectory, env, num_neurons, weight_action)\u001b[0m\n\u001b[1;32m    109\u001b[0m bias_dim \u001b[39m=\u001b[39m mid_dim \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m out_dim\n\u001b[1;32m    111\u001b[0m \u001b[39m# weight\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m# TODO: batched vs mean\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m weight_vector \u001b[39m=\u001b[39m weight_action[:, \u001b[39m0\u001b[39;49m:weight_dim]\n\u001b[1;32m    114\u001b[0m weight_vector \u001b[39m=\u001b[39m weight_vector\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    115\u001b[0m bias_vector \u001b[39m=\u001b[39m weight_action[:, weight_dim:]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "# hyperagent\n",
    "from modules.hyper_agent import HyperAgent\n",
    "device = \"cuda:1\"\n",
    "hyperagent = HyperAgent(cfg).to(device)\n",
    "\n",
    "# hyper stroage\n",
    "obs = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs) \\\n",
    "    + env.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs) \\\n",
    "    + env.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "rewards = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "dones = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "values = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "storage = TensorDict({\n",
    "            \"obs\": obs,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"dones\": dones,\n",
    "            }, batch_size=[exp_cfg.num_rollout_steps, exp_cfg.num_envs])\n",
    "\n",
    "hyper_rollout_steps = 8\n",
    "# 17 + 64 + 64 + 6 + 64 + 64 + 6\n",
    "obs_dim = np.prod(env.observation_space.shape)\n",
    "mid_dim = 64\n",
    "act_dim = np.prod(env.action_space.shape)\n",
    "weight_action_dim = obs_dim + 4*mid_dim + 2*act_dim\n",
    "hyper_storage = TensorDict({\n",
    "                \"trajectories\": torch.stack([storage.clone()for i in range(hyper_rollout_steps)], dim=0),\n",
    "                \"weight_actions\": torch.zeros((hyper_rollout_steps, weight_action_dim)).to(device),\n",
    "                \"logprobs\": torch.zeros((hyper_rollout_steps, 1)).to(device),\n",
    "                \"rewards\": torch.zeros((hyper_rollout_steps, 1)).to(device),\n",
    "                \"dones\": torch.zeros((hyper_rollout_steps, 1)).to(device),\n",
    "                \"values\": torch.zeros((hyper_rollout_steps, 1)).to(device),\n",
    "                }, batch_size=[hyper_rollout_steps,])\n",
    "\n",
    "# env reset\n",
    "next_obs = env.reset()\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "hyperdone = torch.zeros(exp_cfg.num_envs).to(device)\n",
    "print(next_obs.shape)\n",
    "print(hyperdone.shape)\n",
    "sampled_actions = np.array([env.action_space.sample() for i in range(next_obs.shape[0])])\n",
    "print(actions.shape)\n",
    "action = torch.zeros_like(torch.tensor(sampled_actions))\n",
    "reward = torch.zeros([action.shape[0], ])\n",
    "print(rewards.shape)\n",
    "obs_ary = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs) \\\n",
    "    + env.single_observation_space.shape).to(device)\n",
    "actions_ary = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs) \\\n",
    "    + env.single_action_space.shape).to(device)\n",
    "rewards_ary = torch.zeros((exp_cfg.num_rollout_steps, exp_cfg.num_envs)).to(device)\n",
    "obs_ary[0] = next_obs\n",
    "actions_ary[0] = action\n",
    "rewards_ary[0] = reward\n",
    "trajectories = TensorDict({\n",
    "            \"obs\": obs_ary,\n",
    "            \"actions\": actions_ary,\n",
    "            \"rewards\": rewards_ary,\n",
    "            }, batch_size=[exp_cfg.num_rollout_steps, exp_cfg.num_envs])\n",
    "global_step = 0\n",
    "\n",
    "for update_idx in range(start_update_idx, 2):\n",
    "    envs_returns = dict()\n",
    "    envs_lengths = dict()\n",
    "    \n",
    "\n",
    "    for i in range(hyper_rollout_steps):\n",
    "        # next_traj, hyper_env.step(weight_action)\n",
    "        hyper_storage[\"trajectories\"][i] = trajectories\n",
    "        hyper_storage[\"dones\"][i] = hyperdone\n",
    "        with torch.no_grad():\n",
    "            weight_action, log_prob, entropy, value, policy = hyperagent.get_action_value_policy(\n",
    "                                                                trajectories,\n",
    "                                                                env,\n",
    "                                                                64,\n",
    "                                                                None\n",
    "                                                                )\n",
    "\n",
    "        hyper_storage[\"values\"][i] = value.flatten()\n",
    "        hyper_storage[\"weight_actions\"][i] = weight_action\n",
    "        hyper_storage[\"logprobs\"][i] = log_prob\n",
    "        \n",
    "        \n",
    "        trajectories, reward, hyperdone = rollout(i, env, policy, hyper_storage,\n",
    "                next_obs, hyperdone, envs_returns, envs_lengths, global_step)\n",
    "        hyper_storage[\"rewards\"][i] = reward\n",
    "        # print(f\"reward: {reward}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_value = hyperagent.get_value(trajectories).reshape(1, -1)\n",
    "        hyper_storage['advantages'] = torch.zeros_like(hyper_storage['rewards']).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(hyper_rollout_steps)):\n",
    "            if t == hyper_rollout_steps - 1:\n",
    "                nextnonterminal = 1.0 - hyperdone\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - hyper_storage[\"dones\"][t + 1]\n",
    "                nextvalues =  hyper_storage['values'][t + 1]\n",
    "            delta = hyper_storage['rewards'][t] + cfg.ppo.gamma * nextvalues * nextnonterminal - hyper_storage['values'][t]\n",
    "            hyper_storage['advantages'][t] = lastgaelam = delta + cfg.ppo.gamma * cfg.ppo.gae_lambda * nextnonterminal * lastgaelam\n",
    "        hyper_storage['returns'] = hyper_storage['advantages'] + hyper_storage['values']\n",
    "    \n",
    "    # hyper_storage[\"trajectories\"].reshape(hyper_rollout_steps, exp_cfg.num_rollout_steps*exp_cfg.num_envs)\n",
    "    batch_size = hyper_rollout_steps\n",
    "    # num_minibatches = 4\n",
    "    # minibatch_size = int(batch_size // num_minibatches)\n",
    "    b_inds = np.arange(batch_size)\n",
    "    for epoch in range(cfg.ppo.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for mb_index in b_inds:\n",
    "            mb_traj = hyper_storage[\"trajectories\"][mb_index]\n",
    "            mb_weight_action = hyper_storage[\"weight_actions\"][mb_index]\n",
    "            mb_log_prob = hyper_storage[\"logprobs\"][mb_index]\n",
    "            mb_advantages = hyper_storage[\"advantages\"][mb_index]\n",
    "            mb_returns = hyper_storage[\"returns\"][mb_index]\n",
    "            \n",
    "            weight_action, new_logprob, entropy, new_value, policy = hyperagent.get_action_value_policy(\n",
    "                                                                mb_traj,\n",
    "                                                                env,\n",
    "                                                                64,\n",
    "                                                                mb_weight_action\n",
    "                                                                )\n",
    "            logratio = new_logprob.view(-1) - mb_log_prob.view(-1)\n",
    "            ratio = logratio.exp()\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - cfg.ppo.clip_coef, 1 + cfg.ppo.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "            \n",
    "            new_value = new_value.view(-1)\n",
    "            v_loss = 0.5 * ((new_value - mb_returns) ** 2).mean()\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - cfg.ppo.ent_coef * entropy_loss + v_loss * cfg.ppo.vf_coef\n",
    "            hyperagent.optim_zero_grad()\n",
    "            loss.backward()\n",
    "            hyperagent.optim_step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LazyStackedTensorDict(\n",
       "    fields={\n",
       "        actions: Tensor(shape=torch.Size([2, 128, 1, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        dones: Tensor(shape=torch.Size([2, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        obs: Tensor(shape=torch.Size([2, 128, 1, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        rewards: Tensor(shape=torch.Size([2, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([2, 128, 1]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_inds = np.arange(8)\n",
    "mb_ind = b_inds[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb_ind.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mb_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LazyStackedTensorDict(\n",
       "    fields={\n",
       "        actions: Tensor(shape=torch.Size([8, 128, 1, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        dones: Tensor(shape=torch.Size([8, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        obs: Tensor(shape=torch.Size([8, 128, 1, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        rewards: Tensor(shape=torch.Size([8, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([8, 128, 1]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 6, 5, 0, 4, 7, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(b_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"].batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 0], device='cuda:1')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.randint(8, (2,)).to(device)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2397141/1910030952.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  id_ten = torch.tensor(index)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_ten = torch.tensor(index)\n",
    "len(id_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_inds = np.arange(8)\n",
    "np.random.shuffle(b_inds)\n",
    "mb_inds = b_inds[0:2]\n",
    "mb_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gather() received an invalid combination of arguments - got (TensorDict, int, numpy.ndarray, out=NoneType), but expected one of:\n * (Tensor input, int dim, Tensor index, *, bool sparse_grad, Tensor out)\n * (Tensor input, name dim, Tensor index, *, bool sparse_grad, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gathered_tensordict \u001b[39m=\u001b[39m hyper_storage\u001b[39m.\u001b[39;49mgather(dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, index\u001b[39m=\u001b[39;49mmb_inds)\n",
      "File \u001b[0;32m~/anaconda3/envs/HMRL/lib/python3.9/site-packages/tensordict/tensordict.py:2623\u001b[0m, in \u001b[0;36mTensorDictBase.gather\u001b[0;34m(self, dim, index, out)\u001b[0m\n\u001b[1;32m   2580\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgather\u001b[39m(\n\u001b[1;32m   2581\u001b[0m     \u001b[39mself\u001b[39m, dim: \u001b[39mint\u001b[39m, index: Tensor, out: TensorDictBase \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2582\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorDictBase:\n\u001b[1;32m   2583\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gathers values along an axis specified by `dim`.\u001b[39;00m\n\u001b[1;32m   2584\u001b[0m \n\u001b[1;32m   2585\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2621\u001b[0m \u001b[39m        [\"a\", \"b\"]\u001b[39;00m\n\u001b[1;32m   2622\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2623\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgather(\u001b[39mself\u001b[39;49m, dim, index, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mTypeError\u001b[0m: gather() received an invalid combination of arguments - got (TensorDict, int, numpy.ndarray, out=NoneType), but expected one of:\n * (Tensor input, int dim, Tensor index, *, bool sparse_grad, Tensor out)\n * (Tensor input, name dim, Tensor index, *, bool sparse_grad, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "gathered_tensordict = hyper_storage.gather(dim=0, index=mb_inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensordict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "gathered_tensordict = hyper_storage.gather(dim=0, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        advantages: Tensor(shape=torch.Size([2, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        dones: Tensor(shape=torch.Size([2, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        logprobs: Tensor(shape=torch.Size([2, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        returns: Tensor(shape=torch.Size([2, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        rewards: Tensor(shape=torch.Size([2, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        trajectories: TensorDict(\n",
       "            fields={\n",
       "                actions: Tensor(shape=torch.Size([2, 128, 1, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                dones: Tensor(shape=torch.Size([2, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                obs: Tensor(shape=torch.Size([2, 128, 1, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                rewards: Tensor(shape=torch.Size([2, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "            batch_size=torch.Size([2, 128, 1]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        values: Tensor(shape=torch.Size([2, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        weight_actions: Tensor(shape=torch.Size([2, 285]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([2]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gathered_tensordict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute (got 8, expected 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hyper_storage[\u001b[39m\"\u001b[39;49m\u001b[39mtrajectories\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mpermute(b_inds)\n",
      "File \u001b[0;32m~/anaconda3/envs/HMRL/lib/python3.9/site-packages/tensordict/tensordict.py:2717\u001b[0m, in \u001b[0;36mTensorDictBase.permute\u001b[0;34m(self, dims, *dims_list)\u001b[0m\n\u001b[1;32m   2715\u001b[0m     dims_list \u001b[39m=\u001b[39m dims_list[\u001b[39m0\u001b[39m]\n\u001b[1;32m   2716\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dims_list) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape):\n\u001b[0;32m-> 2717\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2718\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumber of dims don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match in permute (got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(dims_list)\u001b[39m}\u001b[39;00m\u001b[39m, expected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2719\u001b[0m     )\n\u001b[1;32m   2721\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(dims_list) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_dims:\n\u001b[1;32m   2722\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute (got 8, expected 3"
     ]
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"].permute(b_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        actions: Tensor(shape=torch.Size([128, 2, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        dones: Tensor(shape=torch.Size([128, 2]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        obs: Tensor(shape=torch.Size([128, 2, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        rewards: Tensor(shape=torch.Size([128, 2]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([128, 2]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.array((0, 1))\n",
    "hyper_storage[\"trajectories\"][0:2].reshape(128, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        actions: Tensor(shape=torch.Size([128, 1, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        dones: Tensor(shape=torch.Size([128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        obs: Tensor(shape=torch.Size([128, 1, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        rewards: Tensor(shape=torch.Size([128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([128, 1]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]], device='cuda:1')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"][\"dones\"][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.logical_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(int(False))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:1')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperdone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:1')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"logprobs\"][0].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        actions: Tensor(shape=torch.Size([128, 1, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        dones: Tensor(shape=torch.Size([128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        obs: Tensor(shape=torch.Size([128, 1, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        rewards: Tensor(shape=torch.Size([128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([128, 1]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 285])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"weight_actions\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        dones: Tensor(shape=torch.Size([4, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        logprobs: Tensor(shape=torch.Size([4, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        trajectories: LazyStackedTensorDict(\n",
       "            fields={\n",
       "                actions: Tensor(shape=torch.Size([4, 128, 1, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                dones: Tensor(shape=torch.Size([4, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                obs: Tensor(shape=torch.Size([4, 128, 1, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                rewards: Tensor(shape=torch.Size([4, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "            batch_size=torch.Size([4, 128, 1]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        values: Tensor(shape=torch.Size([4, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        weight_actions: Tensor(shape=torch.Size([4, 285]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([4]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LazyStackedTensorDict(\n",
       "    fields={\n",
       "        actions: Tensor(shape=torch.Size([4, 128, 1, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        dones: Tensor(shape=torch.Size([4, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        obs: Tensor(shape=torch.Size([4, 128, 1, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        rewards: Tensor(shape=torch.Size([4, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([4, 128, 1]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        actions: Tensor(shape=torch.Size([4, 128, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        dones: Tensor(shape=torch.Size([4, 128]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        obs: Tensor(shape=torch.Size([4, 128, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        rewards: Tensor(shape=torch.Size([4, 128]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([4, 128]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"].reshape(hyper_rollout_steps, exp_cfg.num_rollout_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        actions: Tensor(shape=torch.Size([128, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        dones: Tensor(shape=torch.Size([128]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        obs: Tensor(shape=torch.Size([128, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        rewards: Tensor(shape=torch.Size([128]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([128]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage[\"trajectories\"][0:1].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        dones: Tensor(shape=torch.Size([4, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        logprobs: Tensor(shape=torch.Size([4, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        trajectories: LazyStackedTensorDict(\n",
       "            fields={\n",
       "                actions: Tensor(shape=torch.Size([4, 128, 1, 6]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                dones: Tensor(shape=torch.Size([4, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                obs: Tensor(shape=torch.Size([4, 128, 1, 17]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "                rewards: Tensor(shape=torch.Size([4, 128, 1]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "            batch_size=torch.Size([4, 128, 1]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        values: Tensor(shape=torch.Size([4, 1]), device=cuda:1, dtype=torch.float32, is_shared=True),\n",
       "        weight_actions: Tensor(shape=torch.Size([4, 285]), device=cuda:1, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([4]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7030,  0.9323,  0.7391,  0.7484, -0.8807, -0.1586, -1.2903,\n",
       "          -0.7783, -1.4226, -0.2737,  1.2954, -1.6936,  0.2868,  0.5331,\n",
       "          -1.2357,  1.4036,  0.1558]],\n",
       "\n",
       "        [[-0.7530,  0.7181, -0.5714, -0.6478, -0.8127,  0.6499,  0.8299,\n",
       "           0.7254,  0.2494,  0.4721,  0.0887, -0.7353, -0.7354,  0.6125,\n",
       "           1.1471,  0.6737,  0.6369]]], device='cuda:1')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HMRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8719e50d026025b94f58105440f85a6bf990a5dd7b0a32985b3fd4edbc65bfcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
