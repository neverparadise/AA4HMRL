{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LRU(nn.Module):\n",
    "    def __init__(self,in_features,out_features,state_features, rmin=0, rmax=1,max_phase=6.283):\n",
    "        super().__init__()\n",
    "        self.out_features=out_features\n",
    "        self.D=nn.Parameter(torch.randn([out_features,in_features])/math.sqrt(in_features))\n",
    "        u1=torch.rand(state_features)\n",
    "        u2=torch.rand(state_features)\n",
    "        self.nu_log= nn.Parameter(torch.log(-0.5*torch.log(u1*(rmax+rmin)*(rmax-rmin) + rmin**2)))\n",
    "        self.theta_log= nn.Parameter(torch.log(max_phase*u2))\n",
    "        Lambda_mod=torch.exp(-torch.exp(self.nu_log))\n",
    "        self.gamma_log=nn.Parameter(torch.log(torch.sqrt(torch.ones_like(Lambda_mod)-torch.square(Lambda_mod))))\n",
    "        B_re=torch.randn([state_features,in_features])/math.sqrt(2*in_features)\n",
    "        B_im=torch.randn([state_features,in_features])/math.sqrt(2*in_features)\n",
    "        self.B=nn.Parameter(torch.complex(B_re,B_im))\n",
    "        C_re=torch.randn([out_features,state_features])/math.sqrt(state_features)\n",
    "        C_im=torch.randn([out_features,state_features])/math.sqrt(state_features)\n",
    "        self.C=nn.Parameter(torch.complex(C_re,C_im))\n",
    "        self.state=torch.complex(torch.zeros(state_features),torch.zeros(state_features))\n",
    "\n",
    "    def forward(self, input,state=None):\n",
    "        self.state=self.state.to(self.B.device) if state==None else state\n",
    "        Lambda_mod=torch.exp(-torch.exp(self.nu_log))\n",
    "        Lambda_re=Lambda_mod*torch.cos(torch.exp(self.theta_log))\n",
    "        Lambda_im=Lambda_mod*torch.sin(torch.exp(self.theta_log))\n",
    "        Lambda=torch.complex(Lambda_re,Lambda_im)\n",
    "        Lambda=Lambda.to(self.state.device)\n",
    "        gammas=torch.exp(self.gamma_log).unsqueeze(-1).to(self.B.device)\n",
    "        gammas=gammas.to(self.state.device)\n",
    "        output=torch.empty([i for i in input.shape[:-1]] +[self.out_features],device=self.B.device)\n",
    "        #Handle input of (Batches,Seq_length, Input size)\n",
    "        if input.dim()==3:\n",
    "            for i,batch in enumerate(input):\n",
    "                out_seq=torch.empty(input.shape[1],self.out_features)\n",
    "                for j,step in enumerate(batch):\n",
    "                    self.state=(Lambda@self.state + gammas* self.B@step.to(dtype= self.B.dtype))\n",
    "                    out_step= (self.C@self.state).real + self.D@step\n",
    "                    out_seq[j]=out_step\n",
    "                self.state=torch.complex(torch.zeros_like(self.state.real),torch.zeros_like(self.state.real))\n",
    "                output[i]=out_seq\n",
    "        #Handle input of (Seq_length, Input size)\n",
    "        if input.dim()==2:\n",
    "            for i,step in enumerate(input):\n",
    "                self.state=(Lambda@self.state + gammas* self.B@step.to(dtype= self.B.dtype))\n",
    "                out_step= (self.C@self.state).real + self.D@step\n",
    "                output[i]=out_step\n",
    "            self.state=torch.complex(torch.zeros_like(self.state.real),torch.zeros_like(self.state.real))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LRU(in_features=1,\n",
    "          out_features=32,\n",
    "          state_features=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = torch.randn([32, 100, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259 ms ± 1.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output = rnn(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rnn = torch.compile(LRU(in_features=1,\n",
    "          out_features=32,\n",
    "          state_features=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mtimeit\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39moutput = compiled_rnn(sequence)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2430\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2428\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2429\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2430\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2432\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2433\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2434\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/IPython/core/magics/execution.py:1163\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[1;32m   1162\u001b[0m     number \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m index\n\u001b[0;32m-> 1163\u001b[0m     time_number \u001b[39m=\u001b[39m timer\u001b[39m.\u001b[39;49mtimeit(number)\n\u001b[1;32m   1164\u001b[0m     \u001b[39mif\u001b[39;00m time_number \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m:\n\u001b[1;32m   1165\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/IPython/core/magics/execution.py:157\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    155\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    156\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    158\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:541\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    538\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m count_calls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(pass2\u001b[39m.\u001b[39mgraph_outputs) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    540\u001b[0m     output\u001b[39m.\u001b[39mextend(\n\u001b[0;32m--> 541\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, pass2\u001b[39m.\u001b[39;49mgraph_output_vars(), root)\n\u001b[1;32m    542\u001b[0m     )\n\u001b[1;32m    544\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pass2\u001b[39m.\u001b[39mgraph_outputs) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    545\u001b[0m         output\u001b[39m.\u001b[39mappend(pass2\u001b[39m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    668\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexample_inputs())\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    672\u001b[0m \u001b[39massert\u001b[39;00m callable(compiled_fn), \u001b[39m\"\u001b[39m\u001b[39mcompiler_fn did not return callable\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/__init__.py:1390\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, model_, inputs_):\n\u001b[1;32m   1388\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inductor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompile_fx\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1390\u001b[0m     \u001b[39mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:455\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_compile(\n\u001b[1;32m    442\u001b[0m         model,\n\u001b[1;32m    443\u001b[0m         example_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m         graph_id\u001b[39m=\u001b[39mgraph_id,\n\u001b[1;32m    448\u001b[0m     )\n\u001b[1;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m overrides\u001b[39m.\u001b[39mpatch_functions():\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m     \u001b[39m# TODO: can add logging before/after the call to create_aot_dispatcher_function\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39m# in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39m# once torchdynamo is merged into pytorch\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m aot_autograd(\n\u001b[1;32m    456\u001b[0m         fw_compiler\u001b[39m=\u001b[39;49mfw_compiler,\n\u001b[1;32m    457\u001b[0m         bw_compiler\u001b[39m=\u001b[39;49mbw_compiler,\n\u001b[1;32m    458\u001b[0m         decompositions\u001b[39m=\u001b[39;49mselect_decomp_table(),\n\u001b[1;32m    459\u001b[0m         partition_fn\u001b[39m=\u001b[39;49mfunctools\u001b[39m.\u001b[39;49mpartial(\n\u001b[1;32m    460\u001b[0m             min_cut_rematerialization_partition, compiler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minductor\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    461\u001b[0m         ),\n\u001b[1;32m    462\u001b[0m         keep_inference_input_mutations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    463\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:48\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[39m# NB: NOT cloned!\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[39mwith\u001b[39;00m enable_aot_logging():\n\u001b[0;32m---> 48\u001b[0m         cg \u001b[39m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m         counters[\u001b[39m\"\u001b[39m\u001b[39maot_autograd\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     50\u001b[0m         \u001b[39mreturn\u001b[39;00m eval_frame\u001b[39m.\u001b[39mdisable(cg)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2822\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2819\u001b[0m full_args\u001b[39m.\u001b[39mextend(params_flat)\n\u001b[1;32m   2820\u001b[0m full_args\u001b[39m.\u001b[39mextend(args)\n\u001b[0;32m-> 2822\u001b[0m compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m   2823\u001b[0m     functional_call,\n\u001b[1;32m   2824\u001b[0m     full_args,\n\u001b[1;32m   2825\u001b[0m     aot_config,\n\u001b[1;32m   2826\u001b[0m )\n\u001b[1;32m   2828\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2829\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2830\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2831\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n\u001b[1;32m   2832\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39mruntime_args):\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2515\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2512\u001b[0m compiler_fn \u001b[39m=\u001b[39m partial(aot_wrapper_dedupe, compiler_fn\u001b[39m=\u001b[39mcompiler_fn)\n\u001b[1;32m   2513\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2515\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config)\n\u001b[1;32m   2517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2518\u001b[0m     compiled_fn \u001b[39m=\u001b[39m make_boxed_func(compiled_fn)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1715\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1712\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m     \u001b[39mif\u001b[39;00m ok:\n\u001b[0;32m-> 1715\u001b[0m         \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config)\n\u001b[1;32m   1717\u001b[0m \u001b[39m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[39m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[39m#   }\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[39m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m seen_args \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2104\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[39mwith\u001b[39;00m enable_python_dispatcher():\n\u001b[1;32m   2103\u001b[0m     flattened_joints, _ \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(joint_inputs)\n\u001b[0;32m-> 2104\u001b[0m     fx_g \u001b[39m=\u001b[39m make_fx(joint_forward_backward, aot_config\u001b[39m.\u001b[39;49mdecompositions)(\n\u001b[1;32m   2105\u001b[0m         \u001b[39m*\u001b[39;49mjoint_inputs\n\u001b[1;32m   2106\u001b[0m     )\n\u001b[1;32m   2108\u001b[0m \u001b[39m# There should be *NO* mutating ops in the graph at this point.\u001b[39;00m\n\u001b[1;32m   2109\u001b[0m assert_functional_graph(fx_g\u001b[39m.\u001b[39mgraph)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:714\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[39m# We disable the autocast cache as the autocast cache causes type conversions on parameters to\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[39m# check a cache, which introduces untracked tensors into the graph\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[39m# We also disable tracing by any other tensor proxy-based tracers except the current. The\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# purpose of `make_fx` is to produce graphmodules as a side effect; its internal execution is\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# thus irrelevant to any external functional trace.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39mwith\u001b[39;00m decompose(decomposition_table), fake_tensor_mode, python_dispatcher_mode, \\\n\u001b[1;32m    713\u001b[0m      sym_mode, proxy_mode, disable_autocast_cache(), disable_proxy_modes_tracing(enable_current\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 714\u001b[0m     t \u001b[39m=\u001b[39m dispatch_trace(wrap_key(func, args, fx_tracer), tracer\u001b[39m=\u001b[39;49mfx_tracer, concrete_args\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(phs))\n\u001b[1;32m    716\u001b[0m \u001b[39m# TODO: kind of a bad way to do it, should maybe figure out a better way\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[39mif\u001b[39;00m tracing_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msymbolic\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:443\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdispatch_trace\u001b[39m(\n\u001b[1;32m    439\u001b[0m         root: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, Callable],\n\u001b[1;32m    440\u001b[0m         tracer: Tracer,\n\u001b[1;32m    441\u001b[0m         concrete_args: Optional[Tuple[Any, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GraphModule:\n\u001b[0;32m--> 443\u001b[0m     graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace(root, concrete_args)\n\u001b[1;32m    444\u001b[0m     name \u001b[39m=\u001b[39m root\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(root, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule) \u001b[39melse\u001b[39;00m root\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    445\u001b[0m     \u001b[39mreturn\u001b[39;00m GraphModule(tracer\u001b[39m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_search:\n\u001b[1;32m    772\u001b[0m             _autowrap_check(\n\u001b[1;32m    773\u001b[0m                 patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m             )\n\u001b[1;32m    775\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m             (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),),\n\u001b[1;32m    779\u001b[0m             {},\n\u001b[1;32m    780\u001b[0m             type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    783\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:652\u001b[0m, in \u001b[0;36mTracer.create_args_for_root.<locals>.flatten_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflatten_fn\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[1;32m    651\u001b[0m     tree_args \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_unflatten(\u001b[39mlist\u001b[39m(args), in_spec)\n\u001b[0;32m--> 652\u001b[0m     tree_out \u001b[39m=\u001b[39m root_fn(\u001b[39m*\u001b[39;49mtree_args)\n\u001b[1;32m    653\u001b[0m     out_args, out_spec \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(tree_out)\n\u001b[1;32m    654\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_codegen, _PyTreeCodeGen)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:459\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[0;34m(*proxies)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mwith\u001b[39;00m _pop_mode_temporarily():\n\u001b[1;32m    457\u001b[0m     track_tensor_tree(flat_tensors, flat_proxies, constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tracer\u001b[39m=\u001b[39mtracer)\n\u001b[0;32m--> 459\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mtensors)\n\u001b[1;32m    460\u001b[0m out \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_map_only(\n\u001b[1;32m    461\u001b[0m     torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    462\u001b[0m     \u001b[39mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mproxy),\n\u001b[1;32m    463\u001b[0m     out\n\u001b[1;32m    464\u001b[0m )\n\u001b[1;32m    465\u001b[0m out \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_map_only(\n\u001b[1;32m    466\u001b[0m     (SymInt, SymFloat, SymBool),\n\u001b[1;32m    467\u001b[0m     \u001b[39mlambda\u001b[39;00m t: get_proxy_slot(t\u001b[39m.\u001b[39mnode, tracer)(),\n\u001b[1;32m    468\u001b[0m     out\n\u001b[1;32m    469\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1158\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.traced_joint\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraced_joint\u001b[39m(primals, tangents):\n\u001b[0;32m-> 1158\u001b[0m     \u001b[39mreturn\u001b[39;00m functionalized_f_helper(primals, tangents)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1110\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.functionalized_f_helper\u001b[0;34m(primals, maybe_tangents)\u001b[0m\n\u001b[1;32m   1107\u001b[0m torch\u001b[39m.\u001b[39m_enable_functionalization(reapply_views\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1108\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1109\u001b[0m     \u001b[39m# Run the joint\u001b[39;00m\n\u001b[0;32m-> 1110\u001b[0m     f_outs \u001b[39m=\u001b[39m flat_fn_no_input_mutations(fn, f_primals, f_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1111\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1112\u001b[0m     torch\u001b[39m.\u001b[39m_disable_functionalization()\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1078\u001b[0m, in \u001b[0;36mflat_fn_no_input_mutations\u001b[0;34m(fn, primals, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     primals_after_cloning \u001b[39m=\u001b[39m primals\n\u001b[0;32m-> 1078\u001b[0m outs \u001b[39m=\u001b[39m flat_fn_with_synthetic_bases_expanded(fn, primals, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1079\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1050\u001b[0m, in \u001b[0;36mflat_fn_with_synthetic_bases_expanded\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1048\u001b[0m primals \u001b[39m=\u001b[39m unpack_synthetic_bases(primals_after_cloning, meta\u001b[39m.\u001b[39msynthetic_base_info)\n\u001b[1;32m   1049\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(meta\u001b[39m.\u001b[39mfw_metadata\u001b[39m.\u001b[39minput_info) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(primals)\n\u001b[0;32m-> 1050\u001b[0m outs \u001b[39m=\u001b[39m forward_or_joint(fn, primals_before_cloning, primals, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1051\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:916\u001b[0m, in \u001b[0;36mforward_or_joint\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_or_joint\u001b[39m(\n\u001b[1;32m    909\u001b[0m     fn: Callable,\n\u001b[1;32m    910\u001b[0m     primals_before_cloning: List[Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    914\u001b[0m     keep_input_mutations: \u001b[39mbool\u001b[39m,\n\u001b[1;32m    915\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 916\u001b[0m     outs \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49mprimals_after_cloning)\n\u001b[1;32m    917\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(meta\u001b[39m.\u001b[39mfw_metadata\u001b[39m.\u001b[39moutput_info) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(outs)\n\u001b[1;32m    919\u001b[0m     \u001b[39m# The compiled fw will return mutated input tensors, *including* metadata-only mutation.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m     \u001b[39m# However, if keep_input_mutations is set, the compiled fw only needs to return metadata-mutated inputs.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[39m# (because data-only input mutations are handled directly in the compiled graph)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2793\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.functional_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2789\u001b[0m         warnings\u001b[39m.\u001b[39mfilterwarnings(\n\u001b[1;32m   2790\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAnomaly Detection has been enabled.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2791\u001b[0m         )\n\u001b[1;32m   2792\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mdetect_anomaly(check_nan\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m-> 2793\u001b[0m             out \u001b[39m=\u001b[39m Interpreter(mod)\u001b[39m.\u001b[39;49mrun(\u001b[39m*\u001b[39;49margs[params_len:], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2794\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2795\u001b[0m     out \u001b[39m=\u001b[39m mod(\u001b[39m*\u001b[39margs[params_len:], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/interpreter.py:136\u001b[0m, in \u001b[0;36mInterpreter.run\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv[node] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_node(node)\n\u001b[1;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhile executing \u001b[39m\u001b[39m{\u001b[39;00mnode\u001b[39m.\u001b[39mformat_node()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/interpreter.py:177\u001b[0m, in \u001b[0;36mInterpreter.run_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(kwargs, \u001b[39mdict\u001b[39m)\n\u001b[0;32m--> 177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, n\u001b[39m.\u001b[39;49mop)(n\u001b[39m.\u001b[39;49mtarget, args, kwargs)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/interpreter.py:249\u001b[0m, in \u001b[0;36mInterpreter.call_function\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(target, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    248\u001b[0m \u001b[39m# Execute the function and return the result\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_inductor/overrides.py:38\u001b[0m, in \u001b[0;36mAutogradMonkeypatch.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m replacements \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     config\u001b[39m.\u001b[39mfallback_random\n\u001b[1;32m     35\u001b[0m     \u001b[39mand\u001b[39;00m replacements[func] \u001b[39min\u001b[39;00m replacements_using_triton_random\n\u001b[1;32m     36\u001b[0m ):\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m replacements[func](\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m     simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:487\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39m@count\u001b[39m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__torch_dispatch__\u001b[39m(\u001b[39mself\u001b[39m, func, types, args\u001b[39m=\u001b[39m(), kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    486\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msym_mode\u001b[39m.\u001b[39menable(\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 487\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_torch_dispatch(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:512\u001b[0m, in \u001b[0;36mProxyTorchDispatchMode.inner_torch_dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m [prim\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mdefault]:\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 512\u001b[0m out \u001b[39m=\u001b[39m proxy_call(\u001b[39mself\u001b[39;49m, func, args, kwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:345\u001b[0m, in \u001b[0;36mproxy_call\u001b[0;34m(proxy_mode, func, args, kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m         args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mproxy \u001b[39m=\u001b[39m proxy_out\n\u001b[0;32m--> 345\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    347\u001b[0m \u001b[39m# In some circumstances, we will be tracing in a situation where a tensor\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m# is *statically* known to be a constant (currently, this only happens if\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m# you run torch.tensor; deterministic factory functions like torch.arange\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m# propagating const-ness.  Similarly, we don't require the constant to\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# live on CPU, but we could.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m any_constant \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_any_only(_ProxyTensor, \u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39mconstant \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, (f_args, f_kwargs))\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/utils/_stats.py:15\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(fn):\n\u001b[0;32m---> 15\u001b[0m     \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     17\u001b[0m         \u001b[39mif\u001b[39;00m fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m simple_call_counter:\n\u001b[1;32m     18\u001b[0m             simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output = compiled_rnn(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'triton.language' has no attribute 'associative_scan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<string>:21\u001b[0m, in \u001b[0;36mkernel\u001b[0;34m(X, Z, BLOCK_M, BLOCK_N, AXIS, grid, num_warps, num_stages, extern_libs, stream, warmup)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('2-.-0-.-0-d82511111ad128294e9d31a6ac684238-d6252949da17ceb5f3a278a70250af13-3b85c7bef5f0a641282f3b73af50f599-3d2aedeb40d6d81c66a42791e268f98b-3498c340fd4b6ee7805fd54b882a04f5-e1f133f98d04093da2078dfc51c36b72-b26258bf01f839199e39d64851821f26-d7c06e3b46e708006c15224aac7a1378-f585402118c8a136948ce0a49cfe122c', (torch.float32, torch.float32), (1024, 1, 0), (True, True))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# triton result\u001b[39;00m\n\u001b[1;32m     83\u001b[0m z_tri \u001b[39m=\u001b[39m to_triton(z, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 84\u001b[0m val \u001b[39m=\u001b[39m kernel[(\u001b[39m1\u001b[39;49m,)](x_tri, z_tri, BLOCK_M\u001b[39m=\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], BLOCK_N\u001b[39m=\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], AXIS\u001b[39m=\u001b[39;49maxis, num_warps\u001b[39m=\u001b[39;49mnum_warps)\n\u001b[1;32m     85\u001b[0m out_triton \u001b[39m=\u001b[39m to_numpy(z_tri)\n\u001b[1;32m     86\u001b[0m vals_to_compare\u001b[39m.\u001b[39mappend(out_triton)\n",
      "File \u001b[0;32m<string>:41\u001b[0m, in \u001b[0;36mkernel\u001b[0;34m(X, Z, BLOCK_M, BLOCK_N, AXIS, grid, num_warps, num_stages, extern_libs, stream, warmup)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/triton/compiler.py:1590\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(fn, **kwargs)\u001b[0m\n\u001b[1;32m   1588\u001b[0m so_path \u001b[39m=\u001b[39m make_stub(name, signature, constants)\n\u001b[1;32m   1589\u001b[0m \u001b[39m# create cache manager\u001b[39;00m\n\u001b[0;32m-> 1590\u001b[0m fn_cache_manager \u001b[39m=\u001b[39m CacheManager(make_hash(fn, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m   1591\u001b[0m \u001b[39m# determine name and extension type of provided function\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fn, triton\u001b[39m.\u001b[39mruntime\u001b[39m.\u001b[39mJITFunction):\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/triton/compiler.py:1500\u001b[0m, in \u001b[0;36mmake_hash\u001b[0;34m(fn, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     get_conf_key \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m conf: (\u001b[39msorted\u001b[39m(conf\u001b[39m.\u001b[39mdivisible_by_16), \u001b[39msorted\u001b[39m(conf\u001b[39m.\u001b[39mequal_to_1))\n\u001b[1;32m   1499\u001b[0m     configs_key \u001b[39m=\u001b[39m [get_conf_key(conf) \u001b[39mfor\u001b[39;00m conf \u001b[39min\u001b[39;00m configs]\n\u001b[0;32m-> 1500\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfn\u001b[39m.\u001b[39;49mcache_key\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(signature\u001b[39m.\u001b[39mvalues())\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mconfigs_key\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mconstants\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mnum_warps\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mnum_stages\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1501\u001b[0m     \u001b[39mreturn\u001b[39;00m hashlib\u001b[39m.\u001b[39mmd5(key\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mhexdigest()\n\u001b[1;32m   1502\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(fn, \u001b[39mstr\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/triton/runtime/jit.py:333\u001b[0m, in \u001b[0;36mJITFunction.cache_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhash \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     dependencies_finder \u001b[39m=\u001b[39m DependenciesFinder(\u001b[39mglobals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__globals__\u001b[39m, src\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc)\n\u001b[0;32m--> 333\u001b[0m     dependencies_finder\u001b[39m.\u001b[39;49mvisit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse())\n\u001b[1;32m    334\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhash \u001b[39m=\u001b[39m dependencies_finder\u001b[39m.\u001b[39mret \u001b[39m+\u001b[39m version_key()\n\u001b[1;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhash\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/ast.py:426\u001b[0m, in \u001b[0;36mNodeVisitor.generic_visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m value:\n\u001b[1;32m    425\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, AST):\n\u001b[0;32m--> 426\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(item)\n\u001b[1;32m    427\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, AST):\n\u001b[1;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(value)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/ast.py:426\u001b[0m, in \u001b[0;36mNodeVisitor.generic_visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m value:\n\u001b[1;32m    425\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, AST):\n\u001b[0;32m--> 426\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(item)\n\u001b[1;32m    427\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, AST):\n\u001b[1;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(value)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/ast.py:428\u001b[0m, in \u001b[0;36mNodeVisitor.generic_visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(item)\n\u001b[1;32m    427\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, AST):\n\u001b[0;32m--> 428\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(value)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/triton/runtime/jit.py:55\u001b[0m, in \u001b[0;36mDependenciesFinder.visit_Call\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisit_Call\u001b[39m(\u001b[39mself\u001b[39m, node):\n\u001b[0;32m---> 55\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(node\u001b[39m.\u001b[39;49mfunc)\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m node\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[39mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/triton/runtime/jit.py:52\u001b[0m, in \u001b[0;36mDependenciesFinder.visit_Attribute\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m lhs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m lhs \u001b[39mis\u001b[39;00m triton:\n\u001b[1;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(lhs, node\u001b[39m.\u001b[39;49mattr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'triton.language' has no attribute 'associative_scan'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime.jit import TensorWrapper, reinterpret\n",
    "\n",
    "int_dtypes = ['int8', 'int16', 'int32', 'int64']\n",
    "uint_dtypes = ['uint8', 'uint16', 'uint32', 'uint64']\n",
    "float_dtypes = ['float16', 'float32', 'float64']\n",
    "dtypes = int_dtypes + uint_dtypes + float_dtypes\n",
    "dtypes_with_bfloat16 = dtypes + ['bfloat16']\n",
    "torch_dtypes = ['bool'] + int_dtypes + ['uint8'] + float_dtypes + ['bfloat16']\n",
    "    \n",
    "def to_triton(x: np.ndarray, device='cuda', dst_type=None):\n",
    "    t = x.dtype.name\n",
    "    if t in uint_dtypes:\n",
    "        signed_type_name = t.lstrip('u')  # e.g. \"uint16\" -> \"int16\"\n",
    "        x_signed = x.astype(getattr(np, signed_type_name))\n",
    "        return reinterpret(torch.tensor(x_signed, device=device).contiguous(), getattr(tl, t))\n",
    "    else:\n",
    "        if dst_type and 'float8' in dst_type:\n",
    "            return reinterpret(torch.tensor(x, device=device).contiguous(), getattr(tl, dst_type))\n",
    "        if t == 'float32' and dst_type == 'bfloat16':\n",
    "            return torch.tensor(x, device=device).contiguous().bfloat16()\n",
    "        return torch.tensor(x, device=device).contiguous()\n",
    "    \n",
    "def to_numpy(x):\n",
    "    if isinstance(x, TensorWrapper):\n",
    "        return x.base.cpu().numpy().astype(getattr(np, torch_dtypes(x.dtype)))\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        if x.dtype is torch.bfloat16:\n",
    "            return x.cpu().float().numpy()\n",
    "        return x.cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(f\"Not a triton-compatible tensor: {x}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    use_gpu = True\n",
    "\n",
    "    if use_gpu:\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = None\n",
    "\n",
    "    triton_times = []\n",
    "    loop_times = []\n",
    "    loop_comp_times = []\n",
    "    vals_to_compare = []\n",
    "\n",
    "    op = 'cumsum'\n",
    "    num_warps = 16\n",
    "    dtype_str = 'float32'\n",
    "    axis = 0\n",
    "    shape = (1024, 1)\n",
    "    n_timings = 10\n",
    "\n",
    "    x = np.random.rand(*shape).astype(dtype=np.float32)\n",
    "    inp = torch.tensor(x, device=device, requires_grad=True, dtype=torch.float32)\n",
    "    init = torch.zeros(shape[1], 1, device=device, requires_grad=True)\n",
    "    inp_scan = inp\n",
    "\n",
    "    @triton.jit\n",
    "    def sum_op(a, b):\n",
    "        return a + b\n",
    "\n",
    "    @triton.jit\n",
    "    def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n",
    "        range_m = tl.arange(0, BLOCK_M)\n",
    "        range_n = tl.arange(0, BLOCK_N)\n",
    "        x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n",
    "        #tl.device_print(\"z\", x)\n",
    "        z = tl.associative_scan(x, 0, sum_op)\n",
    "        #tl.device_print(\"z\", z)\n",
    "        tl.store(Z + range_m[:, None] * BLOCK_N + range_n[None, :], z)\n",
    "\n",
    "    z = np.empty_like(x)\n",
    "    x_tri = to_triton(x, device=device)\n",
    "    numpy_op = np.cumsum\n",
    "    z_dtype_str = dtype_str\n",
    "    z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n",
    "    # triton result\n",
    "    z_tri = to_triton(z, device=device)\n",
    "    val = kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis, num_warps=num_warps)\n",
    "    out_triton = to_numpy(z_tri)\n",
    "    vals_to_compare.append(out_triton)\n",
    "\n",
    "    for _ in range(n_timings):\n",
    "        start = time.time_ns()\n",
    "        kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis, num_warps=num_warps)\n",
    "        stop = time.time_ns()\n",
    "        triton_times.append((stop - start) / (10 ** 9))\n",
    "\n",
    "    def f(carry, x):\n",
    "        return carry+x, carry+x\n",
    "\n",
    "    def _fake_scan(f, init, x):\n",
    "        zs = []\n",
    "        carry = init\n",
    "        for xp in x:\n",
    "            carry, out = f(carry, xp)\n",
    "            zs.append(out)\n",
    "        return carry, torch.stack(zs)\n",
    "\n",
    "    expected_carry_out, expected_ys = _fake_scan(f, init, inp_scan)\n",
    "    out_loop = expected_ys[:, 0, :]\n",
    "    vals_to_compare.append(out_loop.cpu().detach().numpy())\n",
    "\n",
    "    for _ in range(n_timings):\n",
    "        start = time.time_ns()\n",
    "        expected_carry_out, expected_ys = _fake_scan(f, init, inp_scan)\n",
    "        stop = time.time_ns()\n",
    "        loop_times.append((stop - start) / (10 ** 9))\n",
    "\n",
    "    _fake_scan_comp = torch.compile(_fake_scan, mode='reduce-overhead', fullgraph=True, dynamic=False)\n",
    "\n",
    "    #Warm-up cycles\n",
    "    for _ in range(5):\n",
    "        expected_carry_out_comp, expected_ys_comp = _fake_scan_comp(f, init, inp_scan)\n",
    "\n",
    "    expected_carry_out_comp, expected_ys_comp = _fake_scan_comp(f, init, inp_scan)\n",
    "    out_loop_comp = expected_ys_comp[:, :, 0]\n",
    "    vals_to_compare.append(out_loop_comp.cpu().detach().numpy())\n",
    "\n",
    "    for _ in range(n_timings):\n",
    "        start = time.time_ns()\n",
    "        expected_carry_out_comp, expected_ys_comp = _fake_scan_comp(f, init, inp_scan)\n",
    "        stop = time.time_ns()\n",
    "        loop_comp_times.append((stop - start) / (10 ** 9))\n",
    "\n",
    "    #Check all results for deviations\n",
    "    for ind1, res1 in enumerate(vals_to_compare):\n",
    "        for ind2, res2 in enumerate(vals_to_compare):\n",
    "            if not np.allclose(res1, res2):\n",
    "                print((ind1, res1))\n",
    "                print((ind2, res2))\n",
    "                raise Exception('Comparison of ' + str(ind1) + ' with ' + str(ind2) + ' failed!')\n",
    "\n",
    "    print('Times regular loop ' + str(np.array(loop_times).mean()))\n",
    "    print('Times compiled loop ' + str(np.array(loop_comp_times).mean()))\n",
    "    print('Times triton ' + str(np.array(triton_times).mean()))\n",
    "    print('Script ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7ee91ee17640122f02738ad5b71799946a97252eaa170610250681b99b684d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
