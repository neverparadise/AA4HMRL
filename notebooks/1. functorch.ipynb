{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchopt\n",
    "from torch.func import grad, grad_and_value, vmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional programming\n",
    "순수함수로 구현해야함\n",
    "- 동일한 입력 인자들에 대해서 동일한 결과를 리턴해야함\n",
    "- 사이드 이펙트가 없어야함. 입력 인자들을 코드 내부에서 수정해서는 안됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 1,\n",
    "        num_neurons: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Basic neural network architecture with linear layers\n",
    "        \n",
    "        Args:\n",
    "            num_layers (int, optional): number of hidden layers\n",
    "            num_neurons (int, optional): neurons for each hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "\n",
    "        # input layer\n",
    "        layers.append(nn.Linear(1, num_neurons))\n",
    "\n",
    "        # hidden layers with linear layer and activation\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.Linear(num_neurons, num_neurons), nn.Tanh()])\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(num_neurons, 1))\n",
    "\n",
    "        # build the network\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x.reshape(-1, 1)).squeeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch의 nn.Module 의 경우 stateful 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn([])\n",
    "model = SimpleNN() # constructed above\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "# modify the state of the model\n",
    "# by applying a single optimization step\n",
    "out1 = model(x)\n",
    "true = torch.tensor(1.0)\n",
    "loss = F.mse_loss(out1, true)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "# recompute the output with exactly the same input\n",
    "out2 = model(x)\n",
    "assert not torch.equal(out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=5, bias=True)\n",
       "    (1): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=5, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 순수함수 형태의 stateless 연산"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`functional_call(model, params, (arg1, arg2, ...))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0357, grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import functional_call\n",
    "\n",
    "x = torch.randn([]) # random input data\n",
    "model = SimpleNN() # constructed above\n",
    "params = dict(model.named_parameters()) # model parameters\n",
    "\n",
    "# make a functional call to the model above\n",
    "out = functional_call(model, params, (x,))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0926, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_fn = grad(model)\n",
    "params = tuple(model.named_parameters())\n",
    "grad_values = grad_fn(x)\n",
    "grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(params, x, t):\n",
    "    pred = functional_call(model, params, (x,))\n",
    "    loss = (t - pred) ** 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Thing passed to transform API must be Tensor, got <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_grad_fn \u001b[39m=\u001b[39m grad(mse_loss)\n\u001b[1;32m      2\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(model\u001b[39m.\u001b[39mnamed_parameters())\n\u001b[0;32m----> 3\u001b[0m grad_values \u001b[39m=\u001b[39m loss_grad_fn(params, x, true)\n\u001b[1;32m      4\u001b[0m grad_values\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1380\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m   1379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1380\u001b[0m     results \u001b[39m=\u001b[39m grad_and_value(func, argnums, has_aux\u001b[39m=\u001b[39;49mhas_aux)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1381\u001b[0m     \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m   1382\u001b[0m         grad, (_, aux) \u001b[39m=\u001b[39m results\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     38\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1243\u001b[0m, in \u001b[0;36mgrad_and_value.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m kwargs \u001b[39m=\u001b[39m _wrap_all_tensors(kwargs, level)\n\u001b[1;32m   1242\u001b[0m diff_args \u001b[39m=\u001b[39m _slice_argnums(args, argnums, as_tuple\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1243\u001b[0m tree_map_(partial(_create_differentiable, level\u001b[39m=\u001b[39;49mlevel), diff_args)\n\u001b[1;32m   1245\u001b[0m output \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1246\u001b[0m \u001b[39mif\u001b[39;00m has_aux:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/pytree_hacks.py:12\u001b[0m, in \u001b[0;36mtree_map_\u001b[0;34m(fn_, pytree)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map_\u001b[39m(fn_, pytree):\n\u001b[1;32m     11\u001b[0m     flat_args, _ \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m---> 12\u001b[0m     [fn_(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m flat_args]\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m pytree\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/pytree_hacks.py:12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map_\u001b[39m(fn_, pytree):\n\u001b[1;32m     11\u001b[0m     flat_args, _ \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m---> 12\u001b[0m     [fn_(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m flat_args]\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m pytree\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:57\u001b[0m, in \u001b[0;36m_create_differentiable\u001b[0;34m(inps, level)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThing passed to transform API must be Tensor, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[39mreturn\u001b[39;00m tree_map(create_differentiable, inps)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, pytree)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map\u001b[39m(fn: Any, pytree: PyTree) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PyTree:\n\u001b[1;32m    195\u001b[0m     flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map\u001b[39m(fn: Any, pytree: PyTree) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PyTree:\n\u001b[1;32m    195\u001b[0m     flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:55\u001b[0m, in \u001b[0;36m_create_differentiable.<locals>.create_differentiable\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[39mwith\u001b[39;00m enable_inplace_requires_grad():\n\u001b[1;32m     54\u001b[0m         \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[0;32m---> 55\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThing passed to transform API must be Tensor, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m                  \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Thing passed to transform API must be Tensor, got <class 'str'>"
     ]
    }
   ],
   "source": [
    "loss_grad_fn = grad(mse_loss)\n",
    "params = tuple(model.named_parameters())\n",
    "grad_values = loss_grad_fn(params, x, true)\n",
    "grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_functional_fwd(_model):\n",
    "    def fn(data, parameters):\n",
    "        return functional_call(_model, parameters, (data,))\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func = make_functional_fwd(model) # functional forward\n",
    "params = tuple(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Thing passed to transform API must be Tensor, got <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grad_params \u001b[39m=\u001b[39m grad(model_func, argnums\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)(x, params)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1380\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m   1379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1380\u001b[0m     results \u001b[39m=\u001b[39m grad_and_value(func, argnums, has_aux\u001b[39m=\u001b[39;49mhas_aux)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1381\u001b[0m     \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m   1382\u001b[0m         grad, (_, aux) \u001b[39m=\u001b[39m results\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     38\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1243\u001b[0m, in \u001b[0;36mgrad_and_value.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m kwargs \u001b[39m=\u001b[39m _wrap_all_tensors(kwargs, level)\n\u001b[1;32m   1242\u001b[0m diff_args \u001b[39m=\u001b[39m _slice_argnums(args, argnums, as_tuple\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1243\u001b[0m tree_map_(partial(_create_differentiable, level\u001b[39m=\u001b[39;49mlevel), diff_args)\n\u001b[1;32m   1245\u001b[0m output \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1246\u001b[0m \u001b[39mif\u001b[39;00m has_aux:\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/pytree_hacks.py:12\u001b[0m, in \u001b[0;36mtree_map_\u001b[0;34m(fn_, pytree)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map_\u001b[39m(fn_, pytree):\n\u001b[1;32m     11\u001b[0m     flat_args, _ \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m---> 12\u001b[0m     [fn_(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m flat_args]\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m pytree\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/pytree_hacks.py:12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map_\u001b[39m(fn_, pytree):\n\u001b[1;32m     11\u001b[0m     flat_args, _ \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m---> 12\u001b[0m     [fn_(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m flat_args]\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m pytree\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:57\u001b[0m, in \u001b[0;36m_create_differentiable\u001b[0;34m(inps, level)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThing passed to transform API must be Tensor, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[39mreturn\u001b[39;00m tree_map(create_differentiable, inps)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, pytree)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map\u001b[39m(fn: Any, pytree: PyTree) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PyTree:\n\u001b[1;32m    195\u001b[0m     flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map\u001b[39m(fn: Any, pytree: PyTree) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PyTree:\n\u001b[1;32m    195\u001b[0m     flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:55\u001b[0m, in \u001b[0;36m_create_differentiable.<locals>.create_differentiable\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[39mwith\u001b[39;00m enable_inplace_requires_grad():\n\u001b[1;32m     54\u001b[0m         \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[0;32m---> 55\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThing passed to transform API must be Tensor, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m                  \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Thing passed to transform API must be Tensor, got <class 'str'>"
     ]
    }
   ],
   "source": [
    "grad_params = grad(model_func, argnums=1)(x, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torchopt.adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/.conda/envs/test_env/lib/python3.10/site-packages/torch/_functorch/deprecated.py:97: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('make_functional', 'torch.func.functional_call')\n"
     ]
    }
   ],
   "source": [
    "fmodel, fparams = functorch.make_functional(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torchopt.Adam(model.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7ee91ee17640122f02738ad5b71799946a97252eaa170610250681b99b684d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
