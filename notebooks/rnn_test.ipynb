{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from typing import Callable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmm_input(b_weight, b_input):\n",
    "    batch_size, feature_dim = b_input.shape\n",
    "    bmm = torch.einsum('nfh, nf -> nh', b_weight, b_input) / feature_dim\n",
    "    return bmm\n",
    "\n",
    "\n",
    "def bmm_output(b_weight, b_input):\n",
    "    batch_size, output_dim, shared_output_dim = b_weight.shape\n",
    "    batch_size, shared_output_dim = b_input.shape\n",
    "    # [batch_size, 6, 32], [batch_size, 32]\n",
    "    bmm = torch.einsum('noh, nh -> no', b_weight, b_input)  / shared_output_dim\n",
    "    return bmm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation_name: str):\n",
    "    if activation_name == 'tanh':\n",
    "        activation = nn.Tanh\n",
    "    elif activation_name == 'learnable':\n",
    "        pass\n",
    "    elif activation_name == 'relu':\n",
    "        activation = nn.ReLU\n",
    "    elif activation_name == 'leakyrelu':\n",
    "        activation = nn.LeakyReLU\n",
    "    elif activation_name == \"prelu\":\n",
    "        activation = nn.PReLU\n",
    "    elif activation_name == 'gelu':\n",
    "        activation = nn.GELU\n",
    "    elif activation_name == 'sigmoid':\n",
    "        activation = nn.Sigmoid\n",
    "    elif activation_name in [ None, 'id', 'identity', 'linear', 'none' ]:\n",
    "        activation = nn.Identity\n",
    "    elif activation_name == 'elu':\n",
    "        activation = nn.ELU\n",
    "    elif activation_name in ['swish', 'silu']:\n",
    "        activation = nn.SiLU\n",
    "    elif activation_name == 'softplus':\n",
    "        activation = nn.Softplus\n",
    "    else:\n",
    "        raise NotImplementedError(\"hidden activation '{}' is not implemented\".format(activation))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = 256\n",
    "        self.final_act = \"identity\"\n",
    "        self.final_act_func = get_activation(self.final_act)()\n",
    "        self.embedding = nn.Linear(1, self.d_model)\n",
    "        self.rnn = nn.GRU(input_size=self.d_model,\n",
    "                          hidden_size=self.d_model,\n",
    "                          num_layers=1,\n",
    "                          bias=True,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True\n",
    "                          )\n",
    "                                                \n",
    "    def forward(self, x):\n",
    "        #  # x: [batch_size, feature_dim]\n",
    "        # print(x.storage())\n",
    "        batch_size = x.size(0)\n",
    "        feature_dim = x.size(1)\n",
    "        ux = x.unsqueeze(-1) # x: [batch_size, feature_dim, 1]\n",
    "        ux = self.embedding(ux) # ux: [batch_size, feature_dim, 32]\n",
    "        weights, h_n = self.rnn(ux) # weights shape: [batch_size, feature_dim, 2*d_model]\n",
    "        print(weights.shape)\n",
    "        weights = weights.reshape(batch_size, feature_dim, 2, self.d_model)\n",
    "        weights = weights.mean(dim=2, keepdim=False)\n",
    "        weights = self.final_act_func(weights)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = 256\n",
    "        self.final_act = \"identity\"\n",
    "        self.final_act_func = get_activation(self.final_act)()\n",
    "        self.num_layers = 1\n",
    "        self.bias = True\n",
    "        # GRU input: (N,L,H_in) output: (N,L,H_out)\n",
    "        self.rnn = nn.GRU(input_size=self.d_model, \n",
    "                          hidden_size=self.d_model,\n",
    "                          num_layers=self.num_layers,\n",
    "                          bias=self.bias,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True\n",
    "                          )\n",
    "        \n",
    "    def forward(self, out_dim, embed_featrue):\n",
    "        # shared_feature: [batch_size, shared_dim]\n",
    "        batch_size, *dims = embed_featrue.shape\n",
    "        x = embed_featrue.unsqueeze(1) \n",
    "        # embed_featrue: [batch_size, 1, shared_dim]\n",
    "        # features = [embed_featrue for i in range(out_dim)]\n",
    "        # embed_featrue = torch.cat(features, dim=1) \n",
    "        x = x.expand(batch_size, out_dim, *dims)\n",
    "        x = x.reshape(batch_size, out_dim, -1)\n",
    "         # embed_featrue: [batch_size, out_dim, d_model]\n",
    "        weights, _ = self.rnn(x) \n",
    "        # weights shape: [batch_size, out_dim, 2*d_model]\n",
    "        weights = weights.reshape(batch_size, out_dim, 2, self.d_model)\n",
    "        weights = weights.mean(dim=2, keepdim=False)\n",
    "        # weights shape: [batch_size, out_dim, d_model]\n",
    "        weights = self.final_act_func(weights)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 17, 512])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 6, 256])\n",
      "tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]])\n",
      "torch.Size([32, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "encoder = RNNEncoder()\n",
    "mean_decoder = RNNDecoder()\n",
    "torch.manual_seed(0)\n",
    "cov_mat_decoder1 = RNNDecoder()\n",
    "torch.manual_seed(42)\n",
    "cov_mat_decoder2 = RNNDecoder()\n",
    "\n",
    "obs = torch.ones([32, 17])\n",
    "enc_weights = encoder(obs)\n",
    "hidden = bmm_input(enc_weights, obs)\n",
    "print(hidden.shape)\n",
    "mean_weights = mean_decoder(6, hidden)\n",
    "dec_weights1 = cov_mat_decoder1(6, hidden)\n",
    "dec_weights2 = cov_mat_decoder2(6, hidden)\n",
    "print(dec_weights1.shape)\n",
    "print(dec_weights1 == dec_weights2)\n",
    "\n",
    "mean = bmm_output(mean_weights, hidden)\n",
    "cov_mat = torch.relu(dec_weights1 @ dec_weights2.mT)\n",
    "print(cov_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6, 6])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat2 = torch.relu(dec_weights1 @ dec_weights1.mT)\n",
    "cov_mat2 = dec_weights2 @ dec_weights2.mT\n",
    "cov_mat2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal, Categorical, MultivariateNormal\n",
    "dist = MultivariateNormal(loc=mean, covariance_matrix=cov_mat2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dist.log_prob(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.randn([32, 6])\n",
    "logstd = torch.randn([32, 6])\n",
    "std = logstd.exp()\n",
    "dist = Normal(mean,std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.eye(6)\n",
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6, 6])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.eye(6, 6)\n",
    "bi = torch.stack([i for j in range(32)])\n",
    "bi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dist.sample()\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dist.log_prob(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = dist.entropy()\n",
    "e.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9e147bd2cac9f8cba0aaeac8e3aa69463602115a7ce7416575bcd10d62ea995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
