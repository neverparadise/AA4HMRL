{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 64, 17])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = torch.nn.Linear(4, 17)\n",
    "a = torch.ones([32, 64, 64, 4])\n",
    "b = encoder(a)\n",
    "b.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defaults': [{'ppo': 'ppo'}, {'nn': 'nn'}, {'ccnn_img': 'ccnn_img'}, {'ccnn_seq': 'ccnn_seq'}, '_self_'], 'hydra': {'run': {'dir': 'outputs/${now:%Y-%m-%d/%H-%M-%S}'}}, 'experiment': {'env_ids': ['Breakout-v5'], 'seed': 42, 'max_episode_steps': 1000, 'num_rollout_steps': 128, 'num_envs': 64, 'total_timesteps': 10000000, 'save_ckpt': False, 'num_checkpoints': 20, 'print_interval': 100, 'stop_after_epochs': 500, 'capture_video': False, 'device': 2, 'cuda': True, 'torch_deterministic': True, 'resume': False, 'resume_update_idx': 0, 'resume_dir': 'None'}, 'evaluation': {'eval_seed': 3142, 'every': 8, 'num_eval': 5, 'num_test_envs': 5}, 'wandb': {'mode': 'online', 'project': 'DomainAgnosticRL', 'entity': None, 'name': None, 'group': None, 'tags': None, 'notes': None}, 'paths': {'dir': 'outputs/${now:%Y-%m-%d/%H-%M-%S}', 'log': 'outputs/${now:%Y-%m-%d/%H-%M-%S}/runs', 'video': 'outputs/${now:%Y-%m-%d/%H-%M-%S}/videos', 'checkpoints': 'outputs/${now:%Y-%m-%d/%H-%M-%S}/checkpoints', 'src': 'outputs/${now:%Y-%m-%d/%H-%M-%S}/src', 'scripts': 'outputs/${now:%Y-%m-%d/%H-%M-%S}/scripts'}, 'nn': {'actor_critic': {'use_mlp': True, 'use_transformer': True, 'encoder_net_1d': 'rnn', 'encoder_net_2d': 'cnn', 'decoder_net': 'rnn', 'd_model': 64, 'activation': 'tanh', 'optimizer': 'adam', 'gradient_visualization': False, 'input_to_hidden': 'pooling', 'hidden_to_output': 'pooling'}, 'cnn': {'kernel_size': 3, 'stride': 1, 'num_layers': 3}, 'mlp': {'depth': 1, 'hidden_dim': 64, 'expansion_factor': 32, 'dropout': 0.0, 'activation': 'gelu'}, 'rnn': {'final_activation': 'identity'}, 's4': {'d_state': 64, 'lr': 0.0001, 'num_layers': 2, 'dropout': 0.0, 'final_activation': 'identity', 'initializer': 'uniform'}, 'weight_decay': {'actor': 0.0, 'critic': 0.0}, 'transformer': {'dropout': 0.0, 'dim_feedforward': 64, 'num_heads': 1, 'num_encoder_layers': 1, 'activation': 'gelu'}}, 'ppo': {'actor_lr': 0.0001, 'critic_lr': 0.0005, 'actor_weight_decay': 0.0, 'critic_weight_decay': 0.0, 'anneal_lr': False, 'update_epochs': 5, 'num_minibatches': 8, 'batch_size': 'None', 'minibatch_size': 'None', 'max_grad_norm': 0.5, 'norm_adv': True, 'const_coef': 0.0, 'clip_coef': 0.2, 'clip_vloss': True, 'ent_coef': 0.0, 'vf_coef': 0.5, 'gamma': 0.99, 'gae_lambda': 0.95}, 'ccnn_seq': {'net': {'type': 'ResNet', 'no_hidden': 64, 'no_blocks': 2, 'no_stages': -1, 'data_dim': 2, 'dropout': 0.1, 'dropout_in': 0.0, 'dropout_type': 'Dropout2d', 'norm': 'BatchNorm', 'nonlinearity': 'GELU', 'block_width_factors': [0.0], 'block': {'type': 'S4', 'prenorm': True}, 'prenorm': True, 'downsampling': [], 'downsampling_size': -1}, 'kernel': {'type': 'MAGNet', 'bias': True, 'no_hidden': 64, 'no_layers': 2, 'omega_0': 2085.433586112234, 'input_scale': 0.0, 'size': 33, 'chang_initialize': True, 'norm': 'Identity', 'nonlinearity': 'Identity', 'init_spatial_value': 1.0, 'num_edges': -1, 'bottleneck_factor': -1}, 'mask': {'type': 'gaussian', 'init_value': 0.75, 'threshold': 0.1, 'temperature': 0, 'dynamic_cropping': True, 'learn_mean': False}, 'conv': {'bias': True, 'causal': False, 'type': 'SeparableFlexConv', 'use_fft': False, 'padding': 'same', 'stride': 1, 'cache': False}}, 'ccnn_img': {'net': {'type': 'ResNet', 'no_hidden': 32, 'no_blocks': 2, 'no_stages': -1, 'data_dim': 1, 'dropout': 0.1, 'dropout_in': 0.0, 'dropout_type': 'Dropout1d', 'norm': 'BatchNorm', 'nonlinearity': 'GELU', 'block_width_factors': [0.0], 'block': {'type': 'S4', 'prenorm': True}, 'prenorm': True, 'downsampling': [], 'downsampling_size': -1}, 'kernel': {'type': 'MAGNet', 'bias': True, 'no_hidden': 32, 'no_layers': 2, 'omega_0': 976.781, 'input_scale': 0.0, 'size': 33, 'chang_initialize': True, 'norm': 'Identity', 'nonlinearity': 'Identity', 'init_spatial_value': 1.0, 'num_edges': -1, 'bottleneck_factor': -1}, 'mask': {'type': 'gaussian', 'init_value': 0.075, 'threshold': 0.1, 'temperature': 0, 'dynamic_cropping': True, 'learn_mean': False}, 'conv': {'bias': True, 'causal': True, 'type': 'SeparableFlexConv', 'use_fft': True, 'padding': 'same', 'stride': 1, 'cache': False}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from ccnn2.model_constructor import construct_models\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "cfg_path = \"/home/kukjin/kukjin/Projects/MultiEnvRL/DARL_transformer/configs/ppo_trainer.yaml\"\n",
    "nn_cfg_path = \"/home/kukjin/kukjin/Projects/MultiEnvRL/DARL_transformer/configs/nn/nn.yaml\"\n",
    "ppo_cfg_path = \"/home/kukjin/kukjin/Projects/MultiEnvRL/DARL_transformer/configs/ppo/ppo.yaml\"\n",
    "ccnn_cfg_img_path = \"/home/kukjin/kukjin/Projects/MultiEnvRL/DARL_transformer/configs/ccnn_img/ccnn_img.yaml\"\n",
    "ccnn_cfg_seq_path = \"/home/kukjin/kukjin/Projects/MultiEnvRL/DARL_transformer/configs/ccnn_seq/ccnn_seq.yaml\"\n",
    "\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "nn_cfg = OmegaConf.load(nn_cfg_path)\n",
    "ppo_cfg = OmegaConf.load(ppo_cfg_path)\n",
    "ccnn_seq_cfg = OmegaConf.load(ccnn_cfg_img_path)\n",
    "ccnn_img_cfg = OmegaConf.load(ccnn_cfg_seq_path)\n",
    "\n",
    "cfg.nn = nn_cfg\n",
    "cfg.ppo = ppo_cfg\n",
    "cfg.ccnn_seq = ccnn_seq_cfg\n",
    "cfg.ccnn_img = ccnn_img_cfg\n",
    "\n",
    "cfg\n",
    "\n",
    "# img_model, seq_model = construct_models(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    # TODO\n",
    "    kernel_size = cfg.nn.cnn.kernel_size\n",
    "    stride = cfg.nn.cnn.stride\n",
    "    in_channels = 4\n",
    "    out_channels = 64\n",
    "    final_channels = cfg.nn.actor_critic.d_model\n",
    "    num_layers = cfg.nn.cnn.num_layers\n",
    "    self.leakyrelu = nn.LeakyReLU()\n",
    "    self.first_block = nn.Sequential(\n",
    "          nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                            padding=1, bias=True),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "          nn.LeakyReLU())\n",
    "    \n",
    "    self.conv_layers = nn.ModuleList()\n",
    "    self.channels = [out_channels for i in range(num_layers)]\n",
    "    for i in range(num_layers):  \n",
    "      conv_block = nn.Sequential(\n",
    "                                nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                padding=1, bias=True),\n",
    "                                nn.LeakyReLU())\n",
    "                                \n",
    "      self.conv_layers.append(conv_block)\n",
    "    self.final_block = nn.Sequential(\n",
    "          nn.Conv2d(out_channels, final_channels, kernel_size=kernel_size, stride=stride,\n",
    "                            padding=1, bias=True),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "          nn.LeakyReLU())\n",
    "    self.avg_pooling = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    if len(x.shape) < 4:\n",
    "      x = x.unsqueeze(0)\n",
    "    x = self.first_block(x)\n",
    "    shortcut = x\n",
    "    for conv_block in self.conv_layers:\n",
    "      x = conv_block(x)\n",
    "      x += shortcut\n",
    "      shortcut = x\n",
    "    x = self.avg_pooling(x)\n",
    "    x = x.squeeze().squeeze()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = Resnet(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150336"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = 0\n",
    "for name, param in resnet.named_parameters():\n",
    "    total_params += param.numel()\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_img = torch.randn([32, 4, 64, 64])\n",
    "out = resnet(rand_img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233455"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = 0\n",
    "for name, param in seq_model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/.conda/envs/darl2/lib/python3.10/site-packages/torch/nn/functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_input = torch.randn([32, 1, 1211])\n",
    "\n",
    "out = seq_model(seq_input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccnn2.ccnn_models import resnet as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model_img(\n",
    "    cfg: OmegaConf,\n",
    "):\n",
    "    \"\"\"\n",
    "    :param cfg: configuration file\n",
    "    :return: An instance of torch.nn.Module\n",
    "    \"\"\"\n",
    "    # Get parameters of model from task type\n",
    "    data_dim = 2\n",
    "    in_channels = 3\n",
    "    out_channels = 256\n",
    "    data_type = \"image\"\n",
    "\n",
    "    # Get type of model from task type\n",
    "    net_type = f\"{cfg.ccnn_img.net.type}_{data_type}\"\n",
    "\n",
    "    # Overwrite data_dim in cfg.net\n",
    "    # cfg.net.data_dim = data_dim\n",
    "\n",
    "    # Print automatically derived model parameters.\n",
    "    print(\n",
    "        # f\"Automatic Parameters:\\n dataset = {cfg.dataset.name}, \"\n",
    "        f\"net_name = {net_type},\"\n",
    "        f\" data_dim = {data_dim}\"\n",
    "        f\" in_channels = {in_channels},\"\n",
    "        f\" out_chanels = {out_channels}.\"\n",
    "    )\n",
    "    if out_channels == 2:\n",
    "        print(\n",
    "            \"The model will output one single channel. We use BCEWithLogitsLoss for training.\"\n",
    "        )\n",
    "\n",
    "    # Create and return model\n",
    "    net_type = getattr(models, net_type)\n",
    "    network = net_type(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels if out_channels != 2 else 1,\n",
    "        net_cfg=cfg.ccnn_img.net,\n",
    "        kernel_cfg=cfg.ccnn_img.kernel,\n",
    "        conv_cfg=cfg.ccnn_img.conv,\n",
    "        mask_cfg=cfg.ccnn_img.mask,\n",
    "    )\n",
    "\n",
    "    return network\n",
    "\n",
    "def construct_model_seq(\n",
    "    cfg: OmegaConf,\n",
    "):\n",
    "    \"\"\"\n",
    "    :param cfg: configuration file\n",
    "    :return: An instance of torch.nn.Module\n",
    "    \"\"\"\n",
    "    # Get parameters of model from task type\n",
    "    data_dim = 1\n",
    "    in_channels = 1\n",
    "    out_channels = 256\n",
    "    data_type = \"sequence\"\n",
    "\n",
    "    # Get type of model from task type\n",
    "    net_type = f\"{cfg.ccnn_seq.net.type}_{data_type}\"\n",
    "\n",
    "    # Overwrite data_dim in cfg.net\n",
    "    # cfg.net.data_dim = data_dim\n",
    "\n",
    "    # Print automatically derived model parameters.\n",
    "    print(\n",
    "        # f\"Automatic Parameters:\\n dataset = {cfg.dataset.name}, \"\n",
    "        f\"net_name = {net_type},\"\n",
    "        f\" data_dim = {data_dim}\"\n",
    "        f\" in_channels = {in_channels},\"\n",
    "        f\" out_chanels = {out_channels}.\"\n",
    "    )\n",
    "    if out_channels == 2:\n",
    "        print(\n",
    "            \"The model will output one single channel. We use BCEWithLogitsLoss for training.\"\n",
    "        )\n",
    "\n",
    "    # Create and return model\n",
    "    net_type = getattr(models, net_type)\n",
    "    network = net_type(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels if out_channels != 2 else 1,\n",
    "        net_cfg=cfg.ccnn_seq.net,\n",
    "        kernel_cfg=cfg.ccnn_seq.kernel,\n",
    "        conv_cfg=cfg.ccnn_seq.conv,\n",
    "        mask_cfg=cfg.ccnn_seq.mask,\n",
    "    )\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_name = ResNet_image, data_dim = 2 in_channels = 3, out_chanels = 256.\n",
      "Block 0/4\n",
      "Block 1/4\n",
      "Block 2/4\n",
      "Block 3/4\n",
      "net_name = ResNet_sequence, data_dim = 1 in_channels = 1, out_chanels = 256.\n",
      "Block 0/4\n",
      "Block 1/4\n",
      "Block 2/4\n",
      "Block 3/4\n"
     ]
    }
   ],
   "source": [
    "img_model = construct_model_img(cfg)\n",
    "seq_model = construct_model_seq(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/.conda/envs/darl2/lib/python3.10/site-packages/torch/nn/functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_input = torch.randn([32, 1, 1211])\n",
    "out = seq_model(seq_input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Convolutional kernels must have odd dimensionality. Received torch.Size([1, 3, 33, 20])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m img_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn([\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m out \u001b[39m=\u001b[39m img_model(img_input)\n",
      "File \u001b[0;32m~/.conda/envs/darl2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/hdd_ext/hdd2/kukjin/Projects/MultiEnvRL/DARL_transformer/ccnn/ccnn_models/resnet.py:188\u001b[0m, in \u001b[0;36mResNet_image.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    186\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_in(x)\n\u001b[1;32m    187\u001b[0m \u001b[39m# First layers\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m    189\u001b[0m \u001b[39m# Blocks\u001b[39;00m\n\u001b[1;32m    190\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks(out)\n",
      "File \u001b[0;32m~/.conda/envs/darl2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/hdd_ext/hdd2/kukjin/Projects/MultiEnvRL/DARL_transformer/ccnn/ckconvs/nn/flexconv.py:281\u001b[0m, in \u001b[0;36mSeparableFlexConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    278\u001b[0m     conv_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_types[\u001b[39m\"\u001b[39m\u001b[39mspatial\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    279\u001b[0m \u001b[39m# 3. Compute depthwise convolution\u001b[39;00m\n\u001b[1;32m    280\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel_mixer(\n\u001b[0;32m--> 281\u001b[0m     conv_type(x, conv_kernel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, separable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, causal\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcausal)\n\u001b[1;32m    282\u001b[0m )\n\u001b[1;32m    283\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/hdd_ext/hdd2/kukjin/Projects/MultiEnvRL/DARL_transformer/ccnn/ckconvs/nn/functional/conv.py:36\u001b[0m, in \u001b[0;36mconv\u001b[0;34m(x, kernel, bias, separable, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m conv_function \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconv\u001b[39m\u001b[39m{\u001b[39;00mdata_dim\u001b[39m}\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m kernel_size \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(kernel\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39mdata_dim:])\n\u001b[0;32m---> 36\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mall(\n\u001b[1;32m     37\u001b[0m     kernel_size \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     38\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConvolutional kernels must have odd dimensionality. Received \u001b[39m\u001b[39m{\u001b[39;00mkernel\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m padding \u001b[39m=\u001b[39m (kernel_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m separable:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Convolutional kernels must have odd dimensionality. Received torch.Size([1, 3, 33, 20])"
     ]
    }
   ],
   "source": [
    "img_input = torch.randn([1, 3, 256, 256])\n",
    "out = img_model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 4, 33, 20]' is invalid for input of size 1980",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m img_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn([\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m out \u001b[39m=\u001b[39m img_model(img_input)\n",
      "File \u001b[0;32m~/.conda/envs/darl2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/hdd_ext/hdd2/kukjin/Projects/MultiEnvRL/DARL_transformer/ccnn/ccnn_models/resnet.py:188\u001b[0m, in \u001b[0;36mResNet_image.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    186\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_in(x)\n\u001b[1;32m    187\u001b[0m \u001b[39m# First layers\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m    189\u001b[0m \u001b[39m# Blocks\u001b[39;00m\n\u001b[1;32m    190\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks(out)\n",
      "File \u001b[0;32m~/.conda/envs/darl2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/hdd_ext/hdd2/kukjin/Projects/MultiEnvRL/DARL_transformer/ccnn/ckconvs/nn/flexconv.py:271\u001b[0m, in \u001b[0;36mSeparableFlexConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    270\u001b[0m     \u001b[39m# 1. Compute the masked kernel\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m     conv_kernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct_masked_kernel(x)\n\u001b[1;32m    272\u001b[0m     \u001b[39m# 2. Select convolution type\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     size \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(conv_kernel\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:])\n",
      "File \u001b[0;32m/hdd_ext/hdd2/kukjin/Projects/MultiEnvRL/DARL_transformer/ccnn/ckconvs/nn/flexconv.py:187\u001b[0m, in \u001b[0;36mFlexConvBase.construct_masked_kernel\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39m# 4. sample the kernel\u001b[39;00m\n\u001b[1;32m    186\u001b[0m x_shape \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 187\u001b[0m conv_kernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mKernel(kernel_pos)\u001b[39m.\u001b[39;49mview(\n\u001b[1;32m    188\u001b[0m     \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x_shape[\u001b[39m1\u001b[39;49m], \u001b[39m*\u001b[39;49mkernel_pos\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m:]\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    190\u001b[0m \u001b[39m# 5. construct mask and multiply with conv-kernel\u001b[39;00m\n\u001b[1;32m    191\u001b[0m mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_constructor(\n\u001b[1;32m    192\u001b[0m     kernel_pos,\n\u001b[1;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_mean_param\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dim),\n\u001b[1;32m    194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_width_param\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dim),\n\u001b[1;32m    195\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_temperature,\n\u001b[1;32m    196\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 4, 33, 20]' is invalid for input of size 1980"
     ]
    }
   ],
   "source": [
    "img_input = torch.randn([1, 4, 256, 256])\n",
    "out = img_model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout2d = torch.nn.Dropout2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/.conda/envs/darl2/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "out = dropout2d(seq_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1211])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9e147bd2cac9f8cba0aaeac8e3aa69463602115a7ce7416575bcd10d62ea995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
